{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'apt'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "# 한글 폰트 설치하기 (꼭! 설치가 완료되면 [런타임 다시 시작]을 누르고 다시 실행하기)\n",
        "!apt install fonts-nanum -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] [-e] <local project path> ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -y\n"
          ]
        }
      ],
      "source": [
        "%pip install fonts-nanum -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3krZm7boUYaC"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.font_manager' has no attribute '_rebuild'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\user\\Desktop\\7. 김한호 강사님\\2022.12.13\\resnet34_pretrain_final.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/7.%20%EA%B9%80%ED%95%9C%ED%98%B8%20%EA%B0%95%EC%82%AC%EB%8B%98/2022.12.13/resnet34_pretrain_final.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m font \u001b[39m=\u001b[39m fm\u001b[39m.\u001b[39mFontProperties(fname\u001b[39m=\u001b[39mfontpath, size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/7.%20%EA%B9%80%ED%95%9C%ED%98%B8%20%EA%B0%95%EC%82%AC%EB%8B%98/2022.12.13/resnet34_pretrain_final.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mrc(\u001b[39m'\u001b[39m\u001b[39mfont\u001b[39m\u001b[39m'\u001b[39m, family\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNanumBarunGothic\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/7.%20%EA%B9%80%ED%95%9C%ED%98%B8%20%EA%B0%95%EC%82%AC%EB%8B%98/2022.12.13/resnet34_pretrain_final.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m matplotlib\u001b[39m.\u001b[39;49mfont_manager\u001b[39m.\u001b[39;49m_rebuild()\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.font_manager' has no attribute '_rebuild'"
          ]
        }
      ],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 한글 폰트 설정하기\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=10)\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "matplotlib.font_manager._rebuild()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4Z_KLEjUVZWY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "directory_list = [\n",
        "    './custom_dataset/train/',\n",
        "    './custom_dataset/test/',\n",
        "]\n",
        "\n",
        "# 초기 디렉토리 만들기\n",
        "for directory in directory_list:\n",
        "    if not os.path.isdir(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "# 수집한 이미지를 학습 데이터와 평가 데이터로 구분하는 함수\n",
        "def dataset_split(query, train_cnt):\n",
        "    # 학습 및 평가 데이터셋 디렉토리 만들기\n",
        "    for directory in directory_list:\n",
        "        if not os.path.isdir(directory + '/' + query):\n",
        "            os.makedirs(directory + '/' + query)\n",
        "    # 학습 및 평가 데이터셋 준비하기\n",
        "    cnt = 0\n",
        "    for file_name in os.listdir(query):\n",
        "        if cnt < train_cnt:\n",
        "            # print(f'[Train Dataset] {file_name}')\n",
        "            shutil.move(query + '/' + file_name, './custom_dataset/train/' + query + '/' + file_name)\n",
        "        else:\n",
        "            # print(f'[Test Dataset] {file_name}')\n",
        "            shutil.move(query + '/' + file_name, './custom_dataset/test/' + query + '/' + file_name)\n",
        "        cnt += 1\n",
        "    shutil.rmtree(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kOR6UwvhXsC5"
      },
      "outputs": [],
      "source": [
        "query = '0'\n",
        "dataset_split(query, 210)\n",
        "query = '1'\n",
        "dataset_split(query, 210)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQ48DEenYHev"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device 객체"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pXHpCHvcYJ2P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터셋 크기: 2004\n",
            "테스트 데이터셋 크기: 3038\n",
            "클래스: ['0', '1']\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    # transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "# shuffle : set to True to have the data reshuffled at every epoch (이거 True 설정해도 결과값이 같게 나오네 완전 랜덤으로 섞는건 아닌듯)\n",
        "# num_workers : how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\n",
        "# 보통의 일반적인 환경에서 오픈소스로 풀려있는 모델을 학습시킬때는 코어 개수의 절반정도 수치면 무난하게 시스템 리소스를 사용하며 학습이 가능했습니다\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SYwt5TpIYTGX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0])\n",
            "(3, 454, 1810)\n",
            "===input==> (454, 1810, 3)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def imshow(input, title):\n",
        "    # torch.Tensor를 numpy 객체로 변환\n",
        "    print(input.numpy().shape)\n",
        "\n",
        "    input = input.numpy().transpose((1, 2, 0))\n",
        "    # 이미지 정규화 해제하기\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    input = std * input + mean\n",
        "    input = np.clip(input, 0, 1)\n",
        "    # 이미지 출력\n",
        "\n",
        "    print('===input==>',input.shape)\n",
        "    plt.imshow(input)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 학습 데이터를 배치 단위로 불러오기\n",
        "iterator = iter(train_dataloader)\n",
        "\n",
        "# 현재 배치를 이용해 격자 형태의 이미지를 만들어 시각화\n",
        "inputs, classes = next(iterator)\n",
        "print(classes)\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "G1btTeRKYa0-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# 이미 학습된 resnet34 신경망을 불러온다\n",
        "model = models.resnet34(pretrained=True)\n",
        "print(model)\n",
        "#for name,module in model.named_parameters():\n",
        "#    module.requires_grad = False\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_features = model.fc.in_features\n",
        "# 전이 학습(transfer learning): 모델의 출력 뉴런 수를 3개로 교체하여 마지막 레이어 다시 학습\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "model = model.to(device)\n",
        "for name,module in model.named_parameters():\n",
        "    print( module.requires_grad )\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "#stochastic gradint descent : 확률적 경사하강법\n",
        "#미니배치를 사용하여 다소 부정확할수는 있지만 계산 속도가 빠르다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "K1phMzVPYu9u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Loss: 5.4785 Acc: 53.5429% Time: 7.7399s\n",
            "#1 Loss: 3.8845 Acc: 54.5409% Time: 15.4265s\n",
            "#2 Loss: 5.6512 Acc: 54.9900% Time: 23.1467s\n",
            "#3 Loss: 5.3920 Acc: 54.2914% Time: 30.7902s\n",
            "#4 Loss: 4.8860 Acc: 54.9401% Time: 38.4101s\n",
            "#5 Loss: 4.4315 Acc: 56.0878% Time: 46.0553s\n",
            "#6 Loss: 5.0751 Acc: 55.3892% Time: 53.6808s\n",
            "#7 Loss: 5.1890 Acc: 54.6407% Time: 61.2118s\n",
            "#8 Loss: 4.8743 Acc: 55.1397% Time: 68.8264s\n",
            "#9 Loss: 4.2732 Acc: 56.4371% Time: 76.4265s\n",
            "#10 Loss: 4.4926 Acc: 57.9341% Time: 84.0219s\n",
            "#11 Loss: 4.8677 Acc: 55.7884% Time: 91.5816s\n",
            "#12 Loss: 4.4856 Acc: 55.7385% Time: 99.1684s\n",
            "#13 Loss: 4.5875 Acc: 56.3872% Time: 106.7769s\n",
            "#14 Loss: 5.6784 Acc: 56.9860% Time: 114.3750s\n",
            "#15 Loss: 4.4142 Acc: 56.4870% Time: 122.0486s\n",
            "#16 Loss: 5.0942 Acc: 56.8862% Time: 129.6353s\n",
            "#17 Loss: 4.9072 Acc: 57.7844% Time: 137.2828s\n",
            "#18 Loss: 5.5248 Acc: 57.9840% Time: 144.8415s\n",
            "#19 Loss: 4.8779 Acc: 55.8882% Time: 152.3683s\n",
            "#20 Loss: 4.5187 Acc: 57.7844% Time: 159.9798s\n",
            "#21 Loss: 4.8342 Acc: 57.6846% Time: 167.5248s\n",
            "#22 Loss: 6.1598 Acc: 54.7904% Time: 175.1612s\n",
            "#23 Loss: 5.4318 Acc: 54.9401% Time: 182.8230s\n",
            "#24 Loss: 5.0121 Acc: 57.4850% Time: 190.3764s\n",
            "#25 Loss: 5.8087 Acc: 56.0379% Time: 197.9767s\n",
            "#26 Loss: 5.0576 Acc: 57.3353% Time: 205.5688s\n",
            "#27 Loss: 4.3881 Acc: 56.9361% Time: 213.2388s\n",
            "#28 Loss: 5.2754 Acc: 59.0319% Time: 220.9186s\n",
            "#29 Loss: 4.9998 Acc: 59.1317% Time: 228.5078s\n",
            "#30 Loss: 5.1160 Acc: 55.3393% Time: 236.2135s\n",
            "#31 Loss: 5.6397 Acc: 58.1836% Time: 243.7045s\n",
            "#32 Loss: 5.2362 Acc: 54.1417% Time: 251.1239s\n",
            "#33 Loss: 4.5567 Acc: 58.2335% Time: 258.7566s\n",
            "#34 Loss: 5.2927 Acc: 56.7864% Time: 266.3330s\n",
            "#35 Loss: 4.9533 Acc: 56.3872% Time: 273.9472s\n",
            "#36 Loss: 4.6022 Acc: 57.7844% Time: 281.5119s\n",
            "#37 Loss: 4.4445 Acc: 57.3353% Time: 289.1462s\n",
            "#38 Loss: 3.9468 Acc: 56.4371% Time: 296.7188s\n",
            "#39 Loss: 4.7036 Acc: 56.1377% Time: 304.3390s\n",
            "#40 Loss: 5.7285 Acc: 56.4371% Time: 311.9294s\n",
            "#41 Loss: 4.6229 Acc: 55.7385% Time: 319.5402s\n",
            "#42 Loss: 4.7415 Acc: 56.3872% Time: 327.0824s\n",
            "#43 Loss: 4.0581 Acc: 57.4351% Time: 334.6866s\n",
            "#44 Loss: 4.9298 Acc: 56.5369% Time: 342.3546s\n",
            "#45 Loss: 4.8925 Acc: 56.6866% Time: 349.8718s\n",
            "#46 Loss: 4.6657 Acc: 58.1836% Time: 357.4226s\n",
            "#47 Loss: 4.8882 Acc: 56.8363% Time: 365.0656s\n",
            "#48 Loss: 6.0951 Acc: 56.9361% Time: 372.7119s\n",
            "#49 Loss: 4.8574 Acc: 56.8862% Time: 380.2831s\n",
            "#50 Loss: 5.0090 Acc: 57.8842% Time: 387.8950s\n",
            "#51 Loss: 5.0161 Acc: 56.0379% Time: 395.4913s\n",
            "#52 Loss: 4.7683 Acc: 56.0379% Time: 403.0832s\n",
            "#53 Loss: 4.7639 Acc: 56.6367% Time: 410.7163s\n",
            "#54 Loss: 4.1057 Acc: 57.5349% Time: 418.3238s\n",
            "#55 Loss: 5.3278 Acc: 55.6886% Time: 425.8984s\n",
            "#56 Loss: 5.1812 Acc: 56.0379% Time: 433.5271s\n",
            "#57 Loss: 4.2565 Acc: 59.2814% Time: 441.0787s\n",
            "#58 Loss: 5.0364 Acc: 57.5349% Time: 448.6737s\n",
            "#59 Loss: 3.6695 Acc: 59.4810% Time: 456.2832s\n",
            "#60 Loss: 4.7931 Acc: 55.7884% Time: 463.9021s\n",
            "#61 Loss: 5.3281 Acc: 55.6886% Time: 471.5046s\n",
            "#62 Loss: 4.6324 Acc: 57.8343% Time: 479.0681s\n",
            "#63 Loss: 4.2949 Acc: 58.8822% Time: 486.6695s\n",
            "#64 Loss: 4.8329 Acc: 56.7864% Time: 494.1914s\n",
            "#65 Loss: 4.6428 Acc: 57.3353% Time: 501.8243s\n",
            "#66 Loss: 5.1068 Acc: 57.4351% Time: 509.4233s\n",
            "#67 Loss: 5.3237 Acc: 57.5349% Time: 517.0893s\n",
            "#68 Loss: 5.3563 Acc: 57.2854% Time: 524.7307s\n",
            "#69 Loss: 4.4849 Acc: 59.5808% Time: 532.4069s\n",
            "#70 Loss: 4.3672 Acc: 58.3832% Time: 540.0537s\n",
            "#71 Loss: 4.9319 Acc: 58.1337% Time: 547.7119s\n",
            "#72 Loss: 4.5168 Acc: 57.1357% Time: 555.2649s\n",
            "#73 Loss: 4.7263 Acc: 57.1357% Time: 562.9237s\n",
            "#74 Loss: 4.2548 Acc: 58.2834% Time: 570.4362s\n",
            "#75 Loss: 4.5930 Acc: 57.7844% Time: 578.0453s\n",
            "#76 Loss: 4.6145 Acc: 56.5369% Time: 585.5742s\n",
            "#77 Loss: 4.7886 Acc: 56.4870% Time: 593.2020s\n",
            "#78 Loss: 5.3382 Acc: 57.6347% Time: 600.7650s\n",
            "#79 Loss: 4.6866 Acc: 58.5828% Time: 608.4301s\n",
            "#80 Loss: 4.3237 Acc: 59.1317% Time: 616.0848s\n",
            "#81 Loss: 4.4386 Acc: 58.2335% Time: 623.6377s\n",
            "#82 Loss: 5.5074 Acc: 55.3393% Time: 631.2076s\n",
            "#83 Loss: 4.6409 Acc: 55.7884% Time: 638.7892s\n",
            "#84 Loss: 5.6804 Acc: 55.5888% Time: 646.4487s\n",
            "#85 Loss: 4.7096 Acc: 55.6886% Time: 653.9859s\n",
            "#86 Loss: 3.6254 Acc: 60.8283% Time: 661.5715s\n",
            "#87 Loss: 4.7758 Acc: 57.7844% Time: 668.9911s\n",
            "#88 Loss: 6.0947 Acc: 56.7864% Time: 676.5875s\n",
            "#89 Loss: 5.4175 Acc: 58.6826% Time: 684.2519s\n",
            "#90 Loss: 5.5454 Acc: 57.5848% Time: 691.7765s\n",
            "#91 Loss: 4.7212 Acc: 57.1357% Time: 699.3852s\n",
            "#92 Loss: 4.2938 Acc: 57.9341% Time: 707.0116s\n",
            "#93 Loss: 5.1805 Acc: 56.6367% Time: 714.6014s\n",
            "#94 Loss: 4.5948 Acc: 56.2874% Time: 722.1523s\n",
            "#95 Loss: 4.2354 Acc: 58.1337% Time: 729.7505s\n",
            "#96 Loss: 4.6419 Acc: 56.3872% Time: 737.4142s\n",
            "#97 Loss: 5.9589 Acc: 53.6926% Time: 744.9489s\n",
            "#98 Loss: 4.6461 Acc: 57.4351% Time: 752.5374s\n",
            "#99 Loss: 5.4027 Acc: 56.3373% Time: 760.1757s\n",
            "#100 Loss: 4.7437 Acc: 57.4850% Time: 767.7958s\n",
            "#101 Loss: 3.8039 Acc: 59.4311% Time: 775.4140s\n",
            "#102 Loss: 4.7482 Acc: 56.5868% Time: 782.9305s\n",
            "#103 Loss: 4.3714 Acc: 58.3832% Time: 790.5511s\n",
            "#104 Loss: 3.9322 Acc: 58.7824% Time: 798.1320s\n",
            "#105 Loss: 4.4487 Acc: 55.2395% Time: 805.7567s\n",
            "#106 Loss: 5.9411 Acc: 57.1856% Time: 813.3675s\n",
            "#107 Loss: 5.0051 Acc: 56.6866% Time: 820.9952s\n",
            "#108 Loss: 5.1439 Acc: 55.4391% Time: 828.5481s\n",
            "#109 Loss: 4.1999 Acc: 58.2834% Time: 836.1099s\n",
            "#110 Loss: 4.5922 Acc: 57.8343% Time: 843.8160s\n",
            "#111 Loss: 5.0129 Acc: 55.1896% Time: 851.3575s\n",
            "#112 Loss: 5.8415 Acc: 54.2415% Time: 858.9421s\n",
            "#113 Loss: 4.4412 Acc: 59.1816% Time: 866.5264s\n",
            "#114 Loss: 4.9713 Acc: 59.4311% Time: 874.1402s\n",
            "#115 Loss: 3.9589 Acc: 60.2794% Time: 881.7621s\n",
            "#116 Loss: 4.9588 Acc: 55.8383% Time: 889.3714s\n",
            "#117 Loss: 4.6548 Acc: 58.3333% Time: 896.9762s\n",
            "#118 Loss: 5.1827 Acc: 56.3373% Time: 904.5481s\n",
            "#119 Loss: 4.7237 Acc: 57.2854% Time: 912.1293s\n",
            "#120 Loss: 4.5614 Acc: 58.5828% Time: 919.7254s\n",
            "#121 Loss: 4.7037 Acc: 56.9860% Time: 927.3399s\n",
            "#122 Loss: 4.5587 Acc: 56.8363% Time: 934.8798s\n",
            "#123 Loss: 4.1643 Acc: 58.1836% Time: 942.5234s\n",
            "#124 Loss: 5.3055 Acc: 56.5369% Time: 950.0879s\n",
            "#125 Loss: 4.3210 Acc: 58.8822% Time: 957.6834s\n",
            "#126 Loss: 4.2802 Acc: 56.7365% Time: 965.2945s\n",
            "#127 Loss: 5.2727 Acc: 56.8363% Time: 972.8526s\n",
            "#128 Loss: 5.0359 Acc: 57.0359% Time: 980.5246s\n",
            "#129 Loss: 4.3358 Acc: 58.1836% Time: 988.0729s\n",
            "#130 Loss: 4.4131 Acc: 56.5369% Time: 995.6252s\n",
            "#131 Loss: 4.2619 Acc: 55.6886% Time: 1003.3214s\n",
            "#132 Loss: 4.3159 Acc: 58.0838% Time: 1010.8879s\n",
            "#133 Loss: 4.5257 Acc: 57.8842% Time: 1018.4847s\n",
            "#134 Loss: 5.1328 Acc: 57.1357% Time: 1026.0547s\n",
            "#135 Loss: 5.1516 Acc: 56.5868% Time: 1033.6784s\n",
            "#136 Loss: 6.1243 Acc: 53.9920% Time: 1041.2584s\n",
            "#137 Loss: 4.6921 Acc: 57.0359% Time: 1048.9068s\n",
            "#138 Loss: 4.5215 Acc: 56.8363% Time: 1056.5410s\n",
            "#139 Loss: 5.1510 Acc: 54.1916% Time: 1064.2087s\n",
            "#140 Loss: 5.5306 Acc: 55.4890% Time: 1071.8637s\n",
            "#141 Loss: 5.2829 Acc: 56.4870% Time: 1079.5056s\n",
            "#142 Loss: 4.6506 Acc: 55.7884% Time: 1087.0931s\n",
            "#143 Loss: 4.4776 Acc: 57.6846% Time: 1094.6809s\n",
            "#144 Loss: 4.7839 Acc: 57.6846% Time: 1102.3064s\n",
            "#145 Loss: 5.0283 Acc: 58.3333% Time: 1109.8558s\n",
            "#146 Loss: 5.8502 Acc: 56.6866% Time: 1117.4541s\n",
            "#147 Loss: 4.4462 Acc: 56.4371% Time: 1125.0416s\n",
            "#148 Loss: 4.4033 Acc: 57.8343% Time: 1132.6965s\n",
            "#149 Loss: 5.0050 Acc: 55.2894% Time: 1140.3006s\n",
            "#150 Loss: 4.3077 Acc: 57.4850% Time: 1147.8688s\n",
            "#151 Loss: 4.6627 Acc: 56.1377% Time: 1155.4046s\n",
            "#152 Loss: 5.5706 Acc: 57.1357% Time: 1163.0218s\n",
            "#153 Loss: 5.3293 Acc: 56.3872% Time: 1170.6672s\n",
            "#154 Loss: 4.3107 Acc: 56.5868% Time: 1178.2733s\n",
            "#155 Loss: 4.8872 Acc: 56.4371% Time: 1185.8480s\n",
            "#156 Loss: 4.5120 Acc: 57.2854% Time: 1193.4599s\n",
            "#157 Loss: 4.3728 Acc: 57.0359% Time: 1201.0789s\n",
            "#158 Loss: 4.8265 Acc: 59.1816% Time: 1208.6111s\n",
            "#159 Loss: 4.8560 Acc: 58.4830% Time: 1216.3075s\n",
            "#160 Loss: 4.1870 Acc: 57.4850% Time: 1223.9147s\n",
            "#161 Loss: 4.2505 Acc: 56.7365% Time: 1231.4358s\n",
            "#162 Loss: 3.9207 Acc: 59.9800% Time: 1239.0310s\n",
            "#163 Loss: 4.8655 Acc: 56.7365% Time: 1246.5899s\n",
            "#164 Loss: 5.5636 Acc: 55.2894% Time: 1254.2209s\n",
            "#165 Loss: 5.1193 Acc: 57.9840% Time: 1261.8727s\n",
            "#166 Loss: 5.7238 Acc: 57.0858% Time: 1269.4042s\n",
            "#167 Loss: 3.8255 Acc: 58.7824% Time: 1276.9534s\n",
            "#168 Loss: 4.6594 Acc: 58.1337% Time: 1284.5682s\n",
            "#169 Loss: 4.4291 Acc: 57.3852% Time: 1292.2015s\n",
            "#170 Loss: 5.1067 Acc: 55.5389% Time: 1299.8517s\n",
            "#171 Loss: 4.6669 Acc: 57.5848% Time: 1307.3673s\n",
            "#172 Loss: 5.2208 Acc: 57.1856% Time: 1315.0624s\n",
            "#173 Loss: 4.4668 Acc: 60.1297% Time: 1322.6035s\n",
            "#174 Loss: 4.8846 Acc: 56.4371% Time: 1330.1909s\n",
            "#175 Loss: 5.0122 Acc: 57.7345% Time: 1337.5602s\n",
            "#176 Loss: 5.2703 Acc: 57.1856% Time: 1345.2191s\n",
            "#177 Loss: 5.7721 Acc: 56.9361% Time: 1352.7939s\n",
            "#178 Loss: 4.7654 Acc: 56.9860% Time: 1360.4031s\n",
            "#179 Loss: 4.2589 Acc: 57.5349% Time: 1367.9438s\n",
            "#180 Loss: 4.2904 Acc: 56.9860% Time: 1375.5801s\n",
            "#181 Loss: 5.0140 Acc: 54.7904% Time: 1383.2457s\n",
            "#182 Loss: 4.6934 Acc: 57.1357% Time: 1390.9812s\n",
            "#183 Loss: 5.5339 Acc: 57.4850% Time: 1398.5762s\n",
            "#184 Loss: 4.4769 Acc: 57.5349% Time: 1406.2595s\n",
            "#185 Loss: 5.0989 Acc: 56.1876% Time: 1413.7796s\n",
            "#186 Loss: 5.8430 Acc: 54.3912% Time: 1421.4196s\n",
            "#187 Loss: 4.7811 Acc: 56.2874% Time: 1429.2025s\n",
            "#188 Loss: 5.1513 Acc: 56.2375% Time: 1436.7559s\n",
            "#189 Loss: 4.2252 Acc: 58.1836% Time: 1444.3722s\n",
            "#190 Loss: 4.8197 Acc: 57.0858% Time: 1451.8873s\n",
            "#191 Loss: 5.4038 Acc: 56.4371% Time: 1459.3845s\n",
            "#192 Loss: 3.9346 Acc: 56.6367% Time: 1466.9107s\n",
            "#193 Loss: 4.3366 Acc: 57.7844% Time: 1474.5171s\n",
            "#194 Loss: 4.1155 Acc: 59.1317% Time: 1482.0679s\n",
            "#195 Loss: 4.5771 Acc: 57.7844% Time: 1489.5349s\n",
            "#196 Loss: 4.9856 Acc: 56.5868% Time: 1497.0938s\n",
            "#197 Loss: 4.6631 Acc: 58.5329% Time: 1504.6967s\n",
            "#198 Loss: 4.8240 Acc: 55.2894% Time: 1512.3650s\n",
            "#199 Loss: 4.0387 Acc: 59.3812% Time: 1519.8953s\n",
            "#200 Loss: 4.6273 Acc: 57.1856% Time: 1527.4977s\n",
            "#201 Loss: 4.5896 Acc: 56.2874% Time: 1535.0879s\n",
            "#202 Loss: 6.3849 Acc: 55.8383% Time: 1542.6868s\n",
            "#203 Loss: 4.7309 Acc: 56.8862% Time: 1550.3599s\n",
            "#204 Loss: 4.8600 Acc: 58.1836% Time: 1557.8427s\n",
            "#205 Loss: 4.9836 Acc: 57.7345% Time: 1565.2869s\n",
            "#206 Loss: 4.4020 Acc: 58.0838% Time: 1572.8492s\n",
            "#207 Loss: 5.0587 Acc: 56.0878% Time: 1580.4786s\n",
            "#208 Loss: 5.3771 Acc: 54.6906% Time: 1588.0714s\n",
            "#209 Loss: 5.2272 Acc: 56.2375% Time: 1595.6572s\n",
            "#210 Loss: 4.8462 Acc: 57.1856% Time: 1603.2645s\n",
            "#211 Loss: 6.0580 Acc: 55.2894% Time: 1610.8862s\n",
            "#212 Loss: 5.7487 Acc: 58.1337% Time: 1618.4545s\n",
            "#213 Loss: 5.8068 Acc: 56.8862% Time: 1625.8861s\n",
            "#214 Loss: 4.3821 Acc: 58.7325% Time: 1633.4286s\n",
            "#215 Loss: 5.1064 Acc: 56.4870% Time: 1641.0423s\n",
            "#216 Loss: 4.3647 Acc: 58.4830% Time: 1648.7242s\n",
            "#217 Loss: 4.5080 Acc: 56.5369% Time: 1656.2513s\n",
            "#218 Loss: 5.3143 Acc: 55.7884% Time: 1663.8706s\n",
            "#219 Loss: 5.0373 Acc: 56.1876% Time: 1671.4335s\n",
            "#220 Loss: 4.7660 Acc: 55.7385% Time: 1679.0603s\n",
            "#221 Loss: 6.0135 Acc: 55.9381% Time: 1686.6963s\n",
            "#222 Loss: 4.6765 Acc: 56.9860% Time: 1694.2670s\n",
            "#223 Loss: 5.2520 Acc: 57.8343% Time: 1701.8397s\n",
            "#224 Loss: 5.2591 Acc: 56.1876% Time: 1709.3791s\n",
            "#225 Loss: 4.6367 Acc: 59.1317% Time: 1716.8936s\n",
            "#226 Loss: 5.1026 Acc: 57.9840% Time: 1724.4427s\n",
            "#227 Loss: 4.9948 Acc: 54.9401% Time: 1732.0573s\n",
            "#228 Loss: 4.6613 Acc: 57.5349% Time: 1739.4057s\n",
            "#229 Loss: 4.6833 Acc: 55.1397% Time: 1747.0470s\n",
            "#230 Loss: 4.6601 Acc: 56.3373% Time: 1754.6170s\n",
            "#231 Loss: 5.4971 Acc: 56.6866% Time: 1762.1870s\n",
            "#232 Loss: 4.7205 Acc: 58.6826% Time: 1769.8437s\n",
            "#233 Loss: 4.4677 Acc: 56.1876% Time: 1777.4240s\n",
            "#234 Loss: 4.5194 Acc: 58.3333% Time: 1785.0280s\n",
            "#235 Loss: 4.5398 Acc: 56.8862% Time: 1792.5579s\n",
            "#236 Loss: 4.6377 Acc: 57.6347% Time: 1800.1997s\n",
            "#237 Loss: 4.6691 Acc: 58.6826% Time: 1807.6086s\n",
            "#238 Loss: 5.0663 Acc: 58.1337% Time: 1815.1856s\n",
            "#239 Loss: 4.2194 Acc: 61.1277% Time: 1822.7491s\n",
            "#240 Loss: 5.4128 Acc: 57.2355% Time: 1830.2041s\n",
            "#241 Loss: 4.8791 Acc: 58.0838% Time: 1837.7304s\n",
            "#242 Loss: 4.5642 Acc: 57.9341% Time: 1845.2236s\n",
            "#243 Loss: 4.7739 Acc: 57.4850% Time: 1852.8220s\n",
            "#244 Loss: 5.4893 Acc: 56.5369% Time: 1860.4387s\n",
            "#245 Loss: 4.4918 Acc: 56.2375% Time: 1868.0132s\n",
            "#246 Loss: 3.9712 Acc: 58.5828% Time: 1875.6002s\n",
            "#247 Loss: 4.8992 Acc: 57.1357% Time: 1883.2209s\n",
            "#248 Loss: 4.8555 Acc: 58.2335% Time: 1890.7841s\n",
            "#249 Loss: 5.8872 Acc: 55.0898% Time: 1898.3926s\n",
            "#250 Loss: 5.2235 Acc: 56.0878% Time: 1906.0085s\n",
            "#251 Loss: 5.9904 Acc: 55.7385% Time: 1913.5136s\n",
            "#252 Loss: 5.2081 Acc: 55.2894% Time: 1921.1491s\n",
            "#253 Loss: 4.6546 Acc: 56.2874% Time: 1928.7715s\n",
            "#254 Loss: 5.5117 Acc: 54.6407% Time: 1936.3439s\n",
            "#255 Loss: 4.7680 Acc: 57.1856% Time: 1943.7425s\n",
            "#256 Loss: 5.0699 Acc: 55.5389% Time: 1951.3710s\n",
            "#257 Loss: 5.2789 Acc: 55.7385% Time: 1959.0097s\n",
            "#258 Loss: 6.0551 Acc: 56.0379% Time: 1966.5507s\n",
            "#259 Loss: 5.0375 Acc: 57.0359% Time: 1974.1470s\n",
            "#260 Loss: 3.9300 Acc: 59.9301% Time: 1981.6735s\n",
            "#261 Loss: 4.7437 Acc: 58.4331% Time: 1989.3298s\n",
            "#262 Loss: 5.8092 Acc: 56.0379% Time: 1996.9082s\n",
            "#263 Loss: 4.7071 Acc: 58.1836% Time: 2004.5288s\n",
            "#264 Loss: 4.8842 Acc: 56.3872% Time: 2012.1531s\n",
            "#265 Loss: 4.1952 Acc: 58.4830% Time: 2019.5111s\n",
            "#266 Loss: 4.8097 Acc: 56.6866% Time: 2026.9445s\n",
            "#267 Loss: 4.8190 Acc: 57.8842% Time: 2034.5465s\n",
            "#268 Loss: 4.3638 Acc: 59.4810% Time: 2042.2267s\n",
            "#269 Loss: 5.1972 Acc: 56.5369% Time: 2049.6795s\n",
            "#270 Loss: 5.8702 Acc: 56.4371% Time: 2057.1933s\n",
            "#271 Loss: 4.8414 Acc: 58.0838% Time: 2064.7455s\n",
            "#272 Loss: 5.1687 Acc: 55.5389% Time: 2072.3105s\n",
            "#273 Loss: 5.7393 Acc: 55.5888% Time: 2079.9132s\n",
            "#274 Loss: 4.9479 Acc: 57.1856% Time: 2087.5185s\n",
            "#275 Loss: 4.7119 Acc: 60.1297% Time: 2095.0544s\n",
            "#276 Loss: 5.0425 Acc: 57.3353% Time: 2102.5703s\n",
            "#277 Loss: 4.2884 Acc: 56.9860% Time: 2110.0964s\n",
            "#278 Loss: 5.4742 Acc: 57.6846% Time: 2117.6818s\n",
            "#279 Loss: 4.6508 Acc: 56.9860% Time: 2125.1410s\n",
            "#280 Loss: 4.5257 Acc: 56.0878% Time: 2132.7475s\n",
            "#281 Loss: 4.8016 Acc: 57.8343% Time: 2140.2653s\n",
            "#282 Loss: 5.7553 Acc: 58.3333% Time: 2147.6935s\n",
            "#283 Loss: 4.4852 Acc: 56.6866% Time: 2155.3758s\n",
            "#284 Loss: 5.5125 Acc: 56.9860% Time: 2162.9100s\n",
            "#285 Loss: 4.6422 Acc: 57.0858% Time: 2170.4687s\n",
            "#286 Loss: 5.4372 Acc: 55.9381% Time: 2178.0338s\n",
            "#287 Loss: 4.7734 Acc: 57.0359% Time: 2185.5031s\n",
            "#288 Loss: 5.8126 Acc: 56.2874% Time: 2193.0757s\n",
            "#289 Loss: 4.5460 Acc: 57.3852% Time: 2200.6423s\n",
            "#290 Loss: 4.2465 Acc: 57.7844% Time: 2208.0972s\n",
            "#291 Loss: 4.3038 Acc: 57.0858% Time: 2215.6728s\n",
            "#292 Loss: 4.6115 Acc: 56.1876% Time: 2223.2727s\n",
            "#293 Loss: 4.1771 Acc: 58.4331% Time: 2230.8763s\n",
            "#294 Loss: 4.5596 Acc: 56.6367% Time: 2238.2746s\n",
            "#295 Loss: 5.1281 Acc: 56.5369% Time: 2245.8746s\n",
            "#296 Loss: 4.6843 Acc: 57.0359% Time: 2253.4268s\n",
            "#297 Loss: 5.5180 Acc: 55.8882% Time: 2261.0434s\n",
            "#298 Loss: 4.4277 Acc: 56.1876% Time: 2268.6877s\n",
            "#299 Loss: 4.3758 Acc: 55.4391% Time: 2276.2641s\n",
            "#300 Loss: 5.4881 Acc: 57.6846% Time: 2283.8410s\n",
            "#301 Loss: 4.8800 Acc: 58.4830% Time: 2291.4254s\n",
            "#302 Loss: 5.4435 Acc: 57.4351% Time: 2298.9927s\n",
            "#303 Loss: 4.3383 Acc: 57.5349% Time: 2306.6601s\n",
            "#304 Loss: 5.6377 Acc: 55.7385% Time: 2314.2254s\n",
            "#305 Loss: 5.3045 Acc: 56.5369% Time: 2321.7943s\n",
            "#306 Loss: 4.9970 Acc: 57.6347% Time: 2329.4076s\n",
            "#307 Loss: 4.7104 Acc: 55.5888% Time: 2337.0296s\n",
            "#308 Loss: 5.6525 Acc: 56.6367% Time: 2344.5859s\n",
            "#309 Loss: 4.6134 Acc: 58.0838% Time: 2352.1050s\n",
            "#310 Loss: 4.4412 Acc: 57.6347% Time: 2359.5875s\n",
            "#311 Loss: 4.5001 Acc: 57.7345% Time: 2367.2403s\n",
            "#312 Loss: 5.0210 Acc: 57.9341% Time: 2374.8404s\n",
            "#313 Loss: 5.2054 Acc: 56.6367% Time: 2382.4039s\n",
            "#314 Loss: 4.8153 Acc: 55.9880% Time: 2390.0882s\n",
            "#315 Loss: 4.7403 Acc: 56.0379% Time: 2397.6303s\n",
            "#316 Loss: 4.3070 Acc: 55.9381% Time: 2405.2006s\n",
            "#317 Loss: 4.0399 Acc: 57.9341% Time: 2412.8532s\n",
            "#318 Loss: 4.4810 Acc: 55.6886% Time: 2420.3751s\n",
            "#319 Loss: 4.7601 Acc: 56.4371% Time: 2427.8546s\n",
            "#320 Loss: 4.1612 Acc: 58.5828% Time: 2435.3727s\n",
            "#321 Loss: 5.1982 Acc: 56.9361% Time: 2442.9913s\n",
            "#322 Loss: 4.0653 Acc: 60.0299% Time: 2450.5943s\n",
            "#323 Loss: 4.5961 Acc: 58.4830% Time: 2458.1899s\n",
            "#324 Loss: 4.4516 Acc: 57.4351% Time: 2465.7912s\n",
            "#325 Loss: 5.9216 Acc: 57.3353% Time: 2473.3396s\n",
            "#326 Loss: 4.5333 Acc: 57.1856% Time: 2480.8112s\n",
            "#327 Loss: 5.6781 Acc: 56.7365% Time: 2488.4316s\n",
            "#328 Loss: 4.7072 Acc: 58.2834% Time: 2496.0156s\n",
            "#329 Loss: 4.9638 Acc: 57.0858% Time: 2503.5602s\n",
            "#330 Loss: 4.1261 Acc: 57.8343% Time: 2511.1665s\n",
            "#331 Loss: 4.3078 Acc: 55.4890% Time: 2518.7913s\n",
            "#332 Loss: 5.3276 Acc: 56.6866% Time: 2526.3852s\n",
            "#333 Loss: 4.8781 Acc: 55.7884% Time: 2534.0264s\n",
            "#334 Loss: 5.9328 Acc: 54.4411% Time: 2541.5731s\n",
            "#335 Loss: 4.6960 Acc: 58.5329% Time: 2549.1312s\n",
            "#336 Loss: 4.7838 Acc: 56.6866% Time: 2556.7322s\n",
            "#337 Loss: 5.3492 Acc: 55.5888% Time: 2564.3654s\n",
            "#338 Loss: 3.9554 Acc: 58.3832% Time: 2571.7436s\n",
            "#339 Loss: 5.5542 Acc: 57.5349% Time: 2579.3478s\n",
            "#340 Loss: 4.5761 Acc: 57.1856% Time: 2586.9039s\n",
            "#341 Loss: 4.2521 Acc: 58.9321% Time: 2594.5431s\n",
            "#342 Loss: 5.5933 Acc: 58.1836% Time: 2602.1734s\n",
            "#343 Loss: 4.4566 Acc: 56.3373% Time: 2609.5765s\n",
            "#344 Loss: 5.4373 Acc: 56.4870% Time: 2617.1348s\n",
            "#345 Loss: 4.5362 Acc: 56.6866% Time: 2624.7169s\n",
            "#346 Loss: 3.6430 Acc: 59.5808% Time: 2632.1793s\n",
            "#347 Loss: 4.8965 Acc: 56.4371% Time: 2639.7806s\n",
            "#348 Loss: 4.7126 Acc: 57.8343% Time: 2647.3537s\n",
            "#349 Loss: 4.6500 Acc: 56.2375% Time: 2654.9104s\n",
            "#350 Loss: 4.1463 Acc: 58.0838% Time: 2662.5273s\n",
            "#351 Loss: 4.4812 Acc: 56.6866% Time: 2669.9784s\n",
            "#352 Loss: 5.1859 Acc: 55.4391% Time: 2677.6006s\n",
            "#353 Loss: 5.1984 Acc: 57.1856% Time: 2685.1370s\n",
            "#354 Loss: 4.6751 Acc: 59.0319% Time: 2692.7654s\n",
            "#355 Loss: 5.4461 Acc: 56.9361% Time: 2700.2386s\n",
            "#356 Loss: 4.4493 Acc: 57.6846% Time: 2707.7663s\n",
            "#357 Loss: 4.9310 Acc: 57.5848% Time: 2715.2972s\n",
            "#358 Loss: 5.2430 Acc: 56.6367% Time: 2722.7674s\n",
            "#359 Loss: 5.9898 Acc: 54.3912% Time: 2730.2971s\n",
            "#360 Loss: 4.9753 Acc: 55.9880% Time: 2737.9026s\n",
            "#361 Loss: 5.3410 Acc: 55.5888% Time: 2745.3526s\n",
            "#362 Loss: 4.9886 Acc: 58.9321% Time: 2752.9461s\n",
            "#363 Loss: 4.0372 Acc: 59.8802% Time: 2760.5165s\n",
            "#364 Loss: 4.6806 Acc: 56.7864% Time: 2768.1294s\n",
            "#365 Loss: 4.7688 Acc: 57.7844% Time: 2775.7117s\n",
            "#366 Loss: 6.6978 Acc: 53.8423% Time: 2783.2764s\n",
            "#367 Loss: 5.3459 Acc: 55.4890% Time: 2790.8391s\n",
            "#368 Loss: 4.7757 Acc: 56.6367% Time: 2798.4507s\n",
            "#369 Loss: 6.0045 Acc: 55.8882% Time: 2805.8398s\n",
            "#370 Loss: 4.1132 Acc: 59.1317% Time: 2813.4998s\n",
            "#371 Loss: 4.6261 Acc: 57.3852% Time: 2821.0896s\n",
            "#372 Loss: 3.9181 Acc: 57.2355% Time: 2828.6743s\n",
            "#373 Loss: 4.6998 Acc: 55.8882% Time: 2836.3041s\n",
            "#374 Loss: 4.0231 Acc: 59.4311% Time: 2843.8294s\n",
            "#375 Loss: 4.6797 Acc: 55.3892% Time: 2851.2982s\n",
            "#376 Loss: 5.9413 Acc: 55.8882% Time: 2858.9025s\n",
            "#377 Loss: 4.7658 Acc: 57.7844% Time: 2866.4910s\n",
            "#378 Loss: 4.3588 Acc: 57.0359% Time: 2874.0596s\n",
            "#379 Loss: 4.6681 Acc: 58.3333% Time: 2881.6834s\n",
            "#380 Loss: 4.6593 Acc: 57.0359% Time: 2889.1996s\n",
            "#381 Loss: 4.7309 Acc: 58.2335% Time: 2896.8742s\n",
            "#382 Loss: 5.0833 Acc: 54.6906% Time: 2904.4763s\n",
            "#383 Loss: 5.6563 Acc: 58.0838% Time: 2912.0041s\n",
            "#384 Loss: 5.6034 Acc: 56.4870% Time: 2919.7096s\n",
            "#385 Loss: 6.1772 Acc: 58.8822% Time: 2927.2486s\n",
            "#386 Loss: 4.3690 Acc: 57.6347% Time: 2934.7419s\n",
            "#387 Loss: 3.9914 Acc: 55.8383% Time: 2942.2329s\n",
            "#388 Loss: 5.7899 Acc: 57.1357% Time: 2949.8328s\n",
            "#389 Loss: 5.3837 Acc: 56.7864% Time: 2957.4350s\n",
            "#390 Loss: 3.9064 Acc: 58.8323% Time: 2965.0161s\n",
            "#391 Loss: 5.7098 Acc: 55.5389% Time: 2972.5877s\n",
            "#392 Loss: 4.2612 Acc: 57.7345% Time: 2980.0224s\n",
            "#393 Loss: 6.5236 Acc: 54.9900% Time: 2987.6012s\n",
            "#394 Loss: 6.0500 Acc: 55.6886% Time: 2995.0496s\n",
            "#395 Loss: 4.9719 Acc: 57.1856% Time: 3002.5693s\n",
            "#396 Loss: 5.0120 Acc: 55.8383% Time: 3010.0464s\n",
            "#397 Loss: 4.7505 Acc: 56.9860% Time: 3017.5667s\n",
            "#398 Loss: 5.2089 Acc: 57.2355% Time: 3025.0516s\n",
            "#399 Loss: 5.2084 Acc: 56.9860% Time: 3032.5814s\n",
            "#400 Loss: 4.8346 Acc: 58.3832% Time: 3040.1909s\n",
            "#401 Loss: 4.6832 Acc: 57.2355% Time: 3047.7887s\n",
            "#402 Loss: 4.1517 Acc: 58.6327% Time: 3055.2502s\n",
            "#403 Loss: 5.2003 Acc: 55.4890% Time: 3062.8448s\n",
            "#404 Loss: 6.4191 Acc: 56.9860% Time: 3070.4053s\n",
            "#405 Loss: 5.4318 Acc: 58.4830% Time: 3077.8480s\n",
            "#406 Loss: 4.3054 Acc: 56.6866% Time: 3085.3959s\n",
            "#407 Loss: 4.2545 Acc: 58.5828% Time: 3092.8593s\n",
            "#408 Loss: 4.8591 Acc: 56.0878% Time: 3100.4283s\n",
            "#409 Loss: 4.9308 Acc: 55.5888% Time: 3108.0259s\n",
            "#410 Loss: 5.0099 Acc: 58.6826% Time: 3115.5712s\n",
            "#411 Loss: 5.0040 Acc: 58.2335% Time: 3123.1905s\n",
            "#412 Loss: 4.8100 Acc: 56.7864% Time: 3130.7850s\n",
            "#413 Loss: 4.7643 Acc: 55.9381% Time: 3138.3868s\n",
            "#414 Loss: 5.2511 Acc: 57.0858% Time: 3145.9323s\n",
            "#415 Loss: 5.2751 Acc: 58.1337% Time: 3153.4151s\n",
            "#416 Loss: 5.5453 Acc: 57.5349% Time: 3160.9418s\n",
            "#417 Loss: 5.2439 Acc: 57.8842% Time: 3168.4065s\n",
            "#418 Loss: 4.8107 Acc: 58.7824% Time: 3175.9111s\n",
            "#419 Loss: 5.3488 Acc: 55.6886% Time: 3183.5834s\n",
            "#420 Loss: 4.6689 Acc: 58.9820% Time: 3191.1126s\n",
            "#421 Loss: 4.5955 Acc: 56.5369% Time: 3198.7338s\n",
            "#422 Loss: 4.8304 Acc: 56.0379% Time: 3206.4098s\n",
            "#423 Loss: 5.6819 Acc: 57.4351% Time: 3213.9154s\n",
            "#424 Loss: 5.4421 Acc: 58.2335% Time: 3221.5649s\n",
            "#425 Loss: 5.5981 Acc: 56.5868% Time: 3229.1509s\n",
            "#426 Loss: 5.6474 Acc: 56.2874% Time: 3236.7044s\n",
            "#427 Loss: 5.1846 Acc: 57.0858% Time: 3244.1535s\n",
            "#428 Loss: 5.0342 Acc: 56.8363% Time: 3251.7493s\n",
            "#429 Loss: 5.6844 Acc: 58.3333% Time: 3259.3399s\n",
            "#430 Loss: 4.8227 Acc: 56.6367% Time: 3266.9449s\n",
            "#431 Loss: 5.8835 Acc: 57.6347% Time: 3274.3222s\n",
            "#432 Loss: 4.4018 Acc: 58.1836% Time: 3281.9275s\n",
            "#433 Loss: 4.8750 Acc: 56.6367% Time: 3289.5318s\n",
            "#434 Loss: 4.8720 Acc: 58.0339% Time: 3297.0791s\n",
            "#435 Loss: 5.1338 Acc: 56.4371% Time: 3304.7187s\n",
            "#436 Loss: 4.5795 Acc: 58.4830% Time: 3312.3161s\n",
            "#437 Loss: 4.6716 Acc: 59.0319% Time: 3319.7720s\n",
            "#438 Loss: 5.6485 Acc: 54.4910% Time: 3327.3182s\n",
            "#439 Loss: 4.8846 Acc: 56.9860% Time: 3334.9300s\n",
            "#440 Loss: 5.9981 Acc: 56.4870% Time: 3342.5531s\n",
            "#441 Loss: 4.3666 Acc: 58.7824% Time: 3350.0847s\n",
            "#442 Loss: 5.0048 Acc: 58.2335% Time: 3357.7066s\n",
            "#443 Loss: 5.4795 Acc: 56.1377% Time: 3365.3150s\n",
            "#444 Loss: 4.8955 Acc: 55.8383% Time: 3372.9055s\n",
            "#445 Loss: 4.5008 Acc: 57.9341% Time: 3380.4446s\n",
            "#446 Loss: 5.5351 Acc: 56.0878% Time: 3388.1280s\n",
            "#447 Loss: 5.3589 Acc: 54.7405% Time: 3395.6992s\n",
            "#448 Loss: 4.4379 Acc: 58.9820% Time: 3403.2564s\n",
            "#449 Loss: 4.3951 Acc: 58.9321% Time: 3410.9499s\n",
            "#450 Loss: 4.3859 Acc: 58.2834% Time: 3418.4964s\n",
            "#451 Loss: 4.3756 Acc: 56.9361% Time: 3426.0504s\n",
            "#452 Loss: 5.6764 Acc: 55.3393% Time: 3433.4822s\n",
            "#453 Loss: 4.5197 Acc: 57.0359% Time: 3441.0665s\n",
            "#454 Loss: 4.9931 Acc: 58.6826% Time: 3448.5054s\n",
            "#455 Loss: 4.4796 Acc: 57.7844% Time: 3456.0963s\n",
            "#456 Loss: 4.9720 Acc: 56.4371% Time: 3463.6240s\n",
            "#457 Loss: 5.2894 Acc: 59.0319% Time: 3471.1224s\n",
            "#458 Loss: 4.8556 Acc: 58.1337% Time: 3478.6710s\n",
            "#459 Loss: 4.6191 Acc: 57.9341% Time: 3486.2961s\n",
            "#460 Loss: 5.5746 Acc: 55.0399% Time: 3493.8158s\n",
            "#461 Loss: 4.1825 Acc: 57.5349% Time: 3501.4620s\n",
            "#462 Loss: 5.0077 Acc: 57.2854% Time: 3509.0877s\n",
            "#463 Loss: 3.8358 Acc: 57.6846% Time: 3516.6174s\n",
            "#464 Loss: 4.9111 Acc: 58.1836% Time: 3524.2351s\n",
            "#465 Loss: 3.8973 Acc: 59.5808% Time: 3531.6883s\n",
            "#466 Loss: 4.1387 Acc: 57.7345% Time: 3539.2523s\n",
            "#467 Loss: 4.9035 Acc: 57.0359% Time: 3546.8241s\n",
            "#468 Loss: 5.1632 Acc: 56.5868% Time: 3554.4189s\n",
            "#469 Loss: 4.7146 Acc: 56.1377% Time: 3561.8337s\n",
            "#470 Loss: 4.0842 Acc: 58.4331% Time: 3569.2513s\n",
            "#471 Loss: 5.0487 Acc: 56.8363% Time: 3576.8426s\n",
            "#472 Loss: 5.5156 Acc: 56.0379% Time: 3584.3900s\n",
            "#473 Loss: 5.3130 Acc: 55.9381% Time: 3591.8592s\n",
            "#474 Loss: 4.6110 Acc: 58.4331% Time: 3599.3896s\n",
            "#475 Loss: 5.8122 Acc: 55.3892% Time: 3606.8397s\n",
            "#476 Loss: 3.9291 Acc: 57.8842% Time: 3614.4521s\n",
            "#477 Loss: 4.9590 Acc: 58.1337% Time: 3622.0237s\n",
            "#478 Loss: 4.5648 Acc: 56.3872% Time: 3629.6129s\n",
            "#479 Loss: 5.1155 Acc: 56.6367% Time: 3637.2290s\n",
            "#480 Loss: 5.0668 Acc: 55.2894% Time: 3644.8342s\n",
            "#481 Loss: 4.6909 Acc: 57.6846% Time: 3652.4944s\n",
            "#482 Loss: 4.3018 Acc: 56.3373% Time: 3659.9886s\n",
            "#483 Loss: 4.5646 Acc: 57.7844% Time: 3667.6031s\n",
            "#484 Loss: 4.9155 Acc: 57.3852% Time: 3675.1890s\n",
            "#485 Loss: 5.4290 Acc: 57.5349% Time: 3682.7788s\n",
            "#486 Loss: 5.0074 Acc: 57.8343% Time: 3690.4128s\n",
            "#487 Loss: 5.3306 Acc: 55.4890% Time: 3697.9922s\n",
            "#488 Loss: 4.4506 Acc: 56.5369% Time: 3705.5565s\n",
            "#489 Loss: 4.6852 Acc: 56.4371% Time: 3713.0012s\n",
            "#490 Loss: 3.9157 Acc: 56.4870% Time: 3720.5915s\n",
            "#491 Loss: 5.4851 Acc: 57.8842% Time: 3728.1663s\n",
            "#492 Loss: 3.8483 Acc: 59.3812% Time: 3735.7889s\n",
            "#493 Loss: 3.9859 Acc: 57.6846% Time: 3743.4284s\n",
            "#494 Loss: 5.1774 Acc: 54.9401% Time: 3751.0026s\n",
            "#495 Loss: 5.7275 Acc: 56.4371% Time: 3758.5965s\n",
            "#496 Loss: 5.0930 Acc: 55.7385% Time: 3766.1061s\n",
            "#497 Loss: 4.4822 Acc: 56.2874% Time: 3773.6344s\n",
            "#498 Loss: 4.9372 Acc: 55.9880% Time: 3781.1561s\n",
            "#499 Loss: 4.9560 Acc: 58.6327% Time: 3788.7612s\n",
            "#500 Loss: 4.8679 Acc: 55.9381% Time: 3796.3412s\n",
            "#501 Loss: 4.7201 Acc: 56.9860% Time: 3803.9169s\n",
            "#502 Loss: 4.6031 Acc: 57.4351% Time: 3811.4198s\n",
            "#503 Loss: 4.0302 Acc: 56.8363% Time: 3818.9365s\n",
            "#504 Loss: 6.0083 Acc: 55.1896% Time: 3826.3775s\n",
            "#505 Loss: 4.2571 Acc: 56.2874% Time: 3833.9042s\n",
            "#506 Loss: 4.4333 Acc: 57.6347% Time: 3841.5461s\n",
            "#507 Loss: 4.8364 Acc: 58.0838% Time: 3849.1450s\n",
            "#508 Loss: 4.3605 Acc: 57.2355% Time: 3856.7391s\n",
            "#509 Loss: 4.3452 Acc: 57.1856% Time: 3864.3668s\n",
            "#510 Loss: 4.7915 Acc: 57.8343% Time: 3871.9298s\n",
            "#511 Loss: 5.2308 Acc: 55.7884% Time: 3879.5714s\n",
            "#512 Loss: 5.0251 Acc: 54.8902% Time: 3887.1480s\n",
            "#513 Loss: 4.9100 Acc: 56.3872% Time: 3894.7324s\n",
            "#514 Loss: 5.0922 Acc: 55.4890% Time: 3902.1343s\n",
            "#515 Loss: 4.6034 Acc: 56.2874% Time: 3909.6845s\n",
            "#516 Loss: 4.7420 Acc: 57.5349% Time: 3917.1225s\n",
            "#517 Loss: 4.3775 Acc: 57.5349% Time: 3924.7312s\n",
            "#518 Loss: 5.0629 Acc: 56.1377% Time: 3932.2933s\n",
            "#519 Loss: 5.5230 Acc: 58.0339% Time: 3939.9095s\n",
            "#520 Loss: 5.0868 Acc: 55.2894% Time: 3947.3362s\n",
            "#521 Loss: 5.0556 Acc: 57.8343% Time: 3954.9519s\n",
            "#522 Loss: 5.4090 Acc: 56.7864% Time: 3962.5562s\n",
            "#523 Loss: 5.0560 Acc: 57.9341% Time: 3970.0727s\n",
            "#524 Loss: 4.7380 Acc: 58.0339% Time: 3977.5637s\n",
            "#525 Loss: 5.7290 Acc: 57.0359% Time: 3985.0535s\n",
            "#526 Loss: 5.1647 Acc: 56.6866% Time: 3992.5173s\n",
            "#527 Loss: 4.7497 Acc: 58.7325% Time: 4000.0840s\n",
            "#528 Loss: 4.6540 Acc: 57.3852% Time: 4007.7026s\n",
            "#529 Loss: 4.4669 Acc: 56.8862% Time: 4015.2407s\n",
            "#530 Loss: 4.5642 Acc: 56.9361% Time: 4022.7174s\n",
            "#531 Loss: 5.8039 Acc: 58.0838% Time: 4030.2872s\n",
            "#532 Loss: 5.5298 Acc: 57.2355% Time: 4037.8561s\n",
            "#533 Loss: 5.0933 Acc: 56.7864% Time: 4045.2990s\n",
            "#534 Loss: 4.3109 Acc: 58.4830% Time: 4052.8586s\n",
            "#535 Loss: 4.6123 Acc: 58.0838% Time: 4060.2842s\n",
            "#536 Loss: 4.0164 Acc: 58.3333% Time: 4067.8597s\n",
            "#537 Loss: 4.1968 Acc: 57.3852% Time: 4075.4687s\n",
            "#538 Loss: 4.5574 Acc: 57.4850% Time: 4083.0610s\n",
            "#539 Loss: 4.7260 Acc: 55.6886% Time: 4090.4882s\n",
            "#540 Loss: 4.4161 Acc: 57.2355% Time: 4098.0686s\n",
            "#541 Loss: 5.3672 Acc: 55.9880% Time: 4105.6237s\n",
            "#542 Loss: 5.7200 Acc: 55.8383% Time: 4113.2205s\n",
            "#543 Loss: 4.7916 Acc: 56.6866% Time: 4120.6590s\n",
            "#544 Loss: 4.4102 Acc: 56.8363% Time: 4128.2560s\n",
            "#545 Loss: 3.9877 Acc: 58.0339% Time: 4135.6822s\n",
            "#546 Loss: 4.0627 Acc: 59.2315% Time: 4143.2640s\n",
            "#547 Loss: 4.7307 Acc: 58.0838% Time: 4150.9461s\n",
            "#548 Loss: 4.8900 Acc: 56.9361% Time: 4158.4515s\n",
            "#549 Loss: 5.0471 Acc: 57.8343% Time: 4166.0436s\n",
            "#550 Loss: 4.4844 Acc: 58.5329% Time: 4173.6849s\n",
            "#551 Loss: 4.8937 Acc: 58.7325% Time: 4181.2106s\n",
            "#552 Loss: 5.0210 Acc: 56.5369% Time: 4188.6696s\n",
            "#553 Loss: 4.6066 Acc: 58.6826% Time: 4196.2537s\n",
            "#554 Loss: 4.4955 Acc: 57.4850% Time: 4203.8463s\n",
            "#555 Loss: 5.3140 Acc: 56.8862% Time: 4211.4080s\n",
            "#556 Loss: 4.2613 Acc: 59.3313% Time: 4219.0765s\n",
            "#557 Loss: 4.2107 Acc: 56.9361% Time: 4226.6378s\n",
            "#558 Loss: 4.7045 Acc: 56.5369% Time: 4234.2620s\n",
            "#559 Loss: 5.3337 Acc: 58.2335% Time: 4241.7662s\n",
            "#560 Loss: 4.6227 Acc: 59.5808% Time: 4249.2589s\n",
            "#561 Loss: 5.8608 Acc: 55.4391% Time: 4256.8033s\n",
            "#562 Loss: 6.1779 Acc: 55.2395% Time: 4264.4273s\n",
            "#563 Loss: 5.8897 Acc: 56.1876% Time: 4272.0130s\n",
            "#564 Loss: 4.9087 Acc: 55.9880% Time: 4279.4776s\n",
            "#565 Loss: 4.5670 Acc: 55.5888% Time: 4286.9861s\n",
            "#566 Loss: 4.9861 Acc: 56.5868% Time: 4294.5646s\n",
            "#567 Loss: 4.8165 Acc: 57.5848% Time: 4302.2113s\n",
            "#568 Loss: 5.7240 Acc: 56.9361% Time: 4309.7868s\n",
            "#569 Loss: 4.5510 Acc: 57.6347% Time: 4317.1975s\n",
            "#570 Loss: 5.3400 Acc: 55.5389% Time: 4324.7892s\n",
            "#571 Loss: 4.5067 Acc: 57.7844% Time: 4332.2006s\n",
            "#572 Loss: 4.9254 Acc: 57.9840% Time: 4339.5893s\n",
            "#573 Loss: 5.3351 Acc: 55.7385% Time: 4347.2469s\n",
            "#574 Loss: 5.7147 Acc: 54.6906% Time: 4354.7619s\n",
            "#575 Loss: 5.2320 Acc: 56.7365% Time: 4362.1481s\n",
            "#576 Loss: 5.8316 Acc: 53.2435% Time: 4369.7608s\n",
            "#577 Loss: 5.4464 Acc: 57.5349% Time: 4377.3891s\n",
            "#578 Loss: 5.2119 Acc: 56.8363% Time: 4384.9562s\n",
            "#579 Loss: 4.5183 Acc: 59.3313% Time: 4392.3716s\n",
            "#580 Loss: 5.0411 Acc: 57.4850% Time: 4399.8048s\n",
            "#581 Loss: 4.5056 Acc: 57.6347% Time: 4407.4034s\n",
            "#582 Loss: 4.8325 Acc: 58.1337% Time: 4414.9607s\n",
            "#583 Loss: 4.8911 Acc: 57.4351% Time: 4422.5881s\n",
            "#584 Loss: 4.7721 Acc: 56.0878% Time: 4430.0902s\n",
            "#585 Loss: 5.2372 Acc: 55.7884% Time: 4437.6316s\n",
            "#586 Loss: 5.9486 Acc: 56.5369% Time: 4445.2025s\n",
            "#587 Loss: 5.0134 Acc: 55.6387% Time: 4452.7578s\n",
            "#588 Loss: 4.5344 Acc: 58.4830% Time: 4460.3639s\n",
            "#589 Loss: 4.7472 Acc: 57.7844% Time: 4467.9742s\n",
            "#590 Loss: 3.9152 Acc: 56.5868% Time: 4475.5649s\n",
            "#591 Loss: 5.1223 Acc: 57.6347% Time: 4483.1768s\n",
            "#592 Loss: 5.3237 Acc: 56.9361% Time: 4490.7793s\n",
            "#593 Loss: 5.7733 Acc: 55.7884% Time: 4498.3466s\n",
            "#594 Loss: 4.7619 Acc: 58.8323% Time: 4505.9527s\n",
            "#595 Loss: 4.7136 Acc: 56.4870% Time: 4513.5174s\n",
            "#596 Loss: 5.3528 Acc: 57.2355% Time: 4521.1174s\n",
            "#597 Loss: 4.5775 Acc: 58.6826% Time: 4528.7035s\n",
            "#598 Loss: 5.8390 Acc: 57.2355% Time: 4536.1448s\n",
            "#599 Loss: 5.0930 Acc: 57.4850% Time: 4543.7594s\n",
            "#600 Loss: 4.4032 Acc: 58.4830% Time: 4551.2640s\n",
            "#601 Loss: 5.1041 Acc: 56.7365% Time: 4558.7194s\n",
            "#602 Loss: 5.3650 Acc: 56.6866% Time: 4566.3179s\n",
            "#603 Loss: 4.4016 Acc: 56.6866% Time: 4573.9694s\n",
            "#604 Loss: 3.8911 Acc: 58.1337% Time: 4581.5015s\n",
            "#605 Loss: 4.0331 Acc: 59.8802% Time: 4589.0659s\n",
            "#606 Loss: 5.1928 Acc: 54.9900% Time: 4596.5174s\n",
            "#607 Loss: 4.1961 Acc: 58.2335% Time: 4604.0909s\n",
            "#608 Loss: 5.3737 Acc: 57.8343% Time: 4611.6754s\n",
            "#609 Loss: 4.1464 Acc: 57.9341% Time: 4619.1567s\n",
            "#610 Loss: 5.2647 Acc: 55.3393% Time: 4626.7390s\n",
            "#611 Loss: 4.5449 Acc: 57.0858% Time: 4634.3283s\n",
            "#612 Loss: 4.5770 Acc: 56.1377% Time: 4641.8921s\n",
            "#613 Loss: 5.7801 Acc: 54.5409% Time: 4649.4881s\n",
            "#614 Loss: 4.9709 Acc: 58.2335% Time: 4657.1370s\n",
            "#615 Loss: 5.4191 Acc: 55.9381% Time: 4664.6617s\n",
            "#616 Loss: 5.0076 Acc: 56.3872% Time: 4672.2797s\n",
            "#617 Loss: 5.3416 Acc: 53.3433% Time: 4679.8537s\n",
            "#618 Loss: 5.4895 Acc: 57.5349% Time: 4687.3283s\n",
            "#619 Loss: 4.8837 Acc: 58.3333% Time: 4694.8855s\n",
            "#620 Loss: 4.6453 Acc: 56.7864% Time: 4702.4450s\n",
            "#621 Loss: 4.2000 Acc: 58.7325% Time: 4709.8546s\n",
            "#622 Loss: 5.5556 Acc: 56.0379% Time: 4717.5506s\n",
            "#623 Loss: 5.0610 Acc: 56.5868% Time: 4725.0582s\n",
            "#624 Loss: 4.7142 Acc: 58.2834% Time: 4732.7096s\n",
            "#625 Loss: 5.5230 Acc: 56.1876% Time: 4740.2611s\n",
            "#626 Loss: 4.9245 Acc: 57.7844% Time: 4747.8824s\n",
            "#627 Loss: 4.7426 Acc: 56.2375% Time: 4755.5398s\n",
            "#628 Loss: 4.5234 Acc: 58.3832% Time: 4763.0168s\n",
            "#629 Loss: 4.7815 Acc: 58.1337% Time: 4770.4778s\n",
            "#630 Loss: 5.3406 Acc: 55.4391% Time: 4778.0743s\n",
            "#631 Loss: 4.4731 Acc: 56.6866% Time: 4785.6746s\n",
            "#632 Loss: 5.3740 Acc: 56.4371% Time: 4793.2203s\n",
            "#633 Loss: 4.4969 Acc: 58.3333% Time: 4800.8484s\n",
            "#634 Loss: 5.2367 Acc: 56.9361% Time: 4808.3969s\n",
            "#635 Loss: 4.8712 Acc: 57.1856% Time: 4815.8704s\n",
            "#636 Loss: 6.2058 Acc: 55.5888% Time: 4823.4673s\n",
            "#637 Loss: 4.8052 Acc: 57.5848% Time: 4830.8722s\n",
            "#638 Loss: 4.9464 Acc: 58.2834% Time: 4838.4038s\n",
            "#639 Loss: 4.6611 Acc: 57.8343% Time: 4846.0182s\n",
            "#640 Loss: 5.2233 Acc: 57.2854% Time: 4853.5798s\n",
            "#641 Loss: 4.6911 Acc: 57.0858% Time: 4861.0807s\n",
            "#642 Loss: 5.1720 Acc: 55.6387% Time: 4868.6145s\n",
            "#643 Loss: 4.8648 Acc: 57.5848% Time: 4876.2122s\n",
            "#644 Loss: 4.8260 Acc: 56.2375% Time: 4883.6387s\n",
            "#645 Loss: 4.6299 Acc: 57.8842% Time: 4891.2565s\n",
            "#646 Loss: 5.1880 Acc: 55.7884% Time: 4898.7409s\n",
            "#647 Loss: 4.9380 Acc: 58.0339% Time: 4906.2358s\n",
            "#648 Loss: 4.1623 Acc: 57.5848% Time: 4913.7656s\n",
            "#649 Loss: 4.7225 Acc: 57.2355% Time: 4921.1989s\n",
            "#650 Loss: 4.5609 Acc: 56.7365% Time: 4928.7947s\n",
            "#651 Loss: 4.9281 Acc: 57.9840% Time: 4936.3823s\n",
            "#652 Loss: 4.4943 Acc: 57.1357% Time: 4943.8096s\n",
            "#653 Loss: 4.9832 Acc: 56.4371% Time: 4951.3358s\n",
            "#654 Loss: 4.5766 Acc: 56.9860% Time: 4958.9909s\n",
            "#655 Loss: 5.7200 Acc: 56.0379% Time: 4966.5938s\n",
            "#656 Loss: 4.6205 Acc: 57.8343% Time: 4974.1907s\n",
            "#657 Loss: 4.5726 Acc: 57.2854% Time: 4981.7877s\n",
            "#658 Loss: 5.9758 Acc: 56.9860% Time: 4989.3581s\n",
            "#659 Loss: 4.3782 Acc: 59.9800% Time: 4996.9282s\n",
            "#660 Loss: 4.5972 Acc: 58.4331% Time: 5004.4025s\n",
            "#661 Loss: 4.7845 Acc: 58.6327% Time: 5011.8985s\n",
            "#662 Loss: 4.0565 Acc: 58.4331% Time: 5019.4001s\n",
            "#663 Loss: 4.3684 Acc: 58.9321% Time: 5026.8933s\n",
            "#664 Loss: 5.9619 Acc: 56.7365% Time: 5034.4136s\n",
            "#665 Loss: 4.6762 Acc: 56.5369% Time: 5041.9469s\n",
            "#666 Loss: 4.6741 Acc: 57.5848% Time: 5049.5509s\n",
            "#667 Loss: 4.8518 Acc: 56.8363% Time: 5057.1367s\n",
            "#668 Loss: 4.1548 Acc: 58.9820% Time: 5064.7505s\n",
            "#669 Loss: 4.8838 Acc: 57.7345% Time: 5072.4439s\n",
            "#670 Loss: 5.2131 Acc: 55.1397% Time: 5080.0064s\n",
            "#671 Loss: 4.7515 Acc: 57.1856% Time: 5087.5957s\n",
            "#672 Loss: 4.7511 Acc: 55.9381% Time: 5095.1405s\n",
            "#673 Loss: 5.1335 Acc: 55.6886% Time: 5102.5543s\n",
            "#674 Loss: 4.7553 Acc: 55.8882% Time: 5110.1277s\n",
            "#675 Loss: 6.0537 Acc: 56.6866% Time: 5117.7320s\n",
            "#676 Loss: 4.0141 Acc: 58.8822% Time: 5125.2907s\n",
            "#677 Loss: 4.7560 Acc: 57.5848% Time: 5132.9120s\n",
            "#678 Loss: 5.1750 Acc: 56.8363% Time: 5140.4966s\n",
            "#679 Loss: 4.7951 Acc: 57.2355% Time: 5148.0886s\n",
            "#680 Loss: 5.7675 Acc: 56.8862% Time: 5155.7552s\n",
            "#681 Loss: 5.0886 Acc: 55.8383% Time: 5163.3207s\n",
            "#682 Loss: 4.7198 Acc: 57.5848% Time: 5170.8640s\n",
            "#683 Loss: 5.7227 Acc: 56.4870% Time: 5178.5091s\n",
            "#684 Loss: 5.5475 Acc: 56.8862% Time: 5186.1137s\n",
            "#685 Loss: 5.7028 Acc: 56.3373% Time: 5193.7372s\n",
            "#686 Loss: 4.3791 Acc: 57.3353% Time: 5201.3187s\n",
            "#687 Loss: 5.2312 Acc: 58.5329% Time: 5208.9279s\n",
            "#688 Loss: 5.2694 Acc: 55.8383% Time: 5216.4492s\n",
            "#689 Loss: 5.7160 Acc: 55.5888% Time: 5223.9593s\n",
            "#690 Loss: 3.7417 Acc: 59.5808% Time: 5231.4757s\n",
            "#691 Loss: 4.6167 Acc: 56.5868% Time: 5239.0861s\n",
            "#692 Loss: 4.5583 Acc: 56.4371% Time: 5246.7225s\n",
            "#693 Loss: 5.0170 Acc: 57.8842% Time: 5254.2946s\n",
            "#694 Loss: 4.6090 Acc: 57.5848% Time: 5261.9253s\n",
            "#695 Loss: 4.5872 Acc: 57.0858% Time: 5269.4614s\n",
            "#696 Loss: 4.4804 Acc: 59.0319% Time: 5277.0809s\n",
            "#697 Loss: 4.5057 Acc: 58.1337% Time: 5284.6979s\n",
            "#698 Loss: 4.3826 Acc: 57.5848% Time: 5292.2838s\n",
            "#699 Loss: 5.7260 Acc: 55.5888% Time: 5299.9487s\n",
            "#700 Loss: 5.1737 Acc: 55.6387% Time: 5307.4690s\n",
            "#701 Loss: 4.7098 Acc: 57.8343% Time: 5315.1012s\n",
            "#702 Loss: 4.6479 Acc: 56.8862% Time: 5322.6493s\n",
            "#703 Loss: 4.1948 Acc: 57.1357% Time: 5330.1165s\n",
            "#704 Loss: 4.9341 Acc: 57.4351% Time: 5337.6279s\n",
            "#705 Loss: 5.8457 Acc: 54.0918% Time: 5345.0916s\n",
            "#706 Loss: 6.0240 Acc: 54.9401% Time: 5352.6426s\n",
            "#707 Loss: 4.2283 Acc: 59.0818% Time: 5360.2471s\n",
            "#708 Loss: 4.6732 Acc: 57.1856% Time: 5367.8971s\n",
            "#709 Loss: 4.2256 Acc: 56.4371% Time: 5375.4129s\n",
            "#710 Loss: 4.8452 Acc: 55.6886% Time: 5382.8894s\n",
            "#711 Loss: 4.7152 Acc: 58.9321% Time: 5390.4427s\n",
            "#712 Loss: 5.2009 Acc: 56.7864% Time: 5398.0956s\n",
            "#713 Loss: 4.8160 Acc: 57.7844% Time: 5405.6906s\n",
            "#714 Loss: 5.3764 Acc: 56.2375% Time: 5413.2736s\n",
            "#715 Loss: 5.7519 Acc: 57.2854% Time: 5420.9013s\n",
            "#716 Loss: 5.4977 Acc: 57.2355% Time: 5428.4787s\n",
            "#717 Loss: 4.9953 Acc: 57.9840% Time: 5436.0624s\n",
            "#718 Loss: 4.2006 Acc: 57.5848% Time: 5443.5888s\n",
            "#719 Loss: 4.0385 Acc: 59.2315% Time: 5451.0805s\n",
            "#720 Loss: 5.1456 Acc: 56.4870% Time: 5458.5980s\n",
            "#721 Loss: 5.0713 Acc: 56.3373% Time: 5466.2359s\n",
            "#722 Loss: 4.2771 Acc: 57.3852% Time: 5473.7947s\n",
            "#723 Loss: 5.8454 Acc: 55.5888% Time: 5481.4522s\n",
            "#724 Loss: 5.3977 Acc: 56.6866% Time: 5488.9699s\n",
            "#725 Loss: 4.2965 Acc: 57.2854% Time: 5496.4308s\n",
            "#726 Loss: 4.7250 Acc: 58.4331% Time: 5503.7895s\n",
            "#727 Loss: 4.8311 Acc: 58.7325% Time: 5511.4119s\n",
            "#728 Loss: 4.7879 Acc: 57.6846% Time: 5518.9990s\n",
            "#729 Loss: 4.7855 Acc: 57.3353% Time: 5526.6022s\n",
            "#730 Loss: 5.4238 Acc: 58.0339% Time: 5534.3118s\n",
            "#731 Loss: 6.3792 Acc: 57.1856% Time: 5541.8361s\n",
            "#732 Loss: 6.8486 Acc: 58.7824% Time: 5549.3935s\n",
            "#733 Loss: 5.5987 Acc: 57.2854% Time: 5556.9718s\n",
            "#734 Loss: 4.6329 Acc: 58.7824% Time: 5564.3982s\n",
            "#735 Loss: 5.6576 Acc: 53.9421% Time: 5572.0229s\n",
            "#736 Loss: 5.0827 Acc: 57.1357% Time: 5579.6007s\n",
            "#737 Loss: 4.3350 Acc: 57.7844% Time: 5587.1482s\n",
            "#738 Loss: 5.1187 Acc: 56.5369% Time: 5594.7583s\n",
            "#739 Loss: 5.1789 Acc: 56.3872% Time: 5602.1875s\n",
            "#740 Loss: 4.9637 Acc: 55.8882% Time: 5609.7976s\n",
            "#741 Loss: 4.5013 Acc: 57.1856% Time: 5617.4394s\n",
            "#742 Loss: 4.4255 Acc: 57.1357% Time: 5624.9502s\n",
            "#743 Loss: 4.7464 Acc: 56.7864% Time: 5632.5788s\n",
            "#744 Loss: 5.0051 Acc: 56.0878% Time: 5640.1741s\n",
            "#745 Loss: 5.5240 Acc: 55.3393% Time: 5647.7611s\n",
            "#746 Loss: 5.1112 Acc: 57.8343% Time: 5655.4048s\n",
            "#747 Loss: 5.0914 Acc: 57.1856% Time: 5662.9638s\n",
            "#748 Loss: 4.9811 Acc: 56.9361% Time: 5670.5579s\n",
            "#749 Loss: 4.6154 Acc: 57.7844% Time: 5678.1258s\n",
            "#750 Loss: 4.5575 Acc: 55.2395% Time: 5685.8491s\n",
            "#751 Loss: 5.3416 Acc: 56.0878% Time: 5693.6288s\n",
            "#752 Loss: 4.4490 Acc: 58.0838% Time: 5701.1450s\n",
            "#753 Loss: 4.4476 Acc: 56.7365% Time: 5708.8497s\n",
            "#754 Loss: 4.7112 Acc: 55.8383% Time: 5716.7767s\n",
            "#755 Loss: 4.5682 Acc: 58.1337% Time: 5724.7537s\n",
            "#756 Loss: 4.7923 Acc: 56.1377% Time: 5732.6212s\n",
            "#757 Loss: 4.5491 Acc: 54.8902% Time: 5740.5474s\n",
            "#758 Loss: 4.0013 Acc: 58.1337% Time: 5748.1883s\n",
            "#759 Loss: 4.8083 Acc: 55.2894% Time: 5755.7798s\n",
            "#760 Loss: 4.0423 Acc: 57.2854% Time: 5763.6350s\n",
            "#761 Loss: 4.4050 Acc: 58.2834% Time: 5771.2004s\n",
            "#762 Loss: 5.3561 Acc: 55.8882% Time: 5778.9156s\n",
            "#763 Loss: 5.3041 Acc: 57.6846% Time: 5786.6297s\n",
            "#764 Loss: 5.0228 Acc: 57.5848% Time: 5794.0972s\n",
            "#765 Loss: 4.6496 Acc: 56.6367% Time: 5801.7516s\n",
            "#766 Loss: 5.3900 Acc: 55.6886% Time: 5809.2702s\n",
            "#767 Loss: 5.0642 Acc: 59.2814% Time: 5816.8851s\n",
            "#768 Loss: 5.1544 Acc: 57.8842% Time: 5824.4857s\n",
            "#769 Loss: 4.7629 Acc: 59.6307% Time: 5832.2726s\n",
            "#770 Loss: 5.0830 Acc: 56.4371% Time: 5839.7716s\n",
            "#771 Loss: 4.6788 Acc: 58.5329% Time: 5847.4039s\n",
            "#772 Loss: 4.9553 Acc: 57.2854% Time: 5855.2643s\n",
            "#773 Loss: 4.7432 Acc: 56.5868% Time: 5863.0035s\n",
            "#774 Loss: 4.4408 Acc: 57.0359% Time: 5870.7932s\n",
            "#775 Loss: 4.7273 Acc: 56.3872% Time: 5880.9112s\n",
            "#776 Loss: 5.4291 Acc: 56.1876% Time: 5890.8408s\n",
            "#777 Loss: 4.7630 Acc: 55.8882% Time: 5900.7081s\n",
            "#778 Loss: 3.8360 Acc: 58.6826% Time: 5910.6252s\n",
            "#779 Loss: 5.1438 Acc: 57.5848% Time: 5920.7248s\n",
            "#780 Loss: 4.3322 Acc: 59.9301% Time: 5930.5058s\n",
            "#781 Loss: 4.5572 Acc: 58.1337% Time: 5938.9860s\n",
            "#782 Loss: 4.7800 Acc: 55.6886% Time: 5946.8514s\n",
            "#783 Loss: 5.0161 Acc: 56.2375% Time: 5954.5120s\n",
            "#784 Loss: 4.8145 Acc: 57.5848% Time: 5962.2645s\n",
            "#785 Loss: 5.0978 Acc: 57.1856% Time: 5972.1410s\n",
            "#786 Loss: 5.5736 Acc: 56.3373% Time: 5980.0710s\n",
            "#787 Loss: 5.1155 Acc: 55.4890% Time: 5987.6598s\n",
            "#788 Loss: 4.3653 Acc: 58.4830% Time: 5995.3932s\n",
            "#789 Loss: 4.4986 Acc: 56.9860% Time: 6005.4925s\n",
            "#790 Loss: 4.9302 Acc: 56.0379% Time: 6013.1652s\n",
            "#791 Loss: 5.0420 Acc: 55.8882% Time: 6021.0396s\n",
            "#792 Loss: 4.7126 Acc: 56.8862% Time: 6028.7955s\n",
            "#793 Loss: 4.4986 Acc: 57.7345% Time: 6036.5617s\n",
            "#794 Loss: 5.3546 Acc: 55.3393% Time: 6044.2452s\n",
            "#795 Loss: 5.4530 Acc: 58.0339% Time: 6051.8560s\n",
            "#796 Loss: 4.3008 Acc: 55.4890% Time: 6059.4929s\n",
            "#797 Loss: 4.4718 Acc: 58.0339% Time: 6067.1722s\n",
            "#798 Loss: 4.6235 Acc: 57.7844% Time: 6075.0107s\n",
            "#799 Loss: 5.1393 Acc: 56.3872% Time: 6082.6548s\n",
            "#800 Loss: 4.6318 Acc: 57.1856% Time: 6090.3221s\n",
            "#801 Loss: 4.6648 Acc: 57.3353% Time: 6098.1615s\n",
            "#802 Loss: 4.8795 Acc: 57.0359% Time: 6108.1492s\n",
            "#803 Loss: 5.0308 Acc: 57.2355% Time: 6117.9166s\n",
            "#804 Loss: 5.4572 Acc: 56.0878% Time: 6127.6203s\n",
            "#805 Loss: 4.6052 Acc: 58.2834% Time: 6137.4378s\n",
            "#806 Loss: 4.7942 Acc: 56.9361% Time: 6147.4065s\n",
            "#807 Loss: 5.1417 Acc: 57.0858% Time: 6157.6764s\n",
            "#808 Loss: 5.2031 Acc: 55.0898% Time: 6167.5205s\n",
            "#809 Loss: 4.5760 Acc: 56.9361% Time: 6177.3188s\n",
            "#810 Loss: 5.7203 Acc: 57.3852% Time: 6185.7185s\n",
            "#811 Loss: 4.8013 Acc: 56.0379% Time: 6193.4709s\n",
            "#812 Loss: 4.8052 Acc: 58.6826% Time: 6201.0669s\n",
            "#813 Loss: 4.8879 Acc: 55.9880% Time: 6208.6099s\n",
            "#814 Loss: 4.6590 Acc: 58.2335% Time: 6216.3822s\n",
            "#815 Loss: 4.3754 Acc: 57.3852% Time: 6226.5765s\n",
            "#816 Loss: 4.1827 Acc: 58.0339% Time: 6236.7690s\n",
            "#817 Loss: 4.6708 Acc: 55.1397% Time: 6246.6592s\n",
            "#818 Loss: 4.5101 Acc: 56.7864% Time: 6254.2567s\n",
            "#819 Loss: 4.2700 Acc: 55.6387% Time: 6261.9348s\n",
            "#820 Loss: 4.5337 Acc: 56.7365% Time: 6269.4203s\n",
            "#821 Loss: 4.5293 Acc: 55.4391% Time: 6276.8946s\n",
            "#822 Loss: 4.4487 Acc: 58.1337% Time: 6284.3331s\n",
            "#823 Loss: 4.2815 Acc: 55.9880% Time: 6291.7979s\n",
            "#824 Loss: 4.8981 Acc: 56.6866% Time: 6299.2829s\n",
            "#825 Loss: 4.0678 Acc: 57.7345% Time: 6306.7849s\n",
            "#826 Loss: 5.0477 Acc: 56.8363% Time: 6314.3182s\n",
            "#827 Loss: 5.2428 Acc: 56.0878% Time: 6321.7355s\n",
            "#828 Loss: 4.6148 Acc: 55.9880% Time: 6329.3381s\n",
            "#829 Loss: 4.3960 Acc: 59.8303% Time: 6336.9022s\n",
            "#830 Loss: 4.6416 Acc: 56.9361% Time: 6344.5740s\n",
            "#831 Loss: 4.9757 Acc: 56.8862% Time: 6352.1280s\n",
            "#832 Loss: 4.5321 Acc: 56.1876% Time: 6359.7237s\n",
            "#833 Loss: 4.8996 Acc: 55.5888% Time: 6367.2870s\n",
            "#834 Loss: 4.8682 Acc: 57.1856% Time: 6374.9568s\n",
            "#835 Loss: 5.9191 Acc: 58.0838% Time: 6382.3301s\n",
            "#836 Loss: 4.6893 Acc: 57.9840% Time: 6389.9647s\n",
            "#837 Loss: 5.0864 Acc: 55.8383% Time: 6397.4200s\n",
            "#838 Loss: 5.3984 Acc: 56.2375% Time: 6404.9784s\n",
            "#839 Loss: 5.2208 Acc: 57.2854% Time: 6412.4788s\n",
            "#840 Loss: 5.5538 Acc: 56.2375% Time: 6419.9476s\n",
            "#841 Loss: 4.7824 Acc: 54.9401% Time: 6427.5020s\n",
            "#842 Loss: 4.1923 Acc: 58.2834% Time: 6435.0802s\n",
            "#843 Loss: 4.9984 Acc: 57.3353% Time: 6442.5402s\n",
            "#844 Loss: 5.3757 Acc: 59.1816% Time: 6450.1464s\n",
            "#845 Loss: 4.7194 Acc: 56.8862% Time: 6457.6814s\n",
            "#846 Loss: 5.0315 Acc: 57.2854% Time: 6465.1135s\n",
            "#847 Loss: 4.6188 Acc: 56.6866% Time: 6472.6617s\n",
            "#848 Loss: 4.9462 Acc: 57.2355% Time: 6480.2568s\n",
            "#849 Loss: 4.9346 Acc: 57.6347% Time: 6487.6975s\n",
            "#850 Loss: 5.4375 Acc: 57.9840% Time: 6495.3452s\n",
            "#851 Loss: 4.6956 Acc: 56.7864% Time: 6502.9117s\n",
            "#852 Loss: 4.1146 Acc: 58.1337% Time: 6510.2835s\n",
            "#853 Loss: 5.1505 Acc: 55.4391% Time: 6517.7638s\n",
            "#854 Loss: 4.9961 Acc: 59.8303% Time: 6525.3133s\n",
            "#855 Loss: 4.1888 Acc: 56.4371% Time: 6532.9717s\n",
            "#856 Loss: 5.0085 Acc: 57.4351% Time: 6540.4981s\n",
            "#857 Loss: 4.8919 Acc: 55.5389% Time: 6548.0699s\n",
            "#858 Loss: 3.9777 Acc: 59.2814% Time: 6555.5121s\n",
            "#859 Loss: 5.2300 Acc: 55.6387% Time: 6563.0492s\n",
            "#860 Loss: 5.6232 Acc: 56.5369% Time: 6570.6846s\n",
            "#861 Loss: 6.3661 Acc: 55.4391% Time: 6578.2817s\n",
            "#862 Loss: 5.3853 Acc: 57.4351% Time: 6585.8477s\n",
            "#863 Loss: 4.8305 Acc: 55.1397% Time: 6593.4868s\n",
            "#864 Loss: 5.8834 Acc: 56.9361% Time: 6601.0339s\n",
            "#865 Loss: 5.2414 Acc: 56.7864% Time: 6608.5975s\n",
            "#866 Loss: 4.8712 Acc: 57.0359% Time: 6616.2425s\n",
            "#867 Loss: 4.8712 Acc: 56.8862% Time: 6623.8484s\n",
            "#868 Loss: 5.1119 Acc: 57.7345% Time: 6631.2824s\n",
            "#869 Loss: 5.0954 Acc: 56.7365% Time: 6638.8397s\n",
            "#870 Loss: 4.7175 Acc: 57.7844% Time: 6646.2398s\n",
            "#871 Loss: 4.6183 Acc: 56.5868% Time: 6653.8145s\n",
            "#872 Loss: 5.2970 Acc: 58.0339% Time: 6661.2495s\n",
            "#873 Loss: 5.3090 Acc: 57.6846% Time: 6668.8583s\n",
            "#874 Loss: 4.4592 Acc: 56.8862% Time: 6676.4385s\n",
            "#875 Loss: 4.5075 Acc: 56.2375% Time: 6684.0436s\n",
            "#876 Loss: 5.3240 Acc: 54.6407% Time: 6691.6207s\n",
            "#877 Loss: 4.8790 Acc: 59.2315% Time: 6699.1876s\n",
            "#878 Loss: 5.0937 Acc: 54.8902% Time: 6706.6810s\n",
            "#879 Loss: 5.2138 Acc: 56.7365% Time: 6714.1741s\n",
            "#880 Loss: 5.3624 Acc: 56.6866% Time: 6721.6676s\n",
            "#881 Loss: 5.1975 Acc: 58.5329% Time: 6729.2227s\n",
            "#882 Loss: 4.7418 Acc: 58.2335% Time: 6736.8405s\n",
            "#883 Loss: 5.2973 Acc: 58.1337% Time: 6744.4063s\n",
            "#884 Loss: 4.1609 Acc: 57.6347% Time: 6751.8565s\n",
            "#885 Loss: 4.3076 Acc: 57.6846% Time: 6759.3855s\n",
            "#886 Loss: 6.0498 Acc: 55.2395% Time: 6766.8465s\n",
            "#887 Loss: 5.0422 Acc: 57.7345% Time: 6774.3862s\n",
            "#888 Loss: 4.6736 Acc: 57.9840% Time: 6781.9636s\n",
            "#889 Loss: 4.7032 Acc: 55.9880% Time: 6789.5515s\n",
            "#890 Loss: 4.9199 Acc: 56.6866% Time: 6797.1862s\n",
            "#891 Loss: 4.7089 Acc: 55.9381% Time: 6804.7019s\n",
            "#892 Loss: 4.1433 Acc: 56.6866% Time: 6812.3539s\n",
            "#893 Loss: 5.3210 Acc: 57.7844% Time: 6820.0002s\n",
            "#894 Loss: 4.8812 Acc: 56.0878% Time: 6827.5437s\n",
            "#895 Loss: 6.3879 Acc: 54.9401% Time: 6835.0354s\n",
            "#896 Loss: 5.8308 Acc: 57.2355% Time: 6842.5838s\n",
            "#897 Loss: 4.5843 Acc: 57.9341% Time: 6850.1765s\n",
            "#898 Loss: 4.8236 Acc: 57.0359% Time: 6857.6159s\n",
            "#899 Loss: 4.1992 Acc: 57.5848% Time: 6865.1947s\n",
            "#900 Loss: 4.2554 Acc: 58.1836% Time: 6872.5785s\n",
            "#901 Loss: 4.2953 Acc: 58.7325% Time: 6880.2190s\n",
            "#902 Loss: 4.1129 Acc: 57.8842% Time: 6887.7534s\n",
            "#903 Loss: 4.0485 Acc: 57.9840% Time: 6895.3407s\n",
            "#904 Loss: 4.4255 Acc: 56.3872% Time: 6902.7497s\n",
            "#905 Loss: 5.4211 Acc: 57.7844% Time: 6910.1630s\n",
            "#906 Loss: 5.6405 Acc: 57.0359% Time: 6917.7233s\n",
            "#907 Loss: 5.1412 Acc: 56.9361% Time: 6925.2042s\n",
            "#908 Loss: 4.8610 Acc: 57.2854% Time: 6932.7309s\n",
            "#909 Loss: 5.1122 Acc: 56.4870% Time: 6940.2300s\n",
            "#910 Loss: 5.0572 Acc: 57.4351% Time: 6947.7565s\n",
            "#911 Loss: 4.4597 Acc: 58.0838% Time: 6955.3154s\n",
            "#912 Loss: 4.9193 Acc: 57.8343% Time: 6962.9501s\n",
            "#913 Loss: 5.5376 Acc: 55.1896% Time: 6970.5378s\n",
            "#914 Loss: 4.6916 Acc: 55.5888% Time: 6978.0933s\n",
            "#915 Loss: 5.2960 Acc: 56.5868% Time: 6985.5445s\n",
            "#916 Loss: 4.7287 Acc: 57.4850% Time: 6993.0839s\n",
            "#917 Loss: 4.7338 Acc: 58.1836% Time: 7000.6877s\n",
            "#918 Loss: 4.5342 Acc: 55.9381% Time: 7008.1581s\n",
            "#919 Loss: 5.4240 Acc: 55.6387% Time: 7015.7564s\n",
            "#920 Loss: 4.9600 Acc: 54.9401% Time: 7023.3859s\n",
            "#921 Loss: 5.5177 Acc: 55.2894% Time: 7030.9238s\n",
            "#922 Loss: 4.2329 Acc: 60.5289% Time: 7038.5645s\n",
            "#923 Loss: 5.5593 Acc: 55.6886% Time: 7046.1072s\n",
            "#924 Loss: 4.6748 Acc: 57.3852% Time: 7053.6685s\n",
            "#925 Loss: 4.3662 Acc: 56.5369% Time: 7061.2716s\n",
            "#926 Loss: 5.5250 Acc: 57.1357% Time: 7068.7003s\n",
            "#927 Loss: 5.3234 Acc: 58.8822% Time: 7076.1184s\n",
            "#928 Loss: 4.1070 Acc: 58.7824% Time: 7083.7187s\n",
            "#929 Loss: 4.2614 Acc: 57.7345% Time: 7091.3139s\n",
            "#930 Loss: 4.7750 Acc: 56.3373% Time: 7098.8865s\n",
            "#931 Loss: 5.1653 Acc: 57.1357% Time: 7106.3977s\n",
            "#932 Loss: 4.7293 Acc: 56.3373% Time: 7113.9476s\n",
            "#933 Loss: 4.5767 Acc: 57.6846% Time: 7121.4261s\n",
            "#934 Loss: 5.7186 Acc: 56.6866% Time: 7129.0476s\n",
            "#935 Loss: 4.2221 Acc: 57.5848% Time: 7136.4853s\n",
            "#936 Loss: 4.7678 Acc: 56.8363% Time: 7144.1370s\n",
            "#937 Loss: 4.4034 Acc: 55.8882% Time: 7151.7204s\n",
            "#938 Loss: 4.4065 Acc: 57.2854% Time: 7159.2934s\n",
            "#939 Loss: 4.5349 Acc: 58.5828% Time: 7166.9115s\n",
            "#940 Loss: 5.1064 Acc: 56.3373% Time: 7174.4365s\n",
            "#941 Loss: 4.5277 Acc: 57.8842% Time: 7181.9013s\n",
            "#942 Loss: 4.5937 Acc: 57.1357% Time: 7189.4757s\n",
            "#943 Loss: 4.7905 Acc: 56.2874% Time: 7197.0585s\n",
            "#944 Loss: 5.2309 Acc: 55.4890% Time: 7204.4747s\n",
            "#945 Loss: 5.7297 Acc: 57.4351% Time: 7211.9944s\n",
            "#946 Loss: 5.2079 Acc: 56.7365% Time: 7219.4507s\n",
            "#947 Loss: 5.2255 Acc: 57.5848% Time: 7227.0472s\n",
            "#948 Loss: 5.3915 Acc: 56.3872% Time: 7234.6382s\n",
            "#949 Loss: 5.4530 Acc: 55.6886% Time: 7242.2256s\n",
            "#950 Loss: 5.2337 Acc: 55.4391% Time: 7249.7636s\n",
            "#951 Loss: 4.4789 Acc: 57.2355% Time: 7257.2504s\n",
            "#952 Loss: 5.3364 Acc: 56.1377% Time: 7264.7550s\n",
            "#953 Loss: 4.7182 Acc: 56.9860% Time: 7272.4246s\n",
            "#954 Loss: 5.0748 Acc: 55.7884% Time: 7280.0844s\n",
            "#955 Loss: 4.4264 Acc: 56.7864% Time: 7287.6153s\n",
            "#956 Loss: 4.0303 Acc: 56.8862% Time: 7295.2274s\n",
            "#957 Loss: 4.6666 Acc: 55.4890% Time: 7302.8033s\n",
            "#958 Loss: 3.7170 Acc: 57.6347% Time: 7310.2726s\n",
            "#959 Loss: 4.5866 Acc: 57.7345% Time: 7317.7872s\n",
            "#960 Loss: 4.4303 Acc: 56.4371% Time: 7325.4310s\n",
            "#961 Loss: 5.8378 Acc: 54.4910% Time: 7332.8392s\n",
            "#962 Loss: 4.6919 Acc: 57.2355% Time: 7340.4167s\n",
            "#963 Loss: 5.7535 Acc: 55.6886% Time: 7347.8126s\n",
            "#964 Loss: 4.9148 Acc: 57.4351% Time: 7355.3786s\n",
            "#965 Loss: 5.0746 Acc: 57.0858% Time: 7363.0147s\n",
            "#966 Loss: 4.2481 Acc: 58.9820% Time: 7370.5972s\n",
            "#967 Loss: 4.6513 Acc: 56.6866% Time: 7377.9900s\n",
            "#968 Loss: 4.2797 Acc: 57.6347% Time: 7385.5707s\n",
            "#969 Loss: 5.9452 Acc: 56.1377% Time: 7392.9965s\n",
            "#970 Loss: 4.9517 Acc: 56.8862% Time: 7400.3836s\n",
            "#971 Loss: 4.4356 Acc: 58.4830% Time: 7407.8133s\n",
            "#972 Loss: 4.1646 Acc: 56.7365% Time: 7415.5071s\n",
            "#973 Loss: 5.2204 Acc: 54.6407% Time: 7423.2927s\n",
            "#974 Loss: 4.5635 Acc: 56.2874% Time: 7431.0888s\n",
            "#975 Loss: 4.3706 Acc: 58.0339% Time: 7438.8620s\n",
            "#976 Loss: 4.4267 Acc: 56.6367% Time: 7446.6667s\n",
            "#977 Loss: 4.0403 Acc: 57.6347% Time: 7454.2053s\n",
            "#978 Loss: 5.2666 Acc: 55.6886% Time: 7461.7451s\n",
            "#979 Loss: 4.9416 Acc: 58.0339% Time: 7469.1639s\n",
            "#980 Loss: 5.0284 Acc: 58.8323% Time: 7476.7558s\n",
            "#981 Loss: 5.3545 Acc: 55.5389% Time: 7484.3586s\n",
            "#982 Loss: 4.0765 Acc: 57.1856% Time: 7491.7264s\n",
            "#983 Loss: 4.0790 Acc: 56.5868% Time: 7499.2029s\n",
            "#984 Loss: 4.3022 Acc: 55.5888% Time: 7506.8651s\n",
            "#985 Loss: 3.8509 Acc: 58.1836% Time: 7514.6179s\n",
            "#986 Loss: 4.1941 Acc: 55.9880% Time: 7522.4467s\n",
            "#987 Loss: 5.1287 Acc: 57.0858% Time: 7532.2113s\n",
            "#988 Loss: 4.7190 Acc: 56.7864% Time: 7541.7056s\n",
            "#989 Loss: 4.8283 Acc: 56.2874% Time: 7551.0786s\n",
            "#990 Loss: 4.5365 Acc: 56.9361% Time: 7560.7006s\n",
            "#991 Loss: 4.5856 Acc: 57.2854% Time: 7570.2811s\n",
            "#992 Loss: 4.1541 Acc: 59.4810% Time: 7579.6626s\n",
            "#993 Loss: 5.9981 Acc: 55.6387% Time: 7587.1310s\n",
            "#994 Loss: 4.7917 Acc: 55.5888% Time: 7594.6910s\n",
            "#995 Loss: 5.8413 Acc: 57.1856% Time: 7602.1398s\n",
            "#996 Loss: 4.6330 Acc: 57.8842% Time: 7609.7143s\n",
            "#997 Loss: 7.2556 Acc: 54.9900% Time: 7617.3690s\n",
            "#998 Loss: 4.5813 Acc: 57.7844% Time: 7626.5105s\n",
            "#999 Loss: 4.7870 Acc: 57.5848% Time: 7636.2361s\n"
          ]
        }
      ],
      "source": [
        "# num_epochs = 200\n",
        "# model.train()\n",
        "# start_time = time.time()\n",
        "# writer = SummaryWriter()\n",
        "\n",
        "# # 전체 반복(epoch) 수 만큼 반복하며\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.\n",
        "#     running_corrects = 0\n",
        "\n",
        "#     # 배치 단위로 학습 데이터 불러오기\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         # 모델에 입력(forward)하고 결과 계산\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss = running_loss / len(train_datasets)\n",
        "#     epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "#     writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"acc/train\", epoch_acc, epoch)\n",
        "#     writer.add_scalar(\"sum/train\", epoch_loss, epoch)\n",
        "#     # writer.add_scalar(\"sum/train\", epoch_acc/100, epoch)\n",
        "\n",
        "#     # 학습 과정 중에 결과 출력\n",
        "#     print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "# writer.flush()\n",
        "# writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Loss: 0.7131 Acc: 52.3952% Time: 7.0728s\n",
            "#1 Loss: 0.6968 Acc: 53.2934% Time: 14.2389s\n",
            "#2 Loss: 0.6983 Acc: 54.4411% Time: 21.2877s\n",
            "#3 Loss: 0.6846 Acc: 55.6886% Time: 28.1868s\n",
            "#4 Loss: 0.6783 Acc: 55.6387% Time: 35.1104s\n",
            "#5 Loss: 0.6689 Acc: 58.6826% Time: 42.0342s\n",
            "#6 Loss: 0.6736 Acc: 58.0339% Time: 48.9134s\n",
            "#7 Loss: 0.6630 Acc: 59.8802% Time: 55.7491s\n",
            "#8 Loss: 0.6585 Acc: 59.8802% Time: 62.5428s\n",
            "#9 Loss: 0.6569 Acc: 59.8802% Time: 69.4347s\n",
            "#10 Loss: 0.6542 Acc: 62.2256% Time: 76.3129s\n",
            "#11 Loss: 0.6508 Acc: 61.6267% Time: 83.0944s\n",
            "#12 Loss: 0.6462 Acc: 62.3253% Time: 89.8470s\n",
            "#13 Loss: 0.6449 Acc: 62.0758% Time: 96.6960s\n",
            "#14 Loss: 0.6454 Acc: 63.0240% Time: 103.4993s\n",
            "#15 Loss: 0.6388 Acc: 63.7226% Time: 110.3084s\n",
            "#16 Loss: 0.6490 Acc: 61.0778% Time: 117.2505s\n",
            "#17 Loss: 0.6354 Acc: 65.0699% Time: 124.1203s\n",
            "#18 Loss: 0.6390 Acc: 64.3713% Time: 130.8848s\n",
            "#19 Loss: 0.6351 Acc: 63.7725% Time: 137.6730s\n",
            "#20 Loss: 0.6326 Acc: 64.1717% Time: 144.5241s\n",
            "#21 Loss: 0.6347 Acc: 64.2715% Time: 151.3059s\n",
            "#22 Loss: 0.6360 Acc: 63.3234% Time: 158.0579s\n",
            "#23 Loss: 0.6322 Acc: 63.8224% Time: 164.9223s\n",
            "#24 Loss: 0.6272 Acc: 65.0200% Time: 171.8590s\n",
            "#25 Loss: 0.6308 Acc: 63.6727% Time: 178.7146s\n",
            "#26 Loss: 0.6246 Acc: 64.4711% Time: 185.6059s\n",
            "#27 Loss: 0.6223 Acc: 64.5709% Time: 192.4812s\n",
            "#28 Loss: 0.6202 Acc: 65.7186% Time: 199.3124s\n",
            "#29 Loss: 0.6290 Acc: 64.6208% Time: 206.0763s\n",
            "#30 Loss: 0.6211 Acc: 65.6188% Time: 212.9419s\n",
            "#31 Loss: 0.6134 Acc: 66.8164% Time: 219.5686s\n",
            "#32 Loss: 0.6143 Acc: 66.0679% Time: 226.4162s\n",
            "#33 Loss: 0.6203 Acc: 65.2695% Time: 233.2859s\n",
            "#34 Loss: 0.6123 Acc: 66.7665% Time: 240.0270s\n",
            "#35 Loss: 0.6216 Acc: 65.0699% Time: 246.8760s\n",
            "#36 Loss: 0.6174 Acc: 65.5689% Time: 253.6615s\n",
            "#37 Loss: 0.6148 Acc: 66.1677% Time: 260.4426s\n",
            "#38 Loss: 0.6098 Acc: 67.0659% Time: 267.3743s\n",
            "#39 Loss: 0.6165 Acc: 65.9681% Time: 274.2438s\n",
            "#40 Loss: 0.6115 Acc: 65.7186% Time: 280.9973s\n",
            "#41 Loss: 0.6101 Acc: 64.9202% Time: 287.8094s\n",
            "#42 Loss: 0.6013 Acc: 68.4132% Time: 294.5759s\n",
            "#43 Loss: 0.6096 Acc: 65.2695% Time: 301.3976s\n",
            "#44 Loss: 0.6074 Acc: 65.8184% Time: 308.1853s\n",
            "#45 Loss: 0.6021 Acc: 68.0639% Time: 314.9581s\n",
            "#46 Loss: 0.6019 Acc: 66.7166% Time: 321.7963s\n",
            "#47 Loss: 0.6114 Acc: 65.6687% Time: 328.6233s\n",
            "#48 Loss: 0.6081 Acc: 66.5669% Time: 335.4317s\n",
            "#49 Loss: 0.6042 Acc: 66.5170% Time: 342.1366s\n",
            "#50 Loss: 0.5994 Acc: 68.0639% Time: 349.0019s\n",
            "#51 Loss: 0.5948 Acc: 68.3633% Time: 355.7893s\n",
            "#52 Loss: 0.5988 Acc: 67.7645% Time: 362.6175s\n",
            "#53 Loss: 0.6067 Acc: 66.3174% Time: 369.4248s\n",
            "#54 Loss: 0.5982 Acc: 67.5150% Time: 376.3326s\n",
            "#55 Loss: 0.6017 Acc: 66.8663% Time: 383.1985s\n",
            "#56 Loss: 0.6009 Acc: 66.7166% Time: 389.9889s\n",
            "#57 Loss: 0.6033 Acc: 67.3653% Time: 396.7782s\n",
            "#58 Loss: 0.6001 Acc: 67.4152% Time: 403.5709s\n",
            "#59 Loss: 0.6026 Acc: 67.0659% Time: 410.3848s\n",
            "#60 Loss: 0.6066 Acc: 66.5170% Time: 417.1263s\n",
            "#61 Loss: 0.5979 Acc: 67.4651% Time: 423.9100s\n",
            "#62 Loss: 0.5991 Acc: 67.7146% Time: 430.7389s\n",
            "#63 Loss: 0.6029 Acc: 66.2176% Time: 437.5704s\n",
            "#64 Loss: 0.5996 Acc: 66.9661% Time: 444.3901s\n",
            "#65 Loss: 0.5967 Acc: 67.6647% Time: 451.2010s\n",
            "#66 Loss: 0.5929 Acc: 68.4631% Time: 458.1375s\n",
            "#67 Loss: 0.5955 Acc: 67.9641% Time: 464.9608s\n",
            "#68 Loss: 0.5970 Acc: 67.0160% Time: 471.7363s\n",
            "#69 Loss: 0.5930 Acc: 68.5130% Time: 478.5290s\n",
            "#70 Loss: 0.5998 Acc: 67.4651% Time: 485.3426s\n",
            "#71 Loss: 0.6003 Acc: 67.4152% Time: 492.1791s\n",
            "#72 Loss: 0.5962 Acc: 68.1138% Time: 498.9359s\n",
            "#73 Loss: 0.5971 Acc: 67.8643% Time: 505.7364s\n",
            "#74 Loss: 0.5951 Acc: 68.4132% Time: 512.5634s\n",
            "#75 Loss: 0.5950 Acc: 68.4631% Time: 519.3437s\n",
            "#76 Loss: 0.5919 Acc: 68.8124% Time: 526.0895s\n",
            "#77 Loss: 0.5929 Acc: 67.2655% Time: 532.8985s\n",
            "#78 Loss: 0.5984 Acc: 67.5150% Time: 539.7380s\n",
            "#79 Loss: 0.5896 Acc: 68.4631% Time: 546.5497s\n",
            "#80 Loss: 0.5855 Acc: 69.1118% Time: 553.3369s\n",
            "#81 Loss: 0.5825 Acc: 69.8603% Time: 560.1469s\n",
            "#82 Loss: 0.5932 Acc: 68.4132% Time: 566.8796s\n",
            "#83 Loss: 0.5955 Acc: 68.3134% Time: 573.7083s\n",
            "#84 Loss: 0.5833 Acc: 68.2635% Time: 580.5611s\n",
            "#85 Loss: 0.5853 Acc: 69.4112% Time: 587.3043s\n",
            "#86 Loss: 0.5901 Acc: 68.1637% Time: 594.1677s\n",
            "#87 Loss: 0.5901 Acc: 69.4611% Time: 600.9596s\n",
            "#88 Loss: 0.5965 Acc: 67.3154% Time: 607.7312s\n",
            "#89 Loss: 0.5820 Acc: 69.6607% Time: 614.4573s\n",
            "#90 Loss: 0.5920 Acc: 68.4132% Time: 621.3534s\n",
            "#91 Loss: 0.5808 Acc: 69.5110% Time: 628.1029s\n",
            "#92 Loss: 0.5910 Acc: 67.4651% Time: 634.9192s\n",
            "#93 Loss: 0.5894 Acc: 68.3134% Time: 641.7145s\n",
            "#94 Loss: 0.5823 Acc: 68.8124% Time: 648.5674s\n",
            "#95 Loss: 0.5842 Acc: 68.9621% Time: 655.3132s\n",
            "#96 Loss: 0.5877 Acc: 68.6128% Time: 662.1438s\n",
            "#97 Loss: 0.5939 Acc: 69.0619% Time: 668.9192s\n",
            "#98 Loss: 0.5783 Acc: 69.8104% Time: 675.7368s\n",
            "#99 Loss: 0.5857 Acc: 68.6128% Time: 682.5336s\n",
            "#100 Loss: 0.5872 Acc: 68.7126% Time: 689.3160s\n",
            "#101 Loss: 0.5828 Acc: 69.2116% Time: 696.1098s\n",
            "#102 Loss: 0.5797 Acc: 69.6607% Time: 702.9103s\n",
            "#103 Loss: 0.5911 Acc: 67.9641% Time: 709.6949s\n",
            "#104 Loss: 0.5757 Acc: 69.5609% Time: 716.5149s\n",
            "#105 Loss: 0.5849 Acc: 69.4611% Time: 723.2544s\n",
            "#106 Loss: 0.5907 Acc: 69.0120% Time: 730.0583s\n",
            "#107 Loss: 0.5815 Acc: 68.8623% Time: 736.8570s\n",
            "#108 Loss: 0.5864 Acc: 68.7625% Time: 743.7171s\n",
            "#109 Loss: 0.5781 Acc: 68.8623% Time: 750.5406s\n",
            "#110 Loss: 0.5826 Acc: 69.7106% Time: 757.4743s\n",
            "#111 Loss: 0.5806 Acc: 69.6108% Time: 764.4690s\n",
            "#112 Loss: 0.5826 Acc: 68.6627% Time: 771.2572s\n",
            "#113 Loss: 0.5844 Acc: 68.2635% Time: 778.1203s\n",
            "#114 Loss: 0.5826 Acc: 69.3114% Time: 785.1188s\n",
            "#115 Loss: 0.5751 Acc: 69.2116% Time: 792.0198s\n",
            "#116 Loss: 0.5882 Acc: 68.4631% Time: 798.7650s\n",
            "#117 Loss: 0.5859 Acc: 67.9641% Time: 805.6910s\n",
            "#118 Loss: 0.5821 Acc: 68.8623% Time: 812.4465s\n",
            "#119 Loss: 0.5783 Acc: 68.8124% Time: 819.2610s\n",
            "#120 Loss: 0.5759 Acc: 69.7106% Time: 826.0226s\n",
            "#121 Loss: 0.5762 Acc: 69.8104% Time: 832.9108s\n",
            "#122 Loss: 0.5796 Acc: 69.5609% Time: 839.8243s\n",
            "#123 Loss: 0.5708 Acc: 70.7585% Time: 846.6870s\n",
            "#124 Loss: 0.5800 Acc: 70.2096% Time: 853.5854s\n",
            "#125 Loss: 0.5735 Acc: 70.3094% Time: 860.3490s\n",
            "#126 Loss: 0.5722 Acc: 69.9102% Time: 867.1698s\n",
            "#127 Loss: 0.5816 Acc: 68.5130% Time: 873.9682s\n",
            "#128 Loss: 0.5855 Acc: 67.8643% Time: 880.7839s\n",
            "#129 Loss: 0.5766 Acc: 69.4611% Time: 887.6049s\n",
            "#130 Loss: 0.5787 Acc: 69.2116% Time: 894.3707s\n",
            "#131 Loss: 0.5778 Acc: 69.7605% Time: 901.2124s\n",
            "#132 Loss: 0.5746 Acc: 69.5609% Time: 907.9501s\n",
            "#133 Loss: 0.5751 Acc: 69.1617% Time: 914.7765s\n",
            "#134 Loss: 0.5789 Acc: 68.8623% Time: 921.6551s\n",
            "#135 Loss: 0.5843 Acc: 68.9122% Time: 928.6292s\n",
            "#136 Loss: 0.5696 Acc: 70.1597% Time: 935.5886s\n",
            "#137 Loss: 0.5744 Acc: 68.9621% Time: 942.6124s\n",
            "#138 Loss: 0.5795 Acc: 70.4591% Time: 949.4109s\n",
            "#139 Loss: 0.5662 Acc: 70.1597% Time: 956.3455s\n",
            "#140 Loss: 0.5760 Acc: 68.7126% Time: 963.1159s\n",
            "#141 Loss: 0.5733 Acc: 70.4092% Time: 969.9473s\n",
            "#142 Loss: 0.5697 Acc: 70.3593% Time: 976.7626s\n",
            "#143 Loss: 0.5709 Acc: 70.0599% Time: 983.6856s\n",
            "#144 Loss: 0.5776 Acc: 69.7605% Time: 990.5539s\n",
            "#145 Loss: 0.5759 Acc: 69.3114% Time: 997.3284s\n",
            "#146 Loss: 0.5774 Acc: 69.7605% Time: 1004.0796s\n",
            "#147 Loss: 0.5683 Acc: 70.8084% Time: 1010.9487s\n",
            "#148 Loss: 0.5831 Acc: 67.9641% Time: 1017.8561s\n",
            "#149 Loss: 0.5706 Acc: 69.3613% Time: 1024.8320s\n",
            "#150 Loss: 0.5704 Acc: 72.0060% Time: 1031.9649s\n",
            "#151 Loss: 0.5698 Acc: 70.3094% Time: 1038.6827s\n",
            "#152 Loss: 0.5719 Acc: 69.9102% Time: 1045.4964s\n",
            "#153 Loss: 0.5697 Acc: 69.8603% Time: 1052.2861s\n",
            "#154 Loss: 0.5678 Acc: 70.4092% Time: 1058.9818s\n",
            "#155 Loss: 0.5734 Acc: 70.9581% Time: 1065.6663s\n",
            "#156 Loss: 0.5708 Acc: 69.5609% Time: 1072.4180s\n",
            "#157 Loss: 0.5677 Acc: 70.5589% Time: 1079.0298s\n",
            "#158 Loss: 0.5806 Acc: 69.3613% Time: 1085.7196s\n",
            "#159 Loss: 0.5795 Acc: 69.3613% Time: 1092.3645s\n",
            "#160 Loss: 0.5696 Acc: 69.2116% Time: 1099.0769s\n",
            "#161 Loss: 0.5655 Acc: 69.8104% Time: 1105.9818s\n",
            "#162 Loss: 0.5738 Acc: 69.2116% Time: 1112.7163s\n",
            "#163 Loss: 0.5813 Acc: 68.9621% Time: 1119.3744s\n",
            "#164 Loss: 0.5754 Acc: 69.6108% Time: 1126.0940s\n",
            "#165 Loss: 0.5725 Acc: 70.6587% Time: 1132.8530s\n",
            "#166 Loss: 0.5722 Acc: 70.0599% Time: 1139.5077s\n",
            "#167 Loss: 0.5641 Acc: 70.7086% Time: 1146.3941s\n",
            "#168 Loss: 0.5615 Acc: 69.7106% Time: 1153.2503s\n",
            "#169 Loss: 0.5753 Acc: 70.5589% Time: 1159.8985s\n",
            "#170 Loss: 0.5735 Acc: 69.7605% Time: 1166.6160s\n",
            "#171 Loss: 0.5573 Acc: 70.9082% Time: 1173.3277s\n",
            "#172 Loss: 0.5728 Acc: 70.0599% Time: 1180.0444s\n",
            "#173 Loss: 0.5625 Acc: 70.5090% Time: 1186.7064s\n",
            "#174 Loss: 0.5770 Acc: 70.1597% Time: 1193.4952s\n",
            "#175 Loss: 0.5774 Acc: 70.1098% Time: 1200.3081s\n",
            "#176 Loss: 0.5725 Acc: 70.5589% Time: 1207.0955s\n",
            "#177 Loss: 0.5625 Acc: 71.2076% Time: 1214.0845s\n",
            "#178 Loss: 0.5679 Acc: 70.2096% Time: 1221.1148s\n",
            "#179 Loss: 0.5639 Acc: 71.3074% Time: 1228.1704s\n",
            "#180 Loss: 0.5588 Acc: 70.6587% Time: 1234.8742s\n",
            "#181 Loss: 0.5747 Acc: 69.5609% Time: 1241.6056s\n",
            "#182 Loss: 0.5647 Acc: 70.6587% Time: 1248.4130s\n",
            "#183 Loss: 0.5701 Acc: 70.6088% Time: 1255.2795s\n",
            "#184 Loss: 0.5691 Acc: 69.8104% Time: 1262.0160s\n",
            "#185 Loss: 0.5717 Acc: 70.3094% Time: 1268.6703s\n",
            "#186 Loss: 0.5707 Acc: 69.6607% Time: 1275.4087s\n",
            "#187 Loss: 0.5631 Acc: 70.7585% Time: 1282.2171s\n",
            "#188 Loss: 0.5644 Acc: 70.4591% Time: 1289.0001s\n",
            "#189 Loss: 0.5667 Acc: 70.2096% Time: 1295.7934s\n",
            "#190 Loss: 0.5648 Acc: 70.6088% Time: 1302.3925s\n",
            "#191 Loss: 0.5671 Acc: 69.8104% Time: 1309.1825s\n",
            "#192 Loss: 0.5621 Acc: 71.5070% Time: 1315.9486s\n",
            "#193 Loss: 0.5660 Acc: 70.3593% Time: 1322.6485s\n",
            "#194 Loss: 0.5629 Acc: 70.5589% Time: 1329.4273s\n",
            "#195 Loss: 0.5798 Acc: 69.3613% Time: 1336.1637s\n",
            "#196 Loss: 0.5547 Acc: 71.5070% Time: 1342.7669s\n",
            "#197 Loss: 0.5653 Acc: 70.1098% Time: 1349.3931s\n",
            "#198 Loss: 0.5739 Acc: 71.2076% Time: 1355.9855s\n",
            "#199 Loss: 0.5657 Acc: 70.1597% Time: 1362.8361s\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 200\n",
        "# model.train()\n",
        "start_time = time.time()\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# 전체 반복(epoch) 수 만큼 반복하며\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    # 배치 단위로 학습 데이터 불러오기\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # 모델에 입력(forward)하고 결과 계산\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    # 학습 과정 중에 결과 출력\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    #save\n",
        "    torch.save(model.state_dict(), f'C:/team3/resnet/models/#18 resnet_models/resnet_dict{epoch}.pth')\n",
        "\n",
        "# writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 [Test Phase] Loss: 0.7677 Acc: 42.8242% Time: 1371.3819s\n",
            "#1 [Test Phase] Loss: 0.8208 Acc: 36.7018% Time: 1379.8951s\n",
            "#2 [Test Phase] Loss: 0.6857 Acc: 55.2666% Time: 1388.2703s\n",
            "#3 [Test Phase] Loss: 0.7165 Acc: 56.4845% Time: 1396.3702s\n",
            "#4 [Test Phase] Loss: 0.6286 Acc: 65.3720% Time: 1404.5599s\n",
            "#5 [Test Phase] Loss: 0.6204 Acc: 65.9315% Time: 1412.7507s\n",
            "#6 [Test Phase] Loss: 0.6083 Acc: 67.3469% Time: 1420.9529s\n",
            "#7 [Test Phase] Loss: 0.7986 Acc: 49.9671% Time: 1429.5719s\n",
            "#8 [Test Phase] Loss: 0.7214 Acc: 56.3199% Time: 1438.0098s\n",
            "#9 [Test Phase] Loss: 0.6912 Acc: 59.0849% Time: 1446.7149s\n",
            "#10 [Test Phase] Loss: 0.6781 Acc: 60.4674% Time: 1455.5511s\n",
            "#11 [Test Phase] Loss: 0.7587 Acc: 53.7525% Time: 1464.2950s\n",
            "#12 [Test Phase] Loss: 0.7255 Acc: 56.0237% Time: 1472.8442s\n",
            "#13 [Test Phase] Loss: 0.7395 Acc: 47.5971% Time: 1481.5627s\n",
            "#14 [Test Phase] Loss: 0.6458 Acc: 63.7920% Time: 1490.0309s\n",
            "#15 [Test Phase] Loss: 0.6688 Acc: 60.9612% Time: 1498.4792s\n",
            "#16 [Test Phase] Loss: 0.7066 Acc: 58.2620% Time: 1507.0312s\n",
            "#17 [Test Phase] Loss: 0.6724 Acc: 60.9282% Time: 1515.3950s\n",
            "#18 [Test Phase] Loss: 0.6899 Acc: 59.6445% Time: 1523.7302s\n",
            "#19 [Test Phase] Loss: 0.6775 Acc: 60.1053% Time: 1531.8953s\n",
            "#20 [Test Phase] Loss: 0.5818 Acc: 69.7169% Time: 1540.3440s\n",
            "#21 [Test Phase] Loss: 0.6681 Acc: 61.1916% Time: 1548.8253s\n",
            "#22 [Test Phase] Loss: 0.7098 Acc: 57.3733% Time: 1557.2873s\n",
            "#23 [Test Phase] Loss: 0.7140 Acc: 57.8999% Time: 1566.1263s\n",
            "#24 [Test Phase] Loss: 0.6888 Acc: 55.1679% Time: 1575.1777s\n",
            "#25 [Test Phase] Loss: 0.6713 Acc: 61.7512% Time: 1583.8953s\n",
            "#26 [Test Phase] Loss: 0.6261 Acc: 65.5365% Time: 1592.9541s\n",
            "#27 [Test Phase] Loss: 0.6842 Acc: 60.5332% Time: 1601.8408s\n",
            "#28 [Test Phase] Loss: 0.6946 Acc: 59.6445% Time: 1610.3751s\n",
            "#29 [Test Phase] Loss: 0.7633 Acc: 54.2791% Time: 1619.6236s\n",
            "#30 [Test Phase] Loss: 0.5785 Acc: 70.0790% Time: 1629.2200s\n",
            "#31 [Test Phase] Loss: 0.8809 Acc: 45.9184% Time: 1638.1370s\n",
            "#32 [Test Phase] Loss: 0.6301 Acc: 64.9111% Time: 1647.1568s\n",
            "#33 [Test Phase] Loss: 0.5895 Acc: 69.4207% Time: 1655.8205s\n",
            "#34 [Test Phase] Loss: 0.6885 Acc: 60.4345% Time: 1664.5409s\n",
            "#35 [Test Phase] Loss: 0.7497 Acc: 47.3338% Time: 1673.2577s\n",
            "#36 [Test Phase] Loss: 0.6995 Acc: 59.2824% Time: 1681.9915s\n",
            "#37 [Test Phase] Loss: 0.7571 Acc: 54.4437% Time: 1690.8553s\n",
            "#38 [Test Phase] Loss: 0.7379 Acc: 55.7275% Time: 1699.3908s\n",
            "#39 [Test Phase] Loss: 0.6911 Acc: 60.2370% Time: 1707.9606s\n",
            "#40 [Test Phase] Loss: 0.6923 Acc: 59.8749% Time: 1716.2408s\n",
            "#41 [Test Phase] Loss: 0.6416 Acc: 64.3515% Time: 1724.6492s\n",
            "#42 [Test Phase] Loss: 0.6498 Acc: 63.8907% Time: 1733.0069s\n",
            "#43 [Test Phase] Loss: 0.6968 Acc: 59.2495% Time: 1741.7648s\n",
            "#44 [Test Phase] Loss: 0.5991 Acc: 68.3344% Time: 1750.1321s\n",
            "#45 [Test Phase] Loss: 0.5887 Acc: 68.7953% Time: 1758.6939s\n",
            "#46 [Test Phase] Loss: 0.6327 Acc: 62.4424% Time: 1767.4986s\n",
            "#47 [Test Phase] Loss: 0.7444 Acc: 55.8920% Time: 1776.0357s\n",
            "#48 [Test Phase] Loss: 0.6479 Acc: 63.5616% Time: 1784.6127s\n",
            "#49 [Test Phase] Loss: 0.6870 Acc: 60.6320% Time: 1793.2054s\n",
            "#50 [Test Phase] Loss: 0.5825 Acc: 69.7828% Time: 1801.7934s\n",
            "#51 [Test Phase] Loss: 0.6263 Acc: 65.5365% Time: 1810.3349s\n",
            "#52 [Test Phase] Loss: 0.7287 Acc: 57.3404% Time: 1818.9703s\n",
            "#53 [Test Phase] Loss: 0.7050 Acc: 58.6570% Time: 1827.6535s\n",
            "#54 [Test Phase] Loss: 0.5840 Acc: 69.7169% Time: 1836.3491s\n",
            "#55 [Test Phase] Loss: 0.6748 Acc: 62.1132% Time: 1844.9690s\n",
            "#56 [Test Phase] Loss: 0.6166 Acc: 66.2607% Time: 1853.6155s\n",
            "#57 [Test Phase] Loss: 0.5874 Acc: 68.3015% Time: 1862.2677s\n",
            "#58 [Test Phase] Loss: 0.7380 Acc: 56.1883% Time: 1870.8877s\n",
            "#59 [Test Phase] Loss: 0.8846 Acc: 47.0046% Time: 1879.5802s\n",
            "#60 [Test Phase] Loss: 0.7390 Acc: 56.8795% Time: 1888.3681s\n",
            "#61 [Test Phase] Loss: 0.6901 Acc: 60.5991% Time: 1897.6342s\n",
            "#62 [Test Phase] Loss: 0.7588 Acc: 55.2995% Time: 1906.5411s\n",
            "#63 [Test Phase] Loss: 0.5354 Acc: 73.8644% Time: 1915.4631s\n",
            "#64 [Test Phase] Loss: 0.6444 Acc: 63.7920% Time: 1924.4444s\n",
            "#65 [Test Phase] Loss: 0.6488 Acc: 64.1211% Time: 1933.0870s\n",
            "#66 [Test Phase] Loss: 0.6606 Acc: 63.0349% Time: 1942.0851s\n",
            "#67 [Test Phase] Loss: 0.8124 Acc: 51.0533% Time: 1951.3277s\n",
            "#68 [Test Phase] Loss: 0.8036 Acc: 42.0342% Time: 1960.3229s\n",
            "#69 [Test Phase] Loss: 0.8557 Acc: 48.8150% Time: 1968.8726s\n",
            "#70 [Test Phase] Loss: 0.5691 Acc: 70.4740% Time: 1977.4028s\n",
            "#71 [Test Phase] Loss: 0.6158 Acc: 67.4128% Time: 1986.1790s\n",
            "#72 [Test Phase] Loss: 0.7397 Acc: 56.6491% Time: 1994.8783s\n",
            "#73 [Test Phase] Loss: 0.6726 Acc: 62.3436% Time: 2003.8748s\n",
            "#74 [Test Phase] Loss: 0.7293 Acc: 57.4720% Time: 2013.1539s\n",
            "#75 [Test Phase] Loss: 0.7374 Acc: 56.6820% Time: 2022.1292s\n",
            "#76 [Test Phase] Loss: 0.7835 Acc: 54.2791% Time: 2031.1056s\n",
            "#77 [Test Phase] Loss: 0.8217 Acc: 51.2508% Time: 2040.1143s\n",
            "#78 [Test Phase] Loss: 0.7252 Acc: 57.9658% Time: 2049.0938s\n",
            "#79 [Test Phase] Loss: 0.8307 Acc: 40.0922% Time: 2057.7601s\n",
            "#80 [Test Phase] Loss: 0.6358 Acc: 65.8657% Time: 2066.3779s\n",
            "#81 [Test Phase] Loss: 0.7624 Acc: 55.1020% Time: 2075.0265s\n",
            "#82 [Test Phase] Loss: 0.7482 Acc: 56.8137% Time: 2083.9021s\n",
            "#83 [Test Phase] Loss: 0.6879 Acc: 61.1257% Time: 2092.7649s\n",
            "#84 [Test Phase] Loss: 0.6765 Acc: 61.8828% Time: 2101.9603s\n",
            "#85 [Test Phase] Loss: 0.6255 Acc: 65.8328% Time: 2110.6691s\n",
            "#86 [Test Phase] Loss: 0.7651 Acc: 55.1020% Time: 2119.8507s\n",
            "#87 [Test Phase] Loss: 0.7631 Acc: 55.4970% Time: 2128.7273s\n",
            "#88 [Test Phase] Loss: 0.5796 Acc: 70.4082% Time: 2137.5014s\n",
            "#89 [Test Phase] Loss: 0.7779 Acc: 54.1804% Time: 2145.9477s\n",
            "#90 [Test Phase] Loss: 0.7885 Acc: 43.7459% Time: 2154.6375s\n",
            "#91 [Test Phase] Loss: 0.6763 Acc: 62.1132% Time: 2163.3483s\n",
            "#92 [Test Phase] Loss: 0.6667 Acc: 62.6399% Time: 2172.4547s\n",
            "#93 [Test Phase] Loss: 0.7570 Acc: 55.8591% Time: 2181.4006s\n",
            "#94 [Test Phase] Loss: 0.7266 Acc: 57.8341% Time: 2190.2845s\n",
            "#95 [Test Phase] Loss: 0.6255 Acc: 66.1290% Time: 2199.1404s\n",
            "#96 [Test Phase] Loss: 0.7526 Acc: 56.3529% Time: 2207.7445s\n",
            "#97 [Test Phase] Loss: 0.7845 Acc: 53.4233% Time: 2216.2957s\n",
            "#98 [Test Phase] Loss: 0.5440 Acc: 72.9756% Time: 2225.1452s\n",
            "#99 [Test Phase] Loss: 0.5265 Acc: 74.4898% Time: 2234.0092s\n",
            "#100 [Test Phase] Loss: 0.6910 Acc: 61.8170% Time: 2242.7987s\n",
            "#101 [Test Phase] Loss: 0.7009 Acc: 54.2133% Time: 2251.5300s\n",
            "#102 [Test Phase] Loss: 0.6270 Acc: 66.1290% Time: 2260.1707s\n",
            "#103 [Test Phase] Loss: 0.8199 Acc: 51.3167% Time: 2268.6179s\n",
            "#104 [Test Phase] Loss: 0.7103 Acc: 59.8091% Time: 2277.0476s\n",
            "#105 [Test Phase] Loss: 0.6863 Acc: 61.7182% Time: 2285.5169s\n",
            "#106 [Test Phase] Loss: 0.6185 Acc: 67.1494% Time: 2293.9716s\n",
            "#107 [Test Phase] Loss: 0.6125 Acc: 67.7419% Time: 2302.4863s\n",
            "#108 [Test Phase] Loss: 0.5771 Acc: 70.3094% Time: 2310.8791s\n",
            "#109 [Test Phase] Loss: 0.8688 Acc: 49.5392% Time: 2319.2807s\n",
            "#110 [Test Phase] Loss: 0.6464 Acc: 64.5820% Time: 2327.7135s\n",
            "#111 [Test Phase] Loss: 0.5967 Acc: 68.7623% Time: 2335.9494s\n",
            "#112 [Test Phase] Loss: 0.7782 Acc: 42.5609% Time: 2344.4646s\n",
            "#113 [Test Phase] Loss: 0.6202 Acc: 63.3970% Time: 2352.7699s\n",
            "#114 [Test Phase] Loss: 0.6307 Acc: 63.2982% Time: 2361.3174s\n",
            "#115 [Test Phase] Loss: 0.7556 Acc: 48.5846% Time: 2369.6833s\n",
            "#116 [Test Phase] Loss: 0.7789 Acc: 44.9638% Time: 2378.0360s\n",
            "#117 [Test Phase] Loss: 0.7792 Acc: 45.0296% Time: 2386.4584s\n",
            "#118 [Test Phase] Loss: 0.5264 Acc: 75.6419% Time: 2394.7093s\n",
            "#119 [Test Phase] Loss: 0.5543 Acc: 72.0540% Time: 2403.0073s\n",
            "#120 [Test Phase] Loss: 0.6211 Acc: 64.3186% Time: 2411.3278s\n",
            "#121 [Test Phase] Loss: 0.6470 Acc: 60.5662% Time: 2419.6450s\n",
            "#122 [Test Phase] Loss: 0.7967 Acc: 44.3713% Time: 2427.9624s\n",
            "#123 [Test Phase] Loss: 0.6400 Acc: 62.0145% Time: 2436.4614s\n",
            "#124 [Test Phase] Loss: 0.7420 Acc: 50.2633% Time: 2444.8203s\n",
            "#125 [Test Phase] Loss: 0.7622 Acc: 48.5188% Time: 2453.2890s\n",
            "#126 [Test Phase] Loss: 0.7029 Acc: 54.3779% Time: 2461.6877s\n",
            "#127 [Test Phase] Loss: 0.6397 Acc: 62.2449% Time: 2470.0348s\n",
            "#128 [Test Phase] Loss: 0.5469 Acc: 73.1402% Time: 2478.5543s\n",
            "#129 [Test Phase] Loss: 0.7822 Acc: 46.7413% Time: 2486.9691s\n",
            "#130 [Test Phase] Loss: 0.6244 Acc: 63.4299% Time: 2495.4456s\n",
            "#131 [Test Phase] Loss: 0.7452 Acc: 50.7571% Time: 2504.1104s\n",
            "#132 [Test Phase] Loss: 0.7089 Acc: 54.5425% Time: 2512.5717s\n",
            "#133 [Test Phase] Loss: 0.6728 Acc: 57.6037% Time: 2521.0280s\n",
            "#134 [Test Phase] Loss: 0.6420 Acc: 62.0145% Time: 2529.3007s\n",
            "#135 [Test Phase] Loss: 0.6052 Acc: 66.6228% Time: 2537.8681s\n",
            "#136 [Test Phase] Loss: 0.6879 Acc: 56.9454% Time: 2546.2391s\n",
            "#137 [Test Phase] Loss: 0.8157 Acc: 44.8321% Time: 2554.5953s\n",
            "#138 [Test Phase] Loss: 0.8234 Acc: 44.4700% Time: 2563.0409s\n",
            "#139 [Test Phase] Loss: 0.5271 Acc: 74.8190% Time: 2571.4252s\n",
            "#140 [Test Phase] Loss: 0.6683 Acc: 59.2824% Time: 2580.0027s\n",
            "#141 [Test Phase] Loss: 0.6129 Acc: 65.5036% Time: 2588.7975s\n",
            "#142 [Test Phase] Loss: 0.6771 Acc: 58.8874% Time: 2597.7299s\n",
            "#143 [Test Phase] Loss: 0.7364 Acc: 52.4687% Time: 2606.0091s\n",
            "#144 [Test Phase] Loss: 0.6986 Acc: 56.9124% Time: 2614.3335s\n",
            "#145 [Test Phase] Loss: 0.6151 Acc: 65.7669% Time: 2623.0515s\n",
            "#146 [Test Phase] Loss: 0.6291 Acc: 63.8249% Time: 2631.7965s\n",
            "#147 [Test Phase] Loss: 0.6815 Acc: 58.0645% Time: 2640.1662s\n",
            "#148 [Test Phase] Loss: 0.7806 Acc: 49.3746% Time: 2648.6091s\n",
            "#149 [Test Phase] Loss: 0.6689 Acc: 59.6774% Time: 2657.1494s\n",
            "#150 [Test Phase] Loss: 0.7577 Acc: 50.5925% Time: 2665.3824s\n",
            "#151 [Test Phase] Loss: 0.7399 Acc: 53.1600% Time: 2673.4777s\n",
            "#152 [Test Phase] Loss: 0.7163 Acc: 55.0033% Time: 2681.8770s\n",
            "#153 [Test Phase] Loss: 0.7262 Acc: 54.1804% Time: 2690.2790s\n",
            "#154 [Test Phase] Loss: 0.6832 Acc: 58.7887% Time: 2698.6803s\n",
            "#155 [Test Phase] Loss: 0.8280 Acc: 45.5563% Time: 2707.3807s\n",
            "#156 [Test Phase] Loss: 0.7157 Acc: 51.6787% Time: 2715.8420s\n",
            "#157 [Test Phase] Loss: 0.7607 Acc: 51.5471% Time: 2724.1829s\n",
            "#158 [Test Phase] Loss: 0.6471 Acc: 62.4095% Time: 2732.4941s\n",
            "#159 [Test Phase] Loss: 0.8531 Acc: 44.2396% Time: 2740.9695s\n",
            "#160 [Test Phase] Loss: 0.6591 Acc: 60.6649% Time: 2749.5307s\n",
            "#161 [Test Phase] Loss: 0.6821 Acc: 58.9533% Time: 2758.0977s\n",
            "#162 [Test Phase] Loss: 0.7031 Acc: 56.6491% Time: 2766.7583s\n",
            "#163 [Test Phase] Loss: 0.7965 Acc: 49.1113% Time: 2775.3098s\n",
            "#164 [Test Phase] Loss: 0.6178 Acc: 65.9315% Time: 2784.1981s\n",
            "#165 [Test Phase] Loss: 0.7445 Acc: 53.0612% Time: 2792.9807s\n",
            "#166 [Test Phase] Loss: 0.6158 Acc: 65.4378% Time: 2801.8174s\n",
            "#167 [Test Phase] Loss: 0.8883 Acc: 31.9289% Time: 2810.4910s\n",
            "#168 [Test Phase] Loss: 0.6757 Acc: 60.1712% Time: 2819.2210s\n",
            "#169 [Test Phase] Loss: 0.7482 Acc: 52.6662% Time: 2827.8140s\n",
            "#170 [Test Phase] Loss: 0.6789 Acc: 59.5457% Time: 2836.4569s\n",
            "#171 [Test Phase] Loss: 0.7474 Acc: 52.6662% Time: 2845.0817s\n",
            "#172 [Test Phase] Loss: 0.6285 Acc: 64.1540% Time: 2853.9006s\n",
            "#173 [Test Phase] Loss: 0.8238 Acc: 46.1817% Time: 2862.5161s\n",
            "#174 [Test Phase] Loss: 0.7112 Acc: 56.3529% Time: 2871.1851s\n",
            "#175 [Test Phase] Loss: 0.7834 Acc: 49.9012% Time: 2880.0509s\n",
            "#176 [Test Phase] Loss: 0.7169 Acc: 56.2212% Time: 2888.8702s\n",
            "#177 [Test Phase] Loss: 0.6938 Acc: 57.6366% Time: 2897.6100s\n",
            "#178 [Test Phase] Loss: 0.7781 Acc: 43.2521% Time: 2906.0714s\n",
            "#179 [Test Phase] Loss: 0.6553 Acc: 61.7841% Time: 2915.2285s\n",
            "#180 [Test Phase] Loss: 0.5712 Acc: 70.9019% Time: 2923.7087s\n",
            "#181 [Test Phase] Loss: 0.5825 Acc: 68.8940% Time: 2932.3359s\n",
            "#182 [Test Phase] Loss: 0.6925 Acc: 58.3608% Time: 2940.8786s\n",
            "#183 [Test Phase] Loss: 0.6932 Acc: 58.1633% Time: 2949.5806s\n",
            "#184 [Test Phase] Loss: 0.7923 Acc: 50.2962% Time: 2958.8341s\n",
            "#185 [Test Phase] Loss: 0.7489 Acc: 53.5550% Time: 2967.5507s\n",
            "#186 [Test Phase] Loss: 0.6303 Acc: 64.4174% Time: 2976.3802s\n",
            "#187 [Test Phase] Loss: 0.6065 Acc: 66.4253% Time: 2985.5231s\n",
            "#188 [Test Phase] Loss: 0.6089 Acc: 67.2153% Time: 2994.4887s\n",
            "#189 [Test Phase] Loss: 0.5322 Acc: 76.5306% Time: 3003.3408s\n",
            "#190 [Test Phase] Loss: 0.6586 Acc: 62.1461% Time: 3012.0464s\n",
            "#191 [Test Phase] Loss: 0.6905 Acc: 58.9862% Time: 3020.6165s\n",
            "#192 [Test Phase] Loss: 0.5779 Acc: 69.9144% Time: 3029.1963s\n",
            "#193 [Test Phase] Loss: 0.7114 Acc: 57.0770% Time: 3037.9698s\n",
            "#194 [Test Phase] Loss: 0.5436 Acc: 73.1402% Time: 3046.7300s\n",
            "#195 [Test Phase] Loss: 0.5784 Acc: 69.9144% Time: 3055.4494s\n",
            "#196 [Test Phase] Loss: 0.6773 Acc: 59.8420% Time: 3064.0212s\n",
            "#197 [Test Phase] Loss: 0.7009 Acc: 57.8341% Time: 3072.5264s\n",
            "#198 [Test Phase] Loss: 0.5765 Acc: 69.6840% Time: 3081.0063s\n",
            "#199 [Test Phase] Loss: 0.7516 Acc: 54.3779% Time: 3089.5619s\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "path = \"C:/team3/resnet/models/#18 resnet_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "    model.load_state_dict(torch.load(path + \"/\" + file))\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "            # print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "            # imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "    # writer1.add_scalars(\"Loss/sum\", {'epoch_loss_train': epoch_loss, 'epoch_loss_test': epoch_loss1}, epoch)\n",
        "    # writer1.add_scalars(\"Acc/sum\", {'epoch_acc_train': epoch_acc/100, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    # writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_loss_test': epoch_loss1, 'epoch_acc_train': epoch_acc/100, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#0 Loss: 0.8026 Acc: 50.9524% Time: 39.0565s\n",
        "#0 Loss: 0.8026 Acc: 50.9524% Time: 37.5618s\n",
        "#0 Loss: 0.8026 Acc: 50.9524% Time: 32.1134s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "q78FprL_lPKP"
      },
      "outputs": [],
      "source": [
        "#모델 저장\n",
        "\n",
        "torch.save(model.state_dict(),'resnet_dict.pth')\n",
        "# torch.save(model,'model.pth')\n",
        "torch.save(model,'resnet.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = models.resnet34(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "model.load_state_dict(torch.load('#2 resnet_dict.pth'))\n",
        "model.eval()\n",
        "model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4ZRz72LDZpdP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[Test Phase] Loss: 0.5954 Acc: 72.7781% Time: 9.2968s\n"
          ]
        }
      ],
      "source": [
        "# model.eval()\n",
        "start_time = time.time()\n",
        "writer = SummaryWriter()\n",
        "\n",
        "with torch.no_grad():\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "        print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "        # imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "    epoch_loss = running_loss / len(test_datasets)\n",
        "    epoch_acc = running_corrects / len(test_datasets) * 100.\n",
        "    print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc, time.time() - start_time))\n",
        "    \n",
        "    # writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    # writer.add_scalar(\"acc/train\", epoch_acc, epoch)\n",
        "    # writer.add_scalar(\"sum/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"sum/train\", epoch_acc/100, epoch)\n",
        "\n",
        "writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "[Test Phase] Loss: 1.4326 Acc: 10.5003% Time: 209.7178s\n",
        "[Test Phase] Loss: 1.4326 Acc: 10.5003% Time: 215.8944s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHorVg4-Z68P"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "image = Image.open('test_img.jpg')\n",
        "image = transforms_test(image).unsqueeze(0).to(device)\n",
        "print(image.shape)\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    print(outputs)\n",
        "    print(class_names[preds[0]])\n",
        "    imshow(image.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 저장한 모델 가져와서 쓰기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9VvK7xiaax0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"resnet_dict.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKCkA0fXmEHm"
      },
      "outputs": [],
      "source": [
        "files.download(\"resnet.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0vAUYhKpmIsR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = models.resnet34(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "model.load_state_dict(torch.load('#2 resnet_dict.pth'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwzRuBb1nzxe"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "model = model.to(device)\n",
        "image = Image.open('test.jpg')\n",
        "image = transforms_test(image).unsqueeze(0).to(device)\n",
        "print(image.shape)\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    print(outputs)\n",
        "    print(class_names[preds[0]])\n",
        "    imshow(image.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzKPl5rLoNce"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "team3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "38eac6efdfb6e1d89e5adada41cd1cba1407b7df80f8c1b640481bdc8f4da74b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
