{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ==================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0q9RpKolGbKx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.models.vgg import vgg16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = vgg16(pretrained=True)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "fc = nn.Sequential(\n",
        "    nn.Linear(25088,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,2)\n",
        ")\n",
        "\n",
        "model.classifier = fc\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# from torch.optim.adam import Adam\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터셋 크기: 2004\n",
            "테스트 데이터셋 크기: 3038\n",
            "클래스: ['0', '1']\n"
          ]
        }
      ],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)\n",
        "\n",
        "lr = 0.001\n",
        "optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Loss: 0.6852 Acc: 54.9401% Time: 52.5326s\n",
            "#1 Loss: 0.6738 Acc: 57.3852% Time: 100.8757s\n",
            "#2 Loss: 0.6437 Acc: 62.6248% Time: 149.0655s\n",
            "#3 Loss: 0.6215 Acc: 64.3713% Time: 197.2678s\n",
            "#4 Loss: 0.5560 Acc: 71.1577% Time: 245.4887s\n",
            "#5 Loss: 0.4669 Acc: 77.1457% Time: 293.6255s\n",
            "#6 Loss: 0.4005 Acc: 81.3872% Time: 341.6439s\n",
            "#7 Loss: 0.3001 Acc: 87.0758% Time: 389.8406s\n",
            "#8 Loss: 0.2478 Acc: 90.0200% Time: 438.0329s\n",
            "#9 Loss: 0.2356 Acc: 89.9701% Time: 488.8059s\n",
            "#10 Loss: 0.1546 Acc: 93.7625% Time: 536.9804s\n",
            "#11 Loss: 0.1146 Acc: 95.9082% Time: 585.2110s\n",
            "#12 Loss: 0.0889 Acc: 96.7066% Time: 633.4236s\n",
            "#13 Loss: 0.1146 Acc: 95.5589% Time: 681.5558s\n",
            "#14 Loss: 0.0457 Acc: 98.6028% Time: 729.7275s\n",
            "#15 Loss: 0.0457 Acc: 98.4531% Time: 777.9444s\n",
            "#16 Loss: 0.0730 Acc: 97.8044% Time: 826.1117s\n",
            "#17 Loss: 0.0268 Acc: 99.2515% Time: 874.2859s\n",
            "#18 Loss: 0.0576 Acc: 97.9541% Time: 922.4243s\n",
            "#19 Loss: 0.0298 Acc: 99.1018% Time: 970.5882s\n",
            "#20 Loss: 0.0333 Acc: 98.8024% Time: 1018.9354s\n",
            "#21 Loss: 0.0283 Acc: 98.9521% Time: 1067.1872s\n",
            "#22 Loss: 0.0015 Acc: 100.0000% Time: 1115.4364s\n",
            "#23 Loss: 0.0002 Acc: 100.0000% Time: 1163.5955s\n",
            "#24 Loss: 0.0002 Acc: 100.0000% Time: 1211.9425s\n",
            "#25 Loss: 0.0001 Acc: 100.0000% Time: 1260.1359s\n",
            "#26 Loss: 0.0001 Acc: 100.0000% Time: 1308.3722s\n",
            "#27 Loss: 0.0001 Acc: 100.0000% Time: 1356.5024s\n",
            "#28 Loss: 0.0001 Acc: 100.0000% Time: 1404.6956s\n",
            "#29 Loss: 0.0000 Acc: 100.0000% Time: 1452.8839s\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "writer = SummaryWriter()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    ################################# 여기 ##################################\n",
        "    torch.save(model.state_dict(), f'C:/team3/vgg/models/#3 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 [Test Phase] Loss: 0.4971 Acc: 94.7663% Time: 31.9919s\n",
            "#1 [Test Phase] Loss: 0.6188 Acc: 69.8815% Time: 61.1377s\n",
            "#2 [Test Phase] Loss: 0.9329 Acc: 70.7373% Time: 90.3333s\n",
            "#3 [Test Phase] Loss: 1.0726 Acc: 71.0994% Time: 119.3640s\n",
            "#4 [Test Phase] Loss: 2.4864 Acc: 51.5800% Time: 148.4936s\n",
            "#5 [Test Phase] Loss: 1.8906 Acc: 60.8624% Time: 177.5017s\n",
            "#6 [Test Phase] Loss: 1.6509 Acc: 66.1290% Time: 206.5593s\n",
            "#7 [Test Phase] Loss: 1.8331 Acc: 61.8828% Time: 235.7056s\n",
            "#8 [Test Phase] Loss: 0.8895 Acc: 76.0369% Time: 264.9047s\n",
            "#9 [Test Phase] Loss: 1.4885 Acc: 73.2390% Time: 293.9499s\n",
            "#10 [Test Phase] Loss: 1.5791 Acc: 66.1290% Time: 322.9503s\n",
            "#11 [Test Phase] Loss: 1.4289 Acc: 70.8032% Time: 352.1160s\n",
            "#12 [Test Phase] Loss: 0.8828 Acc: 32.7847% Time: 381.2662s\n",
            "#13 [Test Phase] Loss: 1.1329 Acc: 72.3173% Time: 410.2940s\n",
            "#14 [Test Phase] Loss: 1.3758 Acc: 73.0086% Time: 439.4808s\n",
            "#15 [Test Phase] Loss: 1.6489 Acc: 74.0290% Time: 468.6494s\n",
            "#16 [Test Phase] Loss: 1.7201 Acc: 73.6011% Time: 497.8141s\n",
            "#17 [Test Phase] Loss: 1.8242 Acc: 73.1073% Time: 526.9609s\n",
            "#18 [Test Phase] Loss: 1.6977 Acc: 74.7531% Time: 556.0128s\n",
            "#19 [Test Phase] Loss: 1.7866 Acc: 74.1606% Time: 585.2847s\n",
            "#20 [Test Phase] Loss: 1.8840 Acc: 73.3377% Time: 614.4872s\n",
            "#21 [Test Phase] Loss: 1.9223 Acc: 73.1073% Time: 643.5161s\n",
            "#22 [Test Phase] Loss: 1.9655 Acc: 72.8769% Time: 672.6831s\n",
            "#23 [Test Phase] Loss: 0.7633 Acc: 44.6675% Time: 701.8634s\n",
            "#24 [Test Phase] Loss: 0.7529 Acc: 53.4233% Time: 731.2146s\n",
            "#25 [Test Phase] Loss: 0.3934 Acc: 83.0151% Time: 760.4065s\n",
            "#26 [Test Phase] Loss: 1.3414 Acc: 37.8209% Time: 789.5732s\n",
            "#27 [Test Phase] Loss: 1.2375 Acc: 61.0270% Time: 818.7669s\n",
            "#28 [Test Phase] Loss: 0.7845 Acc: 70.1448% Time: 848.0151s\n",
            "#29 [Test Phase] Loss: 0.8082 Acc: 64.1211% Time: 877.2327s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터셋 크기: 2004\n",
            "테스트 데이터셋 크기: 3038\n",
            "클래스: ['0', '1']\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'SGD' object has no attribute 'SGD'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m클래스:\u001b[39m\u001b[39m'\u001b[39m, class_names)\n\u001b[0;32m    117\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.00001\u001b[39m\n\u001b[1;32m--> 118\u001b[0m optim \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39;49mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[0;32m    119\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m    125\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'SGD' object has no attribute 'SGD'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "################################# 여기 ##################################\n",
        "path = \"C:/team3/vgg/models/#3 vgg_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "    model.load_state_dict(dict_model)\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()\n",
        "\n",
        "\n",
        "\n",
        "# #====================================================================================================\n",
        "# #4\n",
        "# # import torch\n",
        "# # import torch.nn as nn\n",
        "\n",
        "# # from torchvision.models.vgg import vgg16\n",
        "\n",
        "# torch.manual_seed(2022)\n",
        "# torch.cuda.manual_seed(2022)\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# model = vgg16(pretrained=True)\n",
        "\n",
        "# # print(model)\n",
        "\n",
        "# fc = nn.Sequential(\n",
        "#     nn.Linear(25088,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,2)\n",
        "# )\n",
        "\n",
        "# model.classifier = fc\n",
        "# model.to(device)\n",
        "\n",
        "# # import tqdm\n",
        "# # import os\n",
        "\n",
        "# # from torchvision import datasets, models, transforms\n",
        "# # from torchvision.datasets.cifar import CIFAR10\n",
        "# # from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# # from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# # from torch.utils.data.dataloader import DataLoader\n",
        "# # # from torch.optim.adam import Adam\n",
        "\n",
        "# # import torch.optim as optim\n",
        "# # from torch.utils.tensorboard import SummaryWriter\n",
        "# # import time\n",
        "\n",
        "# transforms_train = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# transforms_test = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# data_dir = './custom_dataset'\n",
        "# train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "# test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "# test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "\n",
        "# print('학습 데이터셋 크기:', len(train_datasets))\n",
        "# print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "# class_names = train_datasets.classes\n",
        "# print('클래스:', class_names)\n",
        "\n",
        "# lr = 0.00001\n",
        "# optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# num_epochs = 30\n",
        "# writer = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     criterion.train()\n",
        "#     running_loss = 0.\n",
        "#     running_corrects = 0\n",
        "\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         optim.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss = running_loss / len(train_datasets)\n",
        "#     epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "#     writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "#     writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "#     print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "#     ################################# 여기 ##################################\n",
        "#     torch.save(model.state_dict(), f'C:/team3/vgg/models/#4 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "# writer.close()\n",
        "\n",
        "\n",
        "\n",
        "# import os\n",
        "\n",
        "# ################################# 여기 ##################################\n",
        "# path = \"C:/team3/vgg/models/#4 vgg_models\"\n",
        "# file_list = os.listdir(path)\n",
        "# writer1 = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch, file in enumerate(file_list):\n",
        "#     dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "#     model.load_state_dict(dict_model)\n",
        "#     model.eval()\n",
        "#     model.to(\"cuda\")\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         test_loss = 0.\n",
        "#         test_corrects = 0\n",
        "\n",
        "#         for inputs, labels in test_dataloader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             test_loss += loss.item() * inputs.size(0)\n",
        "#             test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss1 = test_loss / len(test_datasets)\n",
        "#     epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "#     print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "#     writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "#     writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "# writer1.close()\n",
        "\n",
        "\n",
        "# #====================================================================================================\n",
        "# #5\n",
        "# # import torch\n",
        "# # import torch.nn as nn\n",
        "\n",
        "# # from torchvision.models.vgg import vgg16\n",
        "\n",
        "# torch.manual_seed(2022)\n",
        "# torch.cuda.manual_seed(2022)\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# model = vgg16(pretrained=True)\n",
        "\n",
        "# # print(model)\n",
        "\n",
        "# fc = nn.Sequential(\n",
        "#     nn.Linear(25088,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,2)\n",
        "# )\n",
        "\n",
        "# model.classifier = fc\n",
        "# model.to(device)\n",
        "\n",
        "# # import tqdm\n",
        "# # import os\n",
        "\n",
        "# # from torchvision import datasets, models, transforms\n",
        "# # from torchvision.datasets.cifar import CIFAR10\n",
        "# # from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# # from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# # from torch.utils.data.dataloader import DataLoader\n",
        "# # # from torch.optim.adam import Adam\n",
        "\n",
        "# # import torch.optim as optim\n",
        "# # from torch.utils.tensorboard import SummaryWriter\n",
        "# # import time\n",
        "\n",
        "# transforms_train = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# transforms_test = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# data_dir = './custom_dataset'\n",
        "# train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "# test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "# test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# print('학습 데이터셋 크기:', len(train_datasets))\n",
        "# print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "# class_names = train_datasets.classes\n",
        "# print('클래스:', class_names)\n",
        "\n",
        "# lr = 0.001\n",
        "# optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# num_epochs = 30\n",
        "# writer = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     criterion.train()\n",
        "#     running_loss = 0.\n",
        "#     running_corrects = 0\n",
        "\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         optim.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss = running_loss / len(train_datasets)\n",
        "#     epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "#     writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "#     writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "#     print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "#     ################################# 여기 ##################################\n",
        "#     torch.save(model.state_dict(), f'C:/team3/vgg/models/#5 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "# writer.close()\n",
        "\n",
        "\n",
        "\n",
        "# import os\n",
        "\n",
        "# ################################# 여기 ##################################\n",
        "# path = \"C:/team3/vgg/models/#5 vgg_models\"\n",
        "# file_list = os.listdir(path)\n",
        "# writer1 = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch, file in enumerate(file_list):\n",
        "#     dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "#     model.load_state_dict(dict_model)\n",
        "#     model.eval()\n",
        "#     model.to(\"cuda\")\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         test_loss = 0.\n",
        "#         test_corrects = 0\n",
        "\n",
        "#         for inputs, labels in test_dataloader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             test_loss += loss.item() * inputs.size(0)\n",
        "#             test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss1 = test_loss / len(test_datasets)\n",
        "#     epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "#     print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "#     writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "#     writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "# writer1.close()\n",
        "\n",
        "# #====================================================================================================\n",
        "# #6\n",
        "# # import torch\n",
        "# # import torch.nn as nn\n",
        "\n",
        "# # from torchvision.models.vgg import vgg16\n",
        "\n",
        "# torch.manual_seed(2022)\n",
        "# torch.cuda.manual_seed(2022)\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# model = vgg16(pretrained=True)\n",
        "\n",
        "# # print(model)\n",
        "\n",
        "# fc = nn.Sequential(\n",
        "#     nn.Linear(25088,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,2)\n",
        "# )\n",
        "\n",
        "# model.classifier = fc\n",
        "# model.to(device)\n",
        "\n",
        "# # import tqdm\n",
        "# # import os\n",
        "\n",
        "# # from torchvision import datasets, models, transforms\n",
        "# # from torchvision.datasets.cifar import CIFAR10\n",
        "# # from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# # from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# # from torch.utils.data.dataloader import DataLoader\n",
        "# # # from torch.optim.adam import Adam\n",
        "\n",
        "# # import torch.optim as optim\n",
        "# # from torch.utils.tensorboard import SummaryWriter\n",
        "# # import time\n",
        "\n",
        "# transforms_train = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# transforms_test = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# data_dir = './custom_dataset'\n",
        "# train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "# test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "# test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# print('학습 데이터셋 크기:', len(train_datasets))\n",
        "# print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "# class_names = train_datasets.classes\n",
        "# print('클래스:', class_names)\n",
        "\n",
        "# lr = 0.0001\n",
        "# optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# num_epochs = 30\n",
        "# writer = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     criterion.train()\n",
        "#     running_loss = 0.\n",
        "#     running_corrects = 0\n",
        "\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         optim.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss = running_loss / len(train_datasets)\n",
        "#     epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "#     writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "#     writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "#     print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "#     ################################# 여기 ##################################\n",
        "#     torch.save(model.state_dict(), f'C:/team3/vgg/models/#6 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "# writer.close()\n",
        "\n",
        "\n",
        "\n",
        "# import os\n",
        "\n",
        "# ################################# 여기 ##################################\n",
        "# path = \"C:/team3/vgg/models/#6 vgg_models\"\n",
        "# file_list = os.listdir(path)\n",
        "# writer1 = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch, file in enumerate(file_list):\n",
        "#     dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "#     model.load_state_dict(dict_model)\n",
        "#     model.eval()\n",
        "#     model.to(\"cuda\")\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         test_loss = 0.\n",
        "#         test_corrects = 0\n",
        "\n",
        "#         for inputs, labels in test_dataloader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             test_loss += loss.item() * inputs.size(0)\n",
        "#             test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss1 = test_loss / len(test_datasets)\n",
        "#     epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "#     print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "#     writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "#     writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "# writer1.close()\n",
        "\n",
        "# #====================================================================================================\n",
        "# #7\n",
        "# # import torch\n",
        "# # import torch.nn as nn\n",
        "\n",
        "# # from torchvision.models.vgg import vgg16\n",
        "\n",
        "# torch.manual_seed(2022)\n",
        "# torch.cuda.manual_seed(2022)\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# model = vgg16(pretrained=True)\n",
        "\n",
        "# # print(model)\n",
        "\n",
        "# fc = nn.Sequential(\n",
        "#     nn.Linear(25088,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,2)\n",
        "# )\n",
        "\n",
        "# model.classifier = fc\n",
        "# model.to(device)\n",
        "\n",
        "# # import tqdm\n",
        "# # import os\n",
        "\n",
        "# # from torchvision import datasets, models, transforms\n",
        "# # from torchvision.datasets.cifar import CIFAR10\n",
        "# # from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# # from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# # from torch.utils.data.dataloader import DataLoader\n",
        "# # # from torch.optim.adam import Adam\n",
        "\n",
        "# # import torch.optim as optim\n",
        "# # from torch.utils.tensorboard import SummaryWriter\n",
        "# # import time\n",
        "\n",
        "# transforms_train = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# transforms_test = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# data_dir = './custom_dataset'\n",
        "# train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "# test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "# test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# print('학습 데이터셋 크기:', len(train_datasets))\n",
        "# print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "# class_names = train_datasets.classes\n",
        "# print('클래스:', class_names)\n",
        "\n",
        "# lr = 0.00001\n",
        "# optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# num_epochs = 30\n",
        "# writer = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     criterion.train()\n",
        "#     running_loss = 0.\n",
        "#     running_corrects = 0\n",
        "\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         optim.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss = running_loss / len(train_datasets)\n",
        "#     epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "#     writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "#     writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "#     print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "#     ################################# 여기 ##################################\n",
        "#     torch.save(model.state_dict(), f'C:/team3/vgg/models/#7 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "# writer.close()\n",
        "\n",
        "\n",
        "\n",
        "# import os\n",
        "\n",
        "# ################################# 여기 ##################################\n",
        "# path = \"C:/team3/vgg/models/#7 vgg_models\"\n",
        "# file_list = os.listdir(path)\n",
        "# writer1 = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch, file in enumerate(file_list):\n",
        "#     dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "#     model.load_state_dict(dict_model)\n",
        "#     model.eval()\n",
        "#     model.to(\"cuda\")\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         test_loss = 0.\n",
        "#         test_corrects = 0\n",
        "\n",
        "#         for inputs, labels in test_dataloader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             test_loss += loss.item() * inputs.size(0)\n",
        "#             test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss1 = test_loss / len(test_datasets)\n",
        "#     epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "#     print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "#     writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "#     writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "# writer1.close()\n",
        "\n",
        "# #====================================================================================================\n",
        "# #8\n",
        "# # import torch\n",
        "# # import torch.nn as nn\n",
        "\n",
        "# # from torchvision.models.vgg import vgg16\n",
        "\n",
        "# torch.manual_seed(2022)\n",
        "# torch.cuda.manual_seed(2022)\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# model = vgg16(pretrained=True)\n",
        "\n",
        "# # print(model)\n",
        "\n",
        "# fc = nn.Sequential(\n",
        "#     nn.Linear(25088,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,2)\n",
        "# )\n",
        "\n",
        "# model.classifier = fc\n",
        "# model.to(device)\n",
        "\n",
        "# # import tqdm\n",
        "# # import os\n",
        "\n",
        "# # from torchvision import datasets, models, transforms\n",
        "# # from torchvision.datasets.cifar import CIFAR10\n",
        "# # from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# # from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# # from torch.utils.data.dataloader import DataLoader\n",
        "# # # from torch.optim.adam import Adam\n",
        "\n",
        "# # import torch.optim as optim\n",
        "# # from torch.utils.tensorboard import SummaryWriter\n",
        "# # import time\n",
        "\n",
        "# transforms_train = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# transforms_test = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# data_dir = './custom_dataset'\n",
        "# train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "# test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "# test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "\n",
        "# print('학습 데이터셋 크기:', len(train_datasets))\n",
        "# print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "# class_names = train_datasets.classes\n",
        "# print('클래스:', class_names)\n",
        "\n",
        "# lr = 0.001\n",
        "# optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# num_epochs = 30\n",
        "# writer = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     criterion.train()\n",
        "#     running_loss = 0.\n",
        "#     running_corrects = 0\n",
        "\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         optim.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss = running_loss / len(train_datasets)\n",
        "#     epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "#     writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "#     writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "#     print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "#     ################################# 여기 ##################################\n",
        "#     torch.save(model.state_dict(), f'C:/team3/vgg/models/#8 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "# writer.close()\n",
        "\n",
        "\n",
        "\n",
        "# import os\n",
        "\n",
        "# ################################# 여기 ##################################\n",
        "# path = \"C:/team3/vgg/models/#8 vgg_models\"\n",
        "# file_list = os.listdir(path)\n",
        "# writer1 = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch, file in enumerate(file_list):\n",
        "#     dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "#     model.load_state_dict(dict_model)\n",
        "#     model.eval()\n",
        "#     model.to(\"cuda\")\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         test_loss = 0.\n",
        "#         test_corrects = 0\n",
        "\n",
        "#         for inputs, labels in test_dataloader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             test_loss += loss.item() * inputs.size(0)\n",
        "#             test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss1 = test_loss / len(test_datasets)\n",
        "#     epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "#     print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "#     writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "#     writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "# writer1.close()\n",
        "\n",
        "# #====================================================================================================\n",
        "# #9\n",
        "# # import torch\n",
        "# # import torch.nn as nn\n",
        "\n",
        "# # from torchvision.models.vgg import vgg16\n",
        "\n",
        "# torch.manual_seed(2022)\n",
        "# torch.cuda.manual_seed(2022)\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# model = vgg16(pretrained=True)\n",
        "\n",
        "# # print(model)\n",
        "\n",
        "# fc = nn.Sequential(\n",
        "#     nn.Linear(25088,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,2)\n",
        "# )\n",
        "\n",
        "# model.classifier = fc\n",
        "# model.to(device)\n",
        "\n",
        "# # import tqdm\n",
        "# # import os\n",
        "\n",
        "# # from torchvision import datasets, models, transforms\n",
        "# # from torchvision.datasets.cifar import CIFAR10\n",
        "# # from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# # from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# # from torch.utils.data.dataloader import DataLoader\n",
        "# # # from torch.optim.adam import Adam\n",
        "\n",
        "# # import torch.optim as optim\n",
        "# # from torch.utils.tensorboard import SummaryWriter\n",
        "# # import time\n",
        "\n",
        "# transforms_train = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# transforms_test = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# data_dir = './custom_dataset'\n",
        "# train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "# test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "# test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "\n",
        "# print('학습 데이터셋 크기:', len(train_datasets))\n",
        "# print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "# class_names = train_datasets.classes\n",
        "# print('클래스:', class_names)\n",
        "\n",
        "# lr = 0.0001\n",
        "# optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# num_epochs = 30\n",
        "# writer = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     criterion.train()\n",
        "#     running_loss = 0.\n",
        "#     running_corrects = 0\n",
        "\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         optim.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss = running_loss / len(train_datasets)\n",
        "#     epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "#     writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "#     writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "#     print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "#     ################################# 여기 ##################################\n",
        "#     torch.save(model.state_dict(), f'C:/team3/vgg/models/#9 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "# writer.close()\n",
        "\n",
        "\n",
        "\n",
        "# import os\n",
        "\n",
        "# ################################# 여기 ##################################\n",
        "# path = \"C:/team3/vgg/models/#9 vgg_models\"\n",
        "# file_list = os.listdir(path)\n",
        "# writer1 = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch, file in enumerate(file_list):\n",
        "#     dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "#     model.load_state_dict(dict_model)\n",
        "#     model.eval()\n",
        "#     model.to(\"cuda\")\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         test_loss = 0.\n",
        "#         test_corrects = 0\n",
        "\n",
        "#         for inputs, labels in test_dataloader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             test_loss += loss.item() * inputs.size(0)\n",
        "#             test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss1 = test_loss / len(test_datasets)\n",
        "#     epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "#     print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "#     writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "#     writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "# writer1.close()\n",
        "\n",
        "# #====================================================================================================\n",
        "# #10\n",
        "# # import torch\n",
        "# # import torch.nn as nn\n",
        "\n",
        "# # from torchvision.models.vgg import vgg16\n",
        "\n",
        "# torch.manual_seed(2022)\n",
        "# torch.cuda.manual_seed(2022)\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# model = vgg16(pretrained=True)\n",
        "\n",
        "# # print(model)\n",
        "\n",
        "# fc = nn.Sequential(\n",
        "#     nn.Linear(25088,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,4096),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(4096,2)\n",
        "# )\n",
        "\n",
        "# model.classifier = fc\n",
        "# model.to(device)\n",
        "\n",
        "# # import tqdm\n",
        "# # import os\n",
        "\n",
        "# # from torchvision import datasets, models, transforms\n",
        "# # from torchvision.datasets.cifar import CIFAR10\n",
        "# # from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# # from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# # from torch.utils.data.dataloader import DataLoader\n",
        "# # # from torch.optim.adam import Adam\n",
        "\n",
        "# # import torch.optim as optim\n",
        "# # from torch.utils.tensorboard import SummaryWriter\n",
        "# # import time\n",
        "\n",
        "# transforms_train = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# transforms_test = transforms.Compose([\n",
        "#     Resize(224),\n",
        "#     # RandomCrop((224,224),padding=4),\n",
        "#     # RandomHorizontalFlip(p=0.5),\n",
        "#     ToTensor(),\n",
        "#     Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "# ])\n",
        "\n",
        "# data_dir = './custom_dataset'\n",
        "# train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "# test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "# test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "\n",
        "# print('학습 데이터셋 크기:', len(train_datasets))\n",
        "# print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "# class_names = train_datasets.classes\n",
        "# print('클래스:', class_names)\n",
        "\n",
        "# lr = 0.00001\n",
        "# optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# num_epochs = 30\n",
        "# writer = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     criterion.train()\n",
        "#     running_loss = 0.\n",
        "#     running_corrects = 0\n",
        "\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         optim.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "        \n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss = running_loss / len(train_datasets)\n",
        "#     epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "#     writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "#     writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "#     writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "#     print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "#     ################################# 여기 ##################################\n",
        "#     torch.save(model.state_dict(), f'C:/team3/vgg/models/#10 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "# writer.close()\n",
        "\n",
        "\n",
        "\n",
        "# import os\n",
        "\n",
        "# ################################# 여기 ##################################\n",
        "# path = \"C:/team3/vgg/models/#10 vgg_models\"\n",
        "# file_list = os.listdir(path)\n",
        "# writer1 = SummaryWriter()\n",
        "# start_time = time.time()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# for epoch, file in enumerate(file_list):\n",
        "#     dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "#     model.load_state_dict(dict_model)\n",
        "#     model.eval()\n",
        "#     model.to(\"cuda\")\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         test_loss = 0.\n",
        "#         test_corrects = 0\n",
        "\n",
        "#         for inputs, labels in test_dataloader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             test_loss += loss.item() * inputs.size(0)\n",
        "#             test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss1 = test_loss / len(test_datasets)\n",
        "#     epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "#     print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "#     writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "#     writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "#     writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "#     writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "# writer1.close()\n",
        "\n",
        "# #====================================================================================================\n",
        "\n",
        "\n",
        "# #====================================================================================================\n",
        "\n",
        "\n",
        "# #====================================================================================================\n",
        "\n",
        "\n",
        "# #====================================================================================================\n",
        "\n",
        "\n",
        "# #====================================================================================================\n",
        "\n",
        "\n",
        "# #===================================================================================================="
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ==================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터셋 크기: 2004\n",
            "테스트 데이터셋 크기: 3038\n",
            "클래스: ['0', '1']\n",
            "#0 Loss: 0.6928 Acc: 50.8483% Time: 48.2255s\n",
            "#1 Loss: 0.6942 Acc: 49.4511% Time: 96.3623s\n",
            "#2 Loss: 0.6941 Acc: 50.2495% Time: 144.6127s\n",
            "#3 Loss: 0.6925 Acc: 52.5948% Time: 192.9967s\n",
            "#4 Loss: 0.6901 Acc: 52.6946% Time: 241.2597s\n",
            "#5 Loss: 0.6888 Acc: 54.1916% Time: 289.5004s\n",
            "#6 Loss: 0.6901 Acc: 52.8443% Time: 337.6039s\n",
            "#7 Loss: 0.6871 Acc: 55.6387% Time: 385.8393s\n",
            "#8 Loss: 0.6898 Acc: 53.9920% Time: 433.9560s\n",
            "#9 Loss: 0.6889 Acc: 54.2914% Time: 482.1273s\n",
            "#10 Loss: 0.6878 Acc: 54.9900% Time: 530.3976s\n",
            "#11 Loss: 0.6868 Acc: 55.5389% Time: 578.6705s\n",
            "#12 Loss: 0.6881 Acc: 55.1397% Time: 626.8979s\n",
            "#13 Loss: 0.6846 Acc: 55.7385% Time: 675.0507s\n",
            "#14 Loss: 0.6841 Acc: 57.3852% Time: 723.3037s\n",
            "#15 Loss: 0.6849 Acc: 56.5868% Time: 771.4250s\n",
            "#16 Loss: 0.6837 Acc: 57.1357% Time: 819.5993s\n",
            "#17 Loss: 0.6813 Acc: 58.3832% Time: 867.8482s\n",
            "#18 Loss: 0.6834 Acc: 56.8862% Time: 916.0237s\n",
            "#19 Loss: 0.6826 Acc: 57.6347% Time: 964.2092s\n",
            "#20 Loss: 0.6818 Acc: 58.0838% Time: 1012.3516s\n",
            "#21 Loss: 0.6812 Acc: 58.2335% Time: 1060.5382s\n",
            "#22 Loss: 0.6826 Acc: 56.8862% Time: 1108.7339s\n",
            "#23 Loss: 0.6803 Acc: 59.1317% Time: 1156.9473s\n",
            "#24 Loss: 0.6787 Acc: 59.1317% Time: 1205.1141s\n",
            "#25 Loss: 0.6773 Acc: 59.2814% Time: 1253.2964s\n",
            "#26 Loss: 0.6792 Acc: 57.1357% Time: 1301.4900s\n",
            "#27 Loss: 0.6772 Acc: 57.2854% Time: 1349.6210s\n",
            "#28 Loss: 0.6749 Acc: 59.4810% Time: 1397.8570s\n",
            "#29 Loss: 0.6744 Acc: 59.9800% Time: 1446.0862s\n",
            "#0 [Test Phase] Loss: 0.6827 Acc: 75.0823% Time: 29.0238s\n",
            "#1 [Test Phase] Loss: 0.6905 Acc: 56.6491% Time: 58.0121s\n",
            "#2 [Test Phase] Loss: 0.7047 Acc: 43.2851% Time: 87.0212s\n",
            "#3 [Test Phase] Loss: 0.7077 Acc: 41.7051% Time: 116.0548s\n",
            "#4 [Test Phase] Loss: 0.7062 Acc: 43.6471% Time: 145.0173s\n",
            "#5 [Test Phase] Loss: 0.7053 Acc: 45.0625% Time: 174.0484s\n",
            "#6 [Test Phase] Loss: 0.7059 Acc: 45.0296% Time: 203.0065s\n",
            "#7 [Test Phase] Loss: 0.7075 Acc: 44.2067% Time: 232.0051s\n",
            "#8 [Test Phase] Loss: 0.7081 Acc: 44.4042% Time: 261.0257s\n",
            "#9 [Test Phase] Loss: 0.7060 Acc: 46.1488% Time: 289.9448s\n",
            "#10 [Test Phase] Loss: 0.7068 Acc: 46.0171% Time: 319.0120s\n",
            "#11 [Test Phase] Loss: 0.7082 Acc: 45.6880% Time: 348.0180s\n",
            "#12 [Test Phase] Loss: 0.6913 Acc: 54.4766% Time: 377.1650s\n",
            "#13 [Test Phase] Loss: 0.7092 Acc: 45.5563% Time: 406.7706s\n",
            "#14 [Test Phase] Loss: 0.7058 Acc: 47.5313% Time: 436.8529s\n",
            "#15 [Test Phase] Loss: 0.7064 Acc: 47.5642% Time: 467.0160s\n",
            "#16 [Test Phase] Loss: 0.7072 Acc: 47.4654% Time: 496.4852s\n",
            "#17 [Test Phase] Loss: 0.7048 Acc: 48.7163% Time: 525.6241s\n",
            "#18 [Test Phase] Loss: 0.7075 Acc: 47.7617% Time: 554.5833s\n",
            "#19 [Test Phase] Loss: 0.7052 Acc: 48.9467% Time: 583.6089s\n",
            "#20 [Test Phase] Loss: 0.7077 Acc: 48.0908% Time: 612.5865s\n",
            "#21 [Test Phase] Loss: 0.7077 Acc: 48.2554% Time: 641.8204s\n",
            "#22 [Test Phase] Loss: 0.7049 Acc: 49.1442% Time: 671.0416s\n",
            "#23 [Test Phase] Loss: 0.6983 Acc: 43.3180% Time: 700.1445s\n",
            "#24 [Test Phase] Loss: 0.6989 Acc: 43.4826% Time: 729.3257s\n",
            "#25 [Test Phase] Loss: 0.7010 Acc: 41.8367% Time: 758.3326s\n",
            "#26 [Test Phase] Loss: 0.7038 Acc: 40.0592% Time: 787.4994s\n",
            "#27 [Test Phase] Loss: 0.7053 Acc: 39.5326% Time: 816.4589s\n",
            "#28 [Test Phase] Loss: 0.7089 Acc: 37.5905% Time: 845.5020s\n",
            "#29 [Test Phase] Loss: 0.7067 Acc: 40.7505% Time: 874.4809s\n"
          ]
        }
      ],
      "source": [
        "#4\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.models.vgg import vgg16\n",
        "\n",
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = vgg16(pretrained=True)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "fc = nn.Sequential(\n",
        "    nn.Linear(25088,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,2)\n",
        ")\n",
        "\n",
        "model.classifier = fc\n",
        "model.to(device)\n",
        "\n",
        "import tqdm\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# from torch.optim.adam import Adam\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)\n",
        "\n",
        "lr = 0.00001\n",
        "optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "writer = SummaryWriter()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    ################################# 여기 ##################################\n",
        "    torch.save(model.state_dict(), f'C:/team3/vgg/models/#4 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "writer.close()\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "################################# 여기 ##################################\n",
        "path = \"C:/team3/vgg/models/#4 vgg_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "    model.load_state_dict(dict_model)\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#5\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.models.vgg import vgg16\n",
        "\n",
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = vgg16(pretrained=True)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "fc = nn.Sequential(\n",
        "    nn.Linear(25088,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,2)\n",
        ")\n",
        "\n",
        "model.classifier = fc\n",
        "model.to(device)\n",
        "\n",
        "import tqdm\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# from torch.optim.adam import Adam\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)\n",
        "\n",
        "lr = 0.001\n",
        "optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "writer = SummaryWriter()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    ################################# 여기 ##################################\n",
        "    torch.save(model.state_dict(), f'C:/team3/vgg/models/#5 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "writer.close()\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "################################# 여기 ##################################\n",
        "path = \"C:/team3/vgg/models/#5 vgg_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "    model.load_state_dict(dict_model)\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#6\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from torchvision.models.vgg import vgg16\n",
        "\n",
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = vgg16(pretrained=True)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "fc = nn.Sequential(\n",
        "    nn.Linear(25088,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,2)\n",
        ")\n",
        "\n",
        "model.classifier = fc\n",
        "model.to(device)\n",
        "\n",
        "# import tqdm\n",
        "# import os\n",
        "\n",
        "# from torchvision import datasets, models, transforms\n",
        "# from torchvision.datasets.cifar import CIFAR10\n",
        "# from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# from torch.utils.data.dataloader import DataLoader\n",
        "# # from torch.optim.adam import Adam\n",
        "\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# import time\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)\n",
        "\n",
        "lr = 0.0001\n",
        "optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "writer = SummaryWriter()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    ################################# 여기 ##################################\n",
        "    torch.save(model.state_dict(), f'C:/team3/vgg/models/#6 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "writer.close()\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "################################# 여기 ##################################\n",
        "path = \"C:/team3/vgg/models/#6 vgg_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "    model.load_state_dict(dict_model)\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#7\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from torchvision.models.vgg import vgg16\n",
        "\n",
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = vgg16(pretrained=True)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "fc = nn.Sequential(\n",
        "    nn.Linear(25088,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,2)\n",
        ")\n",
        "\n",
        "model.classifier = fc\n",
        "model.to(device)\n",
        "\n",
        "# import tqdm\n",
        "# import os\n",
        "\n",
        "# from torchvision import datasets, models, transforms\n",
        "# from torchvision.datasets.cifar import CIFAR10\n",
        "# from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# from torch.utils.data.dataloader import DataLoader\n",
        "# # from torch.optim.adam import Adam\n",
        "\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# import time\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)\n",
        "\n",
        "lr = 0.00001\n",
        "optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "writer = SummaryWriter()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    ################################# 여기 ##################################\n",
        "    torch.save(model.state_dict(), f'C:/team3/vgg/models/#7 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "writer.close()\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "################################# 여기 ##################################\n",
        "path = \"C:/team3/vgg/models/#7 vgg_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "    model.load_state_dict(dict_model)\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#8\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from torchvision.models.vgg import vgg16\n",
        "\n",
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = vgg16(pretrained=True)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "fc = nn.Sequential(\n",
        "    nn.Linear(25088,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,2)\n",
        ")\n",
        "\n",
        "model.classifier = fc\n",
        "model.to(device)\n",
        "\n",
        "# import tqdm\n",
        "# import os\n",
        "\n",
        "# from torchvision import datasets, models, transforms\n",
        "# from torchvision.datasets.cifar import CIFAR10\n",
        "# from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# from torch.utils.data.dataloader import DataLoader\n",
        "# # from torch.optim.adam import Adam\n",
        "\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# import time\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)\n",
        "\n",
        "lr = 0.001\n",
        "optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "writer = SummaryWriter()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    ################################# 여기 ##################################\n",
        "    torch.save(model.state_dict(), f'C:/team3/vgg/models/#8 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "writer.close()\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "################################# 여기 ##################################\n",
        "path = \"C:/team3/vgg/models/#8 vgg_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "    model.load_state_dict(dict_model)\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#9\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from torchvision.models.vgg import vgg16\n",
        "\n",
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = vgg16(pretrained=True)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "fc = nn.Sequential(\n",
        "    nn.Linear(25088,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,2)\n",
        ")\n",
        "\n",
        "model.classifier = fc\n",
        "model.to(device)\n",
        "\n",
        "# import tqdm\n",
        "# import os\n",
        "\n",
        "# from torchvision import datasets, models, transforms\n",
        "# from torchvision.datasets.cifar import CIFAR10\n",
        "# from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# from torch.utils.data.dataloader import DataLoader\n",
        "# # from torch.optim.adam import Adam\n",
        "\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# import time\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)\n",
        "\n",
        "lr = 0.0001\n",
        "optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "writer = SummaryWriter()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    ################################# 여기 ##################################\n",
        "    torch.save(model.state_dict(), f'C:/team3/vgg/models/#9 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "writer.close()\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "################################# 여기 ##################################\n",
        "path = \"C:/team3/vgg/models/#9 vgg_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "    model.load_state_dict(dict_model)\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "#10\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from torchvision.models.vgg import vgg16\n",
        "\n",
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = vgg16(pretrained=True)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "fc = nn.Sequential(\n",
        "    nn.Linear(25088,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,2)\n",
        ")\n",
        "\n",
        "model.classifier = fc\n",
        "model.to(device)\n",
        "\n",
        "# import tqdm\n",
        "# import os\n",
        "\n",
        "# from torchvision import datasets, models, transforms\n",
        "# from torchvision.datasets.cifar import CIFAR10\n",
        "# from torchvision.transforms import Compose, ToTensor, Resize\n",
        "# from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "# from torch.utils.data.dataloader import DataLoader\n",
        "# # from torch.optim.adam import Adam\n",
        "\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# import time\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=8, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)\n",
        "\n",
        "lr = 0.00001\n",
        "optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "writer = SummaryWriter()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    ################################# 여기 ##################################\n",
        "    torch.save(model.state_dict(), f'C:/team3/vgg/models/#10 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "writer.close()\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "################################# 여기 ##################################\n",
        "path = \"C:/team3/vgg/models/#10 vgg_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "    model.load_state_dict(dict_model)\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "team3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "38eac6efdfb6e1d89e5adada41cd1cba1407b7df80f8c1b640481bdc8f4da74b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
