{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'apt'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "# 한글 폰트 설치하기 (꼭! 설치가 완료되면 [런타임 다시 시작]을 누르고 다시 실행하기)\n",
        "!apt install fonts-nanum -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] [-e] <local project path> ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -y\n"
          ]
        }
      ],
      "source": [
        "%pip install fonts-nanum -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3krZm7boUYaC"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.font_manager' has no attribute '_rebuild'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\user\\Desktop\\7. 김한호 강사님\\2022.12.13\\resnet34_pretrain_final.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/7.%20%EA%B9%80%ED%95%9C%ED%98%B8%20%EA%B0%95%EC%82%AC%EB%8B%98/2022.12.13/resnet34_pretrain_final.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m font \u001b[39m=\u001b[39m fm\u001b[39m.\u001b[39mFontProperties(fname\u001b[39m=\u001b[39mfontpath, size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/7.%20%EA%B9%80%ED%95%9C%ED%98%B8%20%EA%B0%95%EC%82%AC%EB%8B%98/2022.12.13/resnet34_pretrain_final.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mrc(\u001b[39m'\u001b[39m\u001b[39mfont\u001b[39m\u001b[39m'\u001b[39m, family\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNanumBarunGothic\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/7.%20%EA%B9%80%ED%95%9C%ED%98%B8%20%EA%B0%95%EC%82%AC%EB%8B%98/2022.12.13/resnet34_pretrain_final.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m matplotlib\u001b[39m.\u001b[39;49mfont_manager\u001b[39m.\u001b[39;49m_rebuild()\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.font_manager' has no attribute '_rebuild'"
          ]
        }
      ],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 한글 폰트 설정하기\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=10)\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "matplotlib.font_manager._rebuild()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4Z_KLEjUVZWY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "directory_list = [\n",
        "    './custom_dataset/train/',\n",
        "    './custom_dataset/test/',\n",
        "]\n",
        "\n",
        "# 초기 디렉토리 만들기\n",
        "for directory in directory_list:\n",
        "    if not os.path.isdir(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "# 수집한 이미지를 학습 데이터와 평가 데이터로 구분하는 함수\n",
        "def dataset_split(query, train_cnt):\n",
        "    # 학습 및 평가 데이터셋 디렉토리 만들기\n",
        "    for directory in directory_list:\n",
        "        if not os.path.isdir(directory + '/' + query):\n",
        "            os.makedirs(directory + '/' + query)\n",
        "    # 학습 및 평가 데이터셋 준비하기\n",
        "    cnt = 0\n",
        "    for file_name in os.listdir(query):\n",
        "        if cnt < train_cnt:\n",
        "            # print(f'[Train Dataset] {file_name}')\n",
        "            shutil.move(query + '/' + file_name, './custom_dataset/train/' + query + '/' + file_name)\n",
        "        else:\n",
        "            # print(f'[Test Dataset] {file_name}')\n",
        "            shutil.move(query + '/' + file_name, './custom_dataset/test/' + query + '/' + file_name)\n",
        "        cnt += 1\n",
        "    shutil.rmtree(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kOR6UwvhXsC5"
      },
      "outputs": [],
      "source": [
        "query = '0'\n",
        "dataset_split(query, 210)\n",
        "query = '1'\n",
        "dataset_split(query, 210)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQ48DEenYHev"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device 객체"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pXHpCHvcYJ2P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터셋 크기: 2004\n",
            "테스트 데이터셋 크기: 3038\n",
            "클래스: ['0', '1']\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    # transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "# shuffle : set to True to have the data reshuffled at every epoch (이거 True 설정해도 결과값이 같게 나오네 완전 랜덤으로 섞는건 아닌듯)\n",
        "# num_workers : how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\n",
        "# 보통의 일반적인 환경에서 오픈소스로 풀려있는 모델을 학습시킬때는 코어 개수의 절반정도 수치면 무난하게 시스템 리소스를 사용하며 학습이 가능했습니다\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SYwt5TpIYTGX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0])\n",
            "(3, 454, 1810)\n",
            "===input==> (454, 1810, 3)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def imshow(input, title):\n",
        "    # torch.Tensor를 numpy 객체로 변환\n",
        "    print(input.numpy().shape)\n",
        "\n",
        "    input = input.numpy().transpose((1, 2, 0))\n",
        "    # 이미지 정규화 해제하기\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    input = std * input + mean\n",
        "    input = np.clip(input, 0, 1)\n",
        "    # 이미지 출력\n",
        "\n",
        "    print('===input==>',input.shape)\n",
        "    plt.imshow(input)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 학습 데이터를 배치 단위로 불러오기\n",
        "iterator = iter(train_dataloader)\n",
        "\n",
        "# 현재 배치를 이용해 격자 형태의 이미지를 만들어 시각화\n",
        "inputs, classes = next(iterator)\n",
        "print(classes)\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G1btTeRKYa0-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# 이미 학습된 resnet34 신경망을 불러온다\n",
        "model = models.resnet34(pretrained=True)\n",
        "print(model)\n",
        "#for name,module in model.named_parameters():\n",
        "#    module.requires_grad = False\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_features = model.fc.in_features\n",
        "# 전이 학습(transfer learning): 모델의 출력 뉴런 수를 3개로 교체하여 마지막 레이어 다시 학습\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "model = model.to(device)\n",
        "for name,module in model.named_parameters():\n",
        "    print( module.requires_grad )\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "#stochastic gradint descent : 확률적 경사하강법\n",
        "#미니배치를 사용하여 다소 부정확할수는 있지만 계산 속도가 빠르다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "K1phMzVPYu9u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Loss: 5.4785 Acc: 53.5429% Time: 7.7399s\n",
            "#1 Loss: 3.8845 Acc: 54.5409% Time: 15.4265s\n",
            "#2 Loss: 5.6512 Acc: 54.9900% Time: 23.1467s\n",
            "#3 Loss: 5.3920 Acc: 54.2914% Time: 30.7902s\n",
            "#4 Loss: 4.8860 Acc: 54.9401% Time: 38.4101s\n",
            "#5 Loss: 4.4315 Acc: 56.0878% Time: 46.0553s\n",
            "#6 Loss: 5.0751 Acc: 55.3892% Time: 53.6808s\n",
            "#7 Loss: 5.1890 Acc: 54.6407% Time: 61.2118s\n",
            "#8 Loss: 4.8743 Acc: 55.1397% Time: 68.8264s\n",
            "#9 Loss: 4.2732 Acc: 56.4371% Time: 76.4265s\n",
            "#10 Loss: 4.4926 Acc: 57.9341% Time: 84.0219s\n",
            "#11 Loss: 4.8677 Acc: 55.7884% Time: 91.5816s\n",
            "#12 Loss: 4.4856 Acc: 55.7385% Time: 99.1684s\n",
            "#13 Loss: 4.5875 Acc: 56.3872% Time: 106.7769s\n",
            "#14 Loss: 5.6784 Acc: 56.9860% Time: 114.3750s\n",
            "#15 Loss: 4.4142 Acc: 56.4870% Time: 122.0486s\n",
            "#16 Loss: 5.0942 Acc: 56.8862% Time: 129.6353s\n",
            "#17 Loss: 4.9072 Acc: 57.7844% Time: 137.2828s\n",
            "#18 Loss: 5.5248 Acc: 57.9840% Time: 144.8415s\n",
            "#19 Loss: 4.8779 Acc: 55.8882% Time: 152.3683s\n",
            "#20 Loss: 4.5187 Acc: 57.7844% Time: 159.9798s\n",
            "#21 Loss: 4.8342 Acc: 57.6846% Time: 167.5248s\n",
            "#22 Loss: 6.1598 Acc: 54.7904% Time: 175.1612s\n",
            "#23 Loss: 5.4318 Acc: 54.9401% Time: 182.8230s\n",
            "#24 Loss: 5.0121 Acc: 57.4850% Time: 190.3764s\n",
            "#25 Loss: 5.8087 Acc: 56.0379% Time: 197.9767s\n",
            "#26 Loss: 5.0576 Acc: 57.3353% Time: 205.5688s\n",
            "#27 Loss: 4.3881 Acc: 56.9361% Time: 213.2388s\n",
            "#28 Loss: 5.2754 Acc: 59.0319% Time: 220.9186s\n",
            "#29 Loss: 4.9998 Acc: 59.1317% Time: 228.5078s\n",
            "#30 Loss: 5.1160 Acc: 55.3393% Time: 236.2135s\n",
            "#31 Loss: 5.6397 Acc: 58.1836% Time: 243.7045s\n",
            "#32 Loss: 5.2362 Acc: 54.1417% Time: 251.1239s\n",
            "#33 Loss: 4.5567 Acc: 58.2335% Time: 258.7566s\n",
            "#34 Loss: 5.2927 Acc: 56.7864% Time: 266.3330s\n",
            "#35 Loss: 4.9533 Acc: 56.3872% Time: 273.9472s\n",
            "#36 Loss: 4.6022 Acc: 57.7844% Time: 281.5119s\n",
            "#37 Loss: 4.4445 Acc: 57.3353% Time: 289.1462s\n",
            "#38 Loss: 3.9468 Acc: 56.4371% Time: 296.7188s\n",
            "#39 Loss: 4.7036 Acc: 56.1377% Time: 304.3390s\n",
            "#40 Loss: 5.7285 Acc: 56.4371% Time: 311.9294s\n",
            "#41 Loss: 4.6229 Acc: 55.7385% Time: 319.5402s\n",
            "#42 Loss: 4.7415 Acc: 56.3872% Time: 327.0824s\n",
            "#43 Loss: 4.0581 Acc: 57.4351% Time: 334.6866s\n",
            "#44 Loss: 4.9298 Acc: 56.5369% Time: 342.3546s\n",
            "#45 Loss: 4.8925 Acc: 56.6866% Time: 349.8718s\n",
            "#46 Loss: 4.6657 Acc: 58.1836% Time: 357.4226s\n",
            "#47 Loss: 4.8882 Acc: 56.8363% Time: 365.0656s\n",
            "#48 Loss: 6.0951 Acc: 56.9361% Time: 372.7119s\n",
            "#49 Loss: 4.8574 Acc: 56.8862% Time: 380.2831s\n",
            "#50 Loss: 5.0090 Acc: 57.8842% Time: 387.8950s\n",
            "#51 Loss: 5.0161 Acc: 56.0379% Time: 395.4913s\n",
            "#52 Loss: 4.7683 Acc: 56.0379% Time: 403.0832s\n",
            "#53 Loss: 4.7639 Acc: 56.6367% Time: 410.7163s\n",
            "#54 Loss: 4.1057 Acc: 57.5349% Time: 418.3238s\n",
            "#55 Loss: 5.3278 Acc: 55.6886% Time: 425.8984s\n",
            "#56 Loss: 5.1812 Acc: 56.0379% Time: 433.5271s\n",
            "#57 Loss: 4.2565 Acc: 59.2814% Time: 441.0787s\n",
            "#58 Loss: 5.0364 Acc: 57.5349% Time: 448.6737s\n",
            "#59 Loss: 3.6695 Acc: 59.4810% Time: 456.2832s\n",
            "#60 Loss: 4.7931 Acc: 55.7884% Time: 463.9021s\n",
            "#61 Loss: 5.3281 Acc: 55.6886% Time: 471.5046s\n",
            "#62 Loss: 4.6324 Acc: 57.8343% Time: 479.0681s\n",
            "#63 Loss: 4.2949 Acc: 58.8822% Time: 486.6695s\n",
            "#64 Loss: 4.8329 Acc: 56.7864% Time: 494.1914s\n",
            "#65 Loss: 4.6428 Acc: 57.3353% Time: 501.8243s\n",
            "#66 Loss: 5.1068 Acc: 57.4351% Time: 509.4233s\n",
            "#67 Loss: 5.3237 Acc: 57.5349% Time: 517.0893s\n",
            "#68 Loss: 5.3563 Acc: 57.2854% Time: 524.7307s\n",
            "#69 Loss: 4.4849 Acc: 59.5808% Time: 532.4069s\n",
            "#70 Loss: 4.3672 Acc: 58.3832% Time: 540.0537s\n",
            "#71 Loss: 4.9319 Acc: 58.1337% Time: 547.7119s\n",
            "#72 Loss: 4.5168 Acc: 57.1357% Time: 555.2649s\n",
            "#73 Loss: 4.7263 Acc: 57.1357% Time: 562.9237s\n",
            "#74 Loss: 4.2548 Acc: 58.2834% Time: 570.4362s\n",
            "#75 Loss: 4.5930 Acc: 57.7844% Time: 578.0453s\n",
            "#76 Loss: 4.6145 Acc: 56.5369% Time: 585.5742s\n",
            "#77 Loss: 4.7886 Acc: 56.4870% Time: 593.2020s\n",
            "#78 Loss: 5.3382 Acc: 57.6347% Time: 600.7650s\n",
            "#79 Loss: 4.6866 Acc: 58.5828% Time: 608.4301s\n",
            "#80 Loss: 4.3237 Acc: 59.1317% Time: 616.0848s\n",
            "#81 Loss: 4.4386 Acc: 58.2335% Time: 623.6377s\n",
            "#82 Loss: 5.5074 Acc: 55.3393% Time: 631.2076s\n",
            "#83 Loss: 4.6409 Acc: 55.7884% Time: 638.7892s\n",
            "#84 Loss: 5.6804 Acc: 55.5888% Time: 646.4487s\n",
            "#85 Loss: 4.7096 Acc: 55.6886% Time: 653.9859s\n",
            "#86 Loss: 3.6254 Acc: 60.8283% Time: 661.5715s\n",
            "#87 Loss: 4.7758 Acc: 57.7844% Time: 668.9911s\n",
            "#88 Loss: 6.0947 Acc: 56.7864% Time: 676.5875s\n",
            "#89 Loss: 5.4175 Acc: 58.6826% Time: 684.2519s\n",
            "#90 Loss: 5.5454 Acc: 57.5848% Time: 691.7765s\n",
            "#91 Loss: 4.7212 Acc: 57.1357% Time: 699.3852s\n",
            "#92 Loss: 4.2938 Acc: 57.9341% Time: 707.0116s\n",
            "#93 Loss: 5.1805 Acc: 56.6367% Time: 714.6014s\n",
            "#94 Loss: 4.5948 Acc: 56.2874% Time: 722.1523s\n",
            "#95 Loss: 4.2354 Acc: 58.1337% Time: 729.7505s\n",
            "#96 Loss: 4.6419 Acc: 56.3872% Time: 737.4142s\n",
            "#97 Loss: 5.9589 Acc: 53.6926% Time: 744.9489s\n",
            "#98 Loss: 4.6461 Acc: 57.4351% Time: 752.5374s\n",
            "#99 Loss: 5.4027 Acc: 56.3373% Time: 760.1757s\n",
            "#100 Loss: 4.7437 Acc: 57.4850% Time: 767.7958s\n",
            "#101 Loss: 3.8039 Acc: 59.4311% Time: 775.4140s\n",
            "#102 Loss: 4.7482 Acc: 56.5868% Time: 782.9305s\n",
            "#103 Loss: 4.3714 Acc: 58.3832% Time: 790.5511s\n",
            "#104 Loss: 3.9322 Acc: 58.7824% Time: 798.1320s\n",
            "#105 Loss: 4.4487 Acc: 55.2395% Time: 805.7567s\n",
            "#106 Loss: 5.9411 Acc: 57.1856% Time: 813.3675s\n",
            "#107 Loss: 5.0051 Acc: 56.6866% Time: 820.9952s\n",
            "#108 Loss: 5.1439 Acc: 55.4391% Time: 828.5481s\n",
            "#109 Loss: 4.1999 Acc: 58.2834% Time: 836.1099s\n",
            "#110 Loss: 4.5922 Acc: 57.8343% Time: 843.8160s\n",
            "#111 Loss: 5.0129 Acc: 55.1896% Time: 851.3575s\n",
            "#112 Loss: 5.8415 Acc: 54.2415% Time: 858.9421s\n",
            "#113 Loss: 4.4412 Acc: 59.1816% Time: 866.5264s\n",
            "#114 Loss: 4.9713 Acc: 59.4311% Time: 874.1402s\n",
            "#115 Loss: 3.9589 Acc: 60.2794% Time: 881.7621s\n",
            "#116 Loss: 4.9588 Acc: 55.8383% Time: 889.3714s\n",
            "#117 Loss: 4.6548 Acc: 58.3333% Time: 896.9762s\n",
            "#118 Loss: 5.1827 Acc: 56.3373% Time: 904.5481s\n",
            "#119 Loss: 4.7237 Acc: 57.2854% Time: 912.1293s\n",
            "#120 Loss: 4.5614 Acc: 58.5828% Time: 919.7254s\n",
            "#121 Loss: 4.7037 Acc: 56.9860% Time: 927.3399s\n",
            "#122 Loss: 4.5587 Acc: 56.8363% Time: 934.8798s\n",
            "#123 Loss: 4.1643 Acc: 58.1836% Time: 942.5234s\n",
            "#124 Loss: 5.3055 Acc: 56.5369% Time: 950.0879s\n",
            "#125 Loss: 4.3210 Acc: 58.8822% Time: 957.6834s\n",
            "#126 Loss: 4.2802 Acc: 56.7365% Time: 965.2945s\n",
            "#127 Loss: 5.2727 Acc: 56.8363% Time: 972.8526s\n",
            "#128 Loss: 5.0359 Acc: 57.0359% Time: 980.5246s\n",
            "#129 Loss: 4.3358 Acc: 58.1836% Time: 988.0729s\n",
            "#130 Loss: 4.4131 Acc: 56.5369% Time: 995.6252s\n",
            "#131 Loss: 4.2619 Acc: 55.6886% Time: 1003.3214s\n",
            "#132 Loss: 4.3159 Acc: 58.0838% Time: 1010.8879s\n",
            "#133 Loss: 4.5257 Acc: 57.8842% Time: 1018.4847s\n",
            "#134 Loss: 5.1328 Acc: 57.1357% Time: 1026.0547s\n",
            "#135 Loss: 5.1516 Acc: 56.5868% Time: 1033.6784s\n",
            "#136 Loss: 6.1243 Acc: 53.9920% Time: 1041.2584s\n",
            "#137 Loss: 4.6921 Acc: 57.0359% Time: 1048.9068s\n",
            "#138 Loss: 4.5215 Acc: 56.8363% Time: 1056.5410s\n",
            "#139 Loss: 5.1510 Acc: 54.1916% Time: 1064.2087s\n",
            "#140 Loss: 5.5306 Acc: 55.4890% Time: 1071.8637s\n",
            "#141 Loss: 5.2829 Acc: 56.4870% Time: 1079.5056s\n",
            "#142 Loss: 4.6506 Acc: 55.7884% Time: 1087.0931s\n",
            "#143 Loss: 4.4776 Acc: 57.6846% Time: 1094.6809s\n",
            "#144 Loss: 4.7839 Acc: 57.6846% Time: 1102.3064s\n",
            "#145 Loss: 5.0283 Acc: 58.3333% Time: 1109.8558s\n",
            "#146 Loss: 5.8502 Acc: 56.6866% Time: 1117.4541s\n",
            "#147 Loss: 4.4462 Acc: 56.4371% Time: 1125.0416s\n",
            "#148 Loss: 4.4033 Acc: 57.8343% Time: 1132.6965s\n",
            "#149 Loss: 5.0050 Acc: 55.2894% Time: 1140.3006s\n",
            "#150 Loss: 4.3077 Acc: 57.4850% Time: 1147.8688s\n",
            "#151 Loss: 4.6627 Acc: 56.1377% Time: 1155.4046s\n",
            "#152 Loss: 5.5706 Acc: 57.1357% Time: 1163.0218s\n",
            "#153 Loss: 5.3293 Acc: 56.3872% Time: 1170.6672s\n",
            "#154 Loss: 4.3107 Acc: 56.5868% Time: 1178.2733s\n",
            "#155 Loss: 4.8872 Acc: 56.4371% Time: 1185.8480s\n",
            "#156 Loss: 4.5120 Acc: 57.2854% Time: 1193.4599s\n",
            "#157 Loss: 4.3728 Acc: 57.0359% Time: 1201.0789s\n",
            "#158 Loss: 4.8265 Acc: 59.1816% Time: 1208.6111s\n",
            "#159 Loss: 4.8560 Acc: 58.4830% Time: 1216.3075s\n",
            "#160 Loss: 4.1870 Acc: 57.4850% Time: 1223.9147s\n",
            "#161 Loss: 4.2505 Acc: 56.7365% Time: 1231.4358s\n",
            "#162 Loss: 3.9207 Acc: 59.9800% Time: 1239.0310s\n",
            "#163 Loss: 4.8655 Acc: 56.7365% Time: 1246.5899s\n",
            "#164 Loss: 5.5636 Acc: 55.2894% Time: 1254.2209s\n",
            "#165 Loss: 5.1193 Acc: 57.9840% Time: 1261.8727s\n",
            "#166 Loss: 5.7238 Acc: 57.0858% Time: 1269.4042s\n",
            "#167 Loss: 3.8255 Acc: 58.7824% Time: 1276.9534s\n",
            "#168 Loss: 4.6594 Acc: 58.1337% Time: 1284.5682s\n",
            "#169 Loss: 4.4291 Acc: 57.3852% Time: 1292.2015s\n",
            "#170 Loss: 5.1067 Acc: 55.5389% Time: 1299.8517s\n",
            "#171 Loss: 4.6669 Acc: 57.5848% Time: 1307.3673s\n",
            "#172 Loss: 5.2208 Acc: 57.1856% Time: 1315.0624s\n",
            "#173 Loss: 4.4668 Acc: 60.1297% Time: 1322.6035s\n",
            "#174 Loss: 4.8846 Acc: 56.4371% Time: 1330.1909s\n",
            "#175 Loss: 5.0122 Acc: 57.7345% Time: 1337.5602s\n",
            "#176 Loss: 5.2703 Acc: 57.1856% Time: 1345.2191s\n",
            "#177 Loss: 5.7721 Acc: 56.9361% Time: 1352.7939s\n",
            "#178 Loss: 4.7654 Acc: 56.9860% Time: 1360.4031s\n",
            "#179 Loss: 4.2589 Acc: 57.5349% Time: 1367.9438s\n",
            "#180 Loss: 4.2904 Acc: 56.9860% Time: 1375.5801s\n",
            "#181 Loss: 5.0140 Acc: 54.7904% Time: 1383.2457s\n",
            "#182 Loss: 4.6934 Acc: 57.1357% Time: 1390.9812s\n",
            "#183 Loss: 5.5339 Acc: 57.4850% Time: 1398.5762s\n",
            "#184 Loss: 4.4769 Acc: 57.5349% Time: 1406.2595s\n",
            "#185 Loss: 5.0989 Acc: 56.1876% Time: 1413.7796s\n",
            "#186 Loss: 5.8430 Acc: 54.3912% Time: 1421.4196s\n",
            "#187 Loss: 4.7811 Acc: 56.2874% Time: 1429.2025s\n",
            "#188 Loss: 5.1513 Acc: 56.2375% Time: 1436.7559s\n",
            "#189 Loss: 4.2252 Acc: 58.1836% Time: 1444.3722s\n",
            "#190 Loss: 4.8197 Acc: 57.0858% Time: 1451.8873s\n",
            "#191 Loss: 5.4038 Acc: 56.4371% Time: 1459.3845s\n",
            "#192 Loss: 3.9346 Acc: 56.6367% Time: 1466.9107s\n",
            "#193 Loss: 4.3366 Acc: 57.7844% Time: 1474.5171s\n",
            "#194 Loss: 4.1155 Acc: 59.1317% Time: 1482.0679s\n",
            "#195 Loss: 4.5771 Acc: 57.7844% Time: 1489.5349s\n",
            "#196 Loss: 4.9856 Acc: 56.5868% Time: 1497.0938s\n",
            "#197 Loss: 4.6631 Acc: 58.5329% Time: 1504.6967s\n",
            "#198 Loss: 4.8240 Acc: 55.2894% Time: 1512.3650s\n",
            "#199 Loss: 4.0387 Acc: 59.3812% Time: 1519.8953s\n",
            "#200 Loss: 4.6273 Acc: 57.1856% Time: 1527.4977s\n",
            "#201 Loss: 4.5896 Acc: 56.2874% Time: 1535.0879s\n",
            "#202 Loss: 6.3849 Acc: 55.8383% Time: 1542.6868s\n",
            "#203 Loss: 4.7309 Acc: 56.8862% Time: 1550.3599s\n",
            "#204 Loss: 4.8600 Acc: 58.1836% Time: 1557.8427s\n",
            "#205 Loss: 4.9836 Acc: 57.7345% Time: 1565.2869s\n",
            "#206 Loss: 4.4020 Acc: 58.0838% Time: 1572.8492s\n",
            "#207 Loss: 5.0587 Acc: 56.0878% Time: 1580.4786s\n",
            "#208 Loss: 5.3771 Acc: 54.6906% Time: 1588.0714s\n",
            "#209 Loss: 5.2272 Acc: 56.2375% Time: 1595.6572s\n",
            "#210 Loss: 4.8462 Acc: 57.1856% Time: 1603.2645s\n",
            "#211 Loss: 6.0580 Acc: 55.2894% Time: 1610.8862s\n",
            "#212 Loss: 5.7487 Acc: 58.1337% Time: 1618.4545s\n",
            "#213 Loss: 5.8068 Acc: 56.8862% Time: 1625.8861s\n",
            "#214 Loss: 4.3821 Acc: 58.7325% Time: 1633.4286s\n",
            "#215 Loss: 5.1064 Acc: 56.4870% Time: 1641.0423s\n",
            "#216 Loss: 4.3647 Acc: 58.4830% Time: 1648.7242s\n",
            "#217 Loss: 4.5080 Acc: 56.5369% Time: 1656.2513s\n",
            "#218 Loss: 5.3143 Acc: 55.7884% Time: 1663.8706s\n",
            "#219 Loss: 5.0373 Acc: 56.1876% Time: 1671.4335s\n",
            "#220 Loss: 4.7660 Acc: 55.7385% Time: 1679.0603s\n",
            "#221 Loss: 6.0135 Acc: 55.9381% Time: 1686.6963s\n",
            "#222 Loss: 4.6765 Acc: 56.9860% Time: 1694.2670s\n",
            "#223 Loss: 5.2520 Acc: 57.8343% Time: 1701.8397s\n",
            "#224 Loss: 5.2591 Acc: 56.1876% Time: 1709.3791s\n",
            "#225 Loss: 4.6367 Acc: 59.1317% Time: 1716.8936s\n",
            "#226 Loss: 5.1026 Acc: 57.9840% Time: 1724.4427s\n",
            "#227 Loss: 4.9948 Acc: 54.9401% Time: 1732.0573s\n",
            "#228 Loss: 4.6613 Acc: 57.5349% Time: 1739.4057s\n",
            "#229 Loss: 4.6833 Acc: 55.1397% Time: 1747.0470s\n",
            "#230 Loss: 4.6601 Acc: 56.3373% Time: 1754.6170s\n",
            "#231 Loss: 5.4971 Acc: 56.6866% Time: 1762.1870s\n",
            "#232 Loss: 4.7205 Acc: 58.6826% Time: 1769.8437s\n",
            "#233 Loss: 4.4677 Acc: 56.1876% Time: 1777.4240s\n",
            "#234 Loss: 4.5194 Acc: 58.3333% Time: 1785.0280s\n",
            "#235 Loss: 4.5398 Acc: 56.8862% Time: 1792.5579s\n",
            "#236 Loss: 4.6377 Acc: 57.6347% Time: 1800.1997s\n",
            "#237 Loss: 4.6691 Acc: 58.6826% Time: 1807.6086s\n",
            "#238 Loss: 5.0663 Acc: 58.1337% Time: 1815.1856s\n",
            "#239 Loss: 4.2194 Acc: 61.1277% Time: 1822.7491s\n",
            "#240 Loss: 5.4128 Acc: 57.2355% Time: 1830.2041s\n",
            "#241 Loss: 4.8791 Acc: 58.0838% Time: 1837.7304s\n",
            "#242 Loss: 4.5642 Acc: 57.9341% Time: 1845.2236s\n",
            "#243 Loss: 4.7739 Acc: 57.4850% Time: 1852.8220s\n",
            "#244 Loss: 5.4893 Acc: 56.5369% Time: 1860.4387s\n",
            "#245 Loss: 4.4918 Acc: 56.2375% Time: 1868.0132s\n",
            "#246 Loss: 3.9712 Acc: 58.5828% Time: 1875.6002s\n",
            "#247 Loss: 4.8992 Acc: 57.1357% Time: 1883.2209s\n",
            "#248 Loss: 4.8555 Acc: 58.2335% Time: 1890.7841s\n",
            "#249 Loss: 5.8872 Acc: 55.0898% Time: 1898.3926s\n",
            "#250 Loss: 5.2235 Acc: 56.0878% Time: 1906.0085s\n",
            "#251 Loss: 5.9904 Acc: 55.7385% Time: 1913.5136s\n",
            "#252 Loss: 5.2081 Acc: 55.2894% Time: 1921.1491s\n",
            "#253 Loss: 4.6546 Acc: 56.2874% Time: 1928.7715s\n",
            "#254 Loss: 5.5117 Acc: 54.6407% Time: 1936.3439s\n",
            "#255 Loss: 4.7680 Acc: 57.1856% Time: 1943.7425s\n",
            "#256 Loss: 5.0699 Acc: 55.5389% Time: 1951.3710s\n",
            "#257 Loss: 5.2789 Acc: 55.7385% Time: 1959.0097s\n",
            "#258 Loss: 6.0551 Acc: 56.0379% Time: 1966.5507s\n",
            "#259 Loss: 5.0375 Acc: 57.0359% Time: 1974.1470s\n",
            "#260 Loss: 3.9300 Acc: 59.9301% Time: 1981.6735s\n",
            "#261 Loss: 4.7437 Acc: 58.4331% Time: 1989.3298s\n",
            "#262 Loss: 5.8092 Acc: 56.0379% Time: 1996.9082s\n",
            "#263 Loss: 4.7071 Acc: 58.1836% Time: 2004.5288s\n",
            "#264 Loss: 4.8842 Acc: 56.3872% Time: 2012.1531s\n",
            "#265 Loss: 4.1952 Acc: 58.4830% Time: 2019.5111s\n",
            "#266 Loss: 4.8097 Acc: 56.6866% Time: 2026.9445s\n",
            "#267 Loss: 4.8190 Acc: 57.8842% Time: 2034.5465s\n",
            "#268 Loss: 4.3638 Acc: 59.4810% Time: 2042.2267s\n",
            "#269 Loss: 5.1972 Acc: 56.5369% Time: 2049.6795s\n",
            "#270 Loss: 5.8702 Acc: 56.4371% Time: 2057.1933s\n",
            "#271 Loss: 4.8414 Acc: 58.0838% Time: 2064.7455s\n",
            "#272 Loss: 5.1687 Acc: 55.5389% Time: 2072.3105s\n",
            "#273 Loss: 5.7393 Acc: 55.5888% Time: 2079.9132s\n",
            "#274 Loss: 4.9479 Acc: 57.1856% Time: 2087.5185s\n",
            "#275 Loss: 4.7119 Acc: 60.1297% Time: 2095.0544s\n",
            "#276 Loss: 5.0425 Acc: 57.3353% Time: 2102.5703s\n",
            "#277 Loss: 4.2884 Acc: 56.9860% Time: 2110.0964s\n",
            "#278 Loss: 5.4742 Acc: 57.6846% Time: 2117.6818s\n",
            "#279 Loss: 4.6508 Acc: 56.9860% Time: 2125.1410s\n",
            "#280 Loss: 4.5257 Acc: 56.0878% Time: 2132.7475s\n",
            "#281 Loss: 4.8016 Acc: 57.8343% Time: 2140.2653s\n",
            "#282 Loss: 5.7553 Acc: 58.3333% Time: 2147.6935s\n",
            "#283 Loss: 4.4852 Acc: 56.6866% Time: 2155.3758s\n",
            "#284 Loss: 5.5125 Acc: 56.9860% Time: 2162.9100s\n",
            "#285 Loss: 4.6422 Acc: 57.0858% Time: 2170.4687s\n",
            "#286 Loss: 5.4372 Acc: 55.9381% Time: 2178.0338s\n",
            "#287 Loss: 4.7734 Acc: 57.0359% Time: 2185.5031s\n",
            "#288 Loss: 5.8126 Acc: 56.2874% Time: 2193.0757s\n",
            "#289 Loss: 4.5460 Acc: 57.3852% Time: 2200.6423s\n",
            "#290 Loss: 4.2465 Acc: 57.7844% Time: 2208.0972s\n",
            "#291 Loss: 4.3038 Acc: 57.0858% Time: 2215.6728s\n",
            "#292 Loss: 4.6115 Acc: 56.1876% Time: 2223.2727s\n",
            "#293 Loss: 4.1771 Acc: 58.4331% Time: 2230.8763s\n",
            "#294 Loss: 4.5596 Acc: 56.6367% Time: 2238.2746s\n",
            "#295 Loss: 5.1281 Acc: 56.5369% Time: 2245.8746s\n",
            "#296 Loss: 4.6843 Acc: 57.0359% Time: 2253.4268s\n",
            "#297 Loss: 5.5180 Acc: 55.8882% Time: 2261.0434s\n",
            "#298 Loss: 4.4277 Acc: 56.1876% Time: 2268.6877s\n",
            "#299 Loss: 4.3758 Acc: 55.4391% Time: 2276.2641s\n",
            "#300 Loss: 5.4881 Acc: 57.6846% Time: 2283.8410s\n",
            "#301 Loss: 4.8800 Acc: 58.4830% Time: 2291.4254s\n",
            "#302 Loss: 5.4435 Acc: 57.4351% Time: 2298.9927s\n",
            "#303 Loss: 4.3383 Acc: 57.5349% Time: 2306.6601s\n",
            "#304 Loss: 5.6377 Acc: 55.7385% Time: 2314.2254s\n",
            "#305 Loss: 5.3045 Acc: 56.5369% Time: 2321.7943s\n",
            "#306 Loss: 4.9970 Acc: 57.6347% Time: 2329.4076s\n",
            "#307 Loss: 4.7104 Acc: 55.5888% Time: 2337.0296s\n",
            "#308 Loss: 5.6525 Acc: 56.6367% Time: 2344.5859s\n",
            "#309 Loss: 4.6134 Acc: 58.0838% Time: 2352.1050s\n",
            "#310 Loss: 4.4412 Acc: 57.6347% Time: 2359.5875s\n",
            "#311 Loss: 4.5001 Acc: 57.7345% Time: 2367.2403s\n",
            "#312 Loss: 5.0210 Acc: 57.9341% Time: 2374.8404s\n",
            "#313 Loss: 5.2054 Acc: 56.6367% Time: 2382.4039s\n",
            "#314 Loss: 4.8153 Acc: 55.9880% Time: 2390.0882s\n",
            "#315 Loss: 4.7403 Acc: 56.0379% Time: 2397.6303s\n",
            "#316 Loss: 4.3070 Acc: 55.9381% Time: 2405.2006s\n",
            "#317 Loss: 4.0399 Acc: 57.9341% Time: 2412.8532s\n",
            "#318 Loss: 4.4810 Acc: 55.6886% Time: 2420.3751s\n",
            "#319 Loss: 4.7601 Acc: 56.4371% Time: 2427.8546s\n",
            "#320 Loss: 4.1612 Acc: 58.5828% Time: 2435.3727s\n",
            "#321 Loss: 5.1982 Acc: 56.9361% Time: 2442.9913s\n",
            "#322 Loss: 4.0653 Acc: 60.0299% Time: 2450.5943s\n",
            "#323 Loss: 4.5961 Acc: 58.4830% Time: 2458.1899s\n",
            "#324 Loss: 4.4516 Acc: 57.4351% Time: 2465.7912s\n",
            "#325 Loss: 5.9216 Acc: 57.3353% Time: 2473.3396s\n",
            "#326 Loss: 4.5333 Acc: 57.1856% Time: 2480.8112s\n",
            "#327 Loss: 5.6781 Acc: 56.7365% Time: 2488.4316s\n",
            "#328 Loss: 4.7072 Acc: 58.2834% Time: 2496.0156s\n",
            "#329 Loss: 4.9638 Acc: 57.0858% Time: 2503.5602s\n",
            "#330 Loss: 4.1261 Acc: 57.8343% Time: 2511.1665s\n",
            "#331 Loss: 4.3078 Acc: 55.4890% Time: 2518.7913s\n",
            "#332 Loss: 5.3276 Acc: 56.6866% Time: 2526.3852s\n",
            "#333 Loss: 4.8781 Acc: 55.7884% Time: 2534.0264s\n",
            "#334 Loss: 5.9328 Acc: 54.4411% Time: 2541.5731s\n",
            "#335 Loss: 4.6960 Acc: 58.5329% Time: 2549.1312s\n",
            "#336 Loss: 4.7838 Acc: 56.6866% Time: 2556.7322s\n",
            "#337 Loss: 5.3492 Acc: 55.5888% Time: 2564.3654s\n",
            "#338 Loss: 3.9554 Acc: 58.3832% Time: 2571.7436s\n",
            "#339 Loss: 5.5542 Acc: 57.5349% Time: 2579.3478s\n",
            "#340 Loss: 4.5761 Acc: 57.1856% Time: 2586.9039s\n",
            "#341 Loss: 4.2521 Acc: 58.9321% Time: 2594.5431s\n",
            "#342 Loss: 5.5933 Acc: 58.1836% Time: 2602.1734s\n",
            "#343 Loss: 4.4566 Acc: 56.3373% Time: 2609.5765s\n",
            "#344 Loss: 5.4373 Acc: 56.4870% Time: 2617.1348s\n",
            "#345 Loss: 4.5362 Acc: 56.6866% Time: 2624.7169s\n",
            "#346 Loss: 3.6430 Acc: 59.5808% Time: 2632.1793s\n",
            "#347 Loss: 4.8965 Acc: 56.4371% Time: 2639.7806s\n",
            "#348 Loss: 4.7126 Acc: 57.8343% Time: 2647.3537s\n",
            "#349 Loss: 4.6500 Acc: 56.2375% Time: 2654.9104s\n",
            "#350 Loss: 4.1463 Acc: 58.0838% Time: 2662.5273s\n",
            "#351 Loss: 4.4812 Acc: 56.6866% Time: 2669.9784s\n",
            "#352 Loss: 5.1859 Acc: 55.4391% Time: 2677.6006s\n",
            "#353 Loss: 5.1984 Acc: 57.1856% Time: 2685.1370s\n",
            "#354 Loss: 4.6751 Acc: 59.0319% Time: 2692.7654s\n",
            "#355 Loss: 5.4461 Acc: 56.9361% Time: 2700.2386s\n",
            "#356 Loss: 4.4493 Acc: 57.6846% Time: 2707.7663s\n",
            "#357 Loss: 4.9310 Acc: 57.5848% Time: 2715.2972s\n",
            "#358 Loss: 5.2430 Acc: 56.6367% Time: 2722.7674s\n",
            "#359 Loss: 5.9898 Acc: 54.3912% Time: 2730.2971s\n",
            "#360 Loss: 4.9753 Acc: 55.9880% Time: 2737.9026s\n",
            "#361 Loss: 5.3410 Acc: 55.5888% Time: 2745.3526s\n",
            "#362 Loss: 4.9886 Acc: 58.9321% Time: 2752.9461s\n",
            "#363 Loss: 4.0372 Acc: 59.8802% Time: 2760.5165s\n",
            "#364 Loss: 4.6806 Acc: 56.7864% Time: 2768.1294s\n",
            "#365 Loss: 4.7688 Acc: 57.7844% Time: 2775.7117s\n",
            "#366 Loss: 6.6978 Acc: 53.8423% Time: 2783.2764s\n",
            "#367 Loss: 5.3459 Acc: 55.4890% Time: 2790.8391s\n",
            "#368 Loss: 4.7757 Acc: 56.6367% Time: 2798.4507s\n",
            "#369 Loss: 6.0045 Acc: 55.8882% Time: 2805.8398s\n",
            "#370 Loss: 4.1132 Acc: 59.1317% Time: 2813.4998s\n",
            "#371 Loss: 4.6261 Acc: 57.3852% Time: 2821.0896s\n",
            "#372 Loss: 3.9181 Acc: 57.2355% Time: 2828.6743s\n",
            "#373 Loss: 4.6998 Acc: 55.8882% Time: 2836.3041s\n",
            "#374 Loss: 4.0231 Acc: 59.4311% Time: 2843.8294s\n",
            "#375 Loss: 4.6797 Acc: 55.3892% Time: 2851.2982s\n",
            "#376 Loss: 5.9413 Acc: 55.8882% Time: 2858.9025s\n",
            "#377 Loss: 4.7658 Acc: 57.7844% Time: 2866.4910s\n",
            "#378 Loss: 4.3588 Acc: 57.0359% Time: 2874.0596s\n",
            "#379 Loss: 4.6681 Acc: 58.3333% Time: 2881.6834s\n",
            "#380 Loss: 4.6593 Acc: 57.0359% Time: 2889.1996s\n",
            "#381 Loss: 4.7309 Acc: 58.2335% Time: 2896.8742s\n",
            "#382 Loss: 5.0833 Acc: 54.6906% Time: 2904.4763s\n",
            "#383 Loss: 5.6563 Acc: 58.0838% Time: 2912.0041s\n",
            "#384 Loss: 5.6034 Acc: 56.4870% Time: 2919.7096s\n",
            "#385 Loss: 6.1772 Acc: 58.8822% Time: 2927.2486s\n",
            "#386 Loss: 4.3690 Acc: 57.6347% Time: 2934.7419s\n",
            "#387 Loss: 3.9914 Acc: 55.8383% Time: 2942.2329s\n",
            "#388 Loss: 5.7899 Acc: 57.1357% Time: 2949.8328s\n",
            "#389 Loss: 5.3837 Acc: 56.7864% Time: 2957.4350s\n",
            "#390 Loss: 3.9064 Acc: 58.8323% Time: 2965.0161s\n",
            "#391 Loss: 5.7098 Acc: 55.5389% Time: 2972.5877s\n",
            "#392 Loss: 4.2612 Acc: 57.7345% Time: 2980.0224s\n",
            "#393 Loss: 6.5236 Acc: 54.9900% Time: 2987.6012s\n",
            "#394 Loss: 6.0500 Acc: 55.6886% Time: 2995.0496s\n",
            "#395 Loss: 4.9719 Acc: 57.1856% Time: 3002.5693s\n",
            "#396 Loss: 5.0120 Acc: 55.8383% Time: 3010.0464s\n",
            "#397 Loss: 4.7505 Acc: 56.9860% Time: 3017.5667s\n",
            "#398 Loss: 5.2089 Acc: 57.2355% Time: 3025.0516s\n",
            "#399 Loss: 5.2084 Acc: 56.9860% Time: 3032.5814s\n",
            "#400 Loss: 4.8346 Acc: 58.3832% Time: 3040.1909s\n",
            "#401 Loss: 4.6832 Acc: 57.2355% Time: 3047.7887s\n",
            "#402 Loss: 4.1517 Acc: 58.6327% Time: 3055.2502s\n",
            "#403 Loss: 5.2003 Acc: 55.4890% Time: 3062.8448s\n",
            "#404 Loss: 6.4191 Acc: 56.9860% Time: 3070.4053s\n",
            "#405 Loss: 5.4318 Acc: 58.4830% Time: 3077.8480s\n",
            "#406 Loss: 4.3054 Acc: 56.6866% Time: 3085.3959s\n",
            "#407 Loss: 4.2545 Acc: 58.5828% Time: 3092.8593s\n",
            "#408 Loss: 4.8591 Acc: 56.0878% Time: 3100.4283s\n",
            "#409 Loss: 4.9308 Acc: 55.5888% Time: 3108.0259s\n",
            "#410 Loss: 5.0099 Acc: 58.6826% Time: 3115.5712s\n",
            "#411 Loss: 5.0040 Acc: 58.2335% Time: 3123.1905s\n",
            "#412 Loss: 4.8100 Acc: 56.7864% Time: 3130.7850s\n",
            "#413 Loss: 4.7643 Acc: 55.9381% Time: 3138.3868s\n",
            "#414 Loss: 5.2511 Acc: 57.0858% Time: 3145.9323s\n",
            "#415 Loss: 5.2751 Acc: 58.1337% Time: 3153.4151s\n",
            "#416 Loss: 5.5453 Acc: 57.5349% Time: 3160.9418s\n",
            "#417 Loss: 5.2439 Acc: 57.8842% Time: 3168.4065s\n",
            "#418 Loss: 4.8107 Acc: 58.7824% Time: 3175.9111s\n",
            "#419 Loss: 5.3488 Acc: 55.6886% Time: 3183.5834s\n",
            "#420 Loss: 4.6689 Acc: 58.9820% Time: 3191.1126s\n",
            "#421 Loss: 4.5955 Acc: 56.5369% Time: 3198.7338s\n",
            "#422 Loss: 4.8304 Acc: 56.0379% Time: 3206.4098s\n",
            "#423 Loss: 5.6819 Acc: 57.4351% Time: 3213.9154s\n",
            "#424 Loss: 5.4421 Acc: 58.2335% Time: 3221.5649s\n",
            "#425 Loss: 5.5981 Acc: 56.5868% Time: 3229.1509s\n",
            "#426 Loss: 5.6474 Acc: 56.2874% Time: 3236.7044s\n",
            "#427 Loss: 5.1846 Acc: 57.0858% Time: 3244.1535s\n",
            "#428 Loss: 5.0342 Acc: 56.8363% Time: 3251.7493s\n",
            "#429 Loss: 5.6844 Acc: 58.3333% Time: 3259.3399s\n",
            "#430 Loss: 4.8227 Acc: 56.6367% Time: 3266.9449s\n",
            "#431 Loss: 5.8835 Acc: 57.6347% Time: 3274.3222s\n",
            "#432 Loss: 4.4018 Acc: 58.1836% Time: 3281.9275s\n",
            "#433 Loss: 4.8750 Acc: 56.6367% Time: 3289.5318s\n",
            "#434 Loss: 4.8720 Acc: 58.0339% Time: 3297.0791s\n",
            "#435 Loss: 5.1338 Acc: 56.4371% Time: 3304.7187s\n",
            "#436 Loss: 4.5795 Acc: 58.4830% Time: 3312.3161s\n",
            "#437 Loss: 4.6716 Acc: 59.0319% Time: 3319.7720s\n",
            "#438 Loss: 5.6485 Acc: 54.4910% Time: 3327.3182s\n",
            "#439 Loss: 4.8846 Acc: 56.9860% Time: 3334.9300s\n",
            "#440 Loss: 5.9981 Acc: 56.4870% Time: 3342.5531s\n",
            "#441 Loss: 4.3666 Acc: 58.7824% Time: 3350.0847s\n",
            "#442 Loss: 5.0048 Acc: 58.2335% Time: 3357.7066s\n",
            "#443 Loss: 5.4795 Acc: 56.1377% Time: 3365.3150s\n",
            "#444 Loss: 4.8955 Acc: 55.8383% Time: 3372.9055s\n",
            "#445 Loss: 4.5008 Acc: 57.9341% Time: 3380.4446s\n",
            "#446 Loss: 5.5351 Acc: 56.0878% Time: 3388.1280s\n",
            "#447 Loss: 5.3589 Acc: 54.7405% Time: 3395.6992s\n",
            "#448 Loss: 4.4379 Acc: 58.9820% Time: 3403.2564s\n",
            "#449 Loss: 4.3951 Acc: 58.9321% Time: 3410.9499s\n",
            "#450 Loss: 4.3859 Acc: 58.2834% Time: 3418.4964s\n",
            "#451 Loss: 4.3756 Acc: 56.9361% Time: 3426.0504s\n",
            "#452 Loss: 5.6764 Acc: 55.3393% Time: 3433.4822s\n",
            "#453 Loss: 4.5197 Acc: 57.0359% Time: 3441.0665s\n",
            "#454 Loss: 4.9931 Acc: 58.6826% Time: 3448.5054s\n",
            "#455 Loss: 4.4796 Acc: 57.7844% Time: 3456.0963s\n",
            "#456 Loss: 4.9720 Acc: 56.4371% Time: 3463.6240s\n",
            "#457 Loss: 5.2894 Acc: 59.0319% Time: 3471.1224s\n",
            "#458 Loss: 4.8556 Acc: 58.1337% Time: 3478.6710s\n",
            "#459 Loss: 4.6191 Acc: 57.9341% Time: 3486.2961s\n",
            "#460 Loss: 5.5746 Acc: 55.0399% Time: 3493.8158s\n",
            "#461 Loss: 4.1825 Acc: 57.5349% Time: 3501.4620s\n",
            "#462 Loss: 5.0077 Acc: 57.2854% Time: 3509.0877s\n",
            "#463 Loss: 3.8358 Acc: 57.6846% Time: 3516.6174s\n",
            "#464 Loss: 4.9111 Acc: 58.1836% Time: 3524.2351s\n",
            "#465 Loss: 3.8973 Acc: 59.5808% Time: 3531.6883s\n",
            "#466 Loss: 4.1387 Acc: 57.7345% Time: 3539.2523s\n",
            "#467 Loss: 4.9035 Acc: 57.0359% Time: 3546.8241s\n",
            "#468 Loss: 5.1632 Acc: 56.5868% Time: 3554.4189s\n",
            "#469 Loss: 4.7146 Acc: 56.1377% Time: 3561.8337s\n",
            "#470 Loss: 4.0842 Acc: 58.4331% Time: 3569.2513s\n",
            "#471 Loss: 5.0487 Acc: 56.8363% Time: 3576.8426s\n",
            "#472 Loss: 5.5156 Acc: 56.0379% Time: 3584.3900s\n",
            "#473 Loss: 5.3130 Acc: 55.9381% Time: 3591.8592s\n",
            "#474 Loss: 4.6110 Acc: 58.4331% Time: 3599.3896s\n",
            "#475 Loss: 5.8122 Acc: 55.3892% Time: 3606.8397s\n",
            "#476 Loss: 3.9291 Acc: 57.8842% Time: 3614.4521s\n",
            "#477 Loss: 4.9590 Acc: 58.1337% Time: 3622.0237s\n",
            "#478 Loss: 4.5648 Acc: 56.3872% Time: 3629.6129s\n",
            "#479 Loss: 5.1155 Acc: 56.6367% Time: 3637.2290s\n",
            "#480 Loss: 5.0668 Acc: 55.2894% Time: 3644.8342s\n",
            "#481 Loss: 4.6909 Acc: 57.6846% Time: 3652.4944s\n",
            "#482 Loss: 4.3018 Acc: 56.3373% Time: 3659.9886s\n",
            "#483 Loss: 4.5646 Acc: 57.7844% Time: 3667.6031s\n",
            "#484 Loss: 4.9155 Acc: 57.3852% Time: 3675.1890s\n",
            "#485 Loss: 5.4290 Acc: 57.5349% Time: 3682.7788s\n",
            "#486 Loss: 5.0074 Acc: 57.8343% Time: 3690.4128s\n",
            "#487 Loss: 5.3306 Acc: 55.4890% Time: 3697.9922s\n",
            "#488 Loss: 4.4506 Acc: 56.5369% Time: 3705.5565s\n",
            "#489 Loss: 4.6852 Acc: 56.4371% Time: 3713.0012s\n",
            "#490 Loss: 3.9157 Acc: 56.4870% Time: 3720.5915s\n",
            "#491 Loss: 5.4851 Acc: 57.8842% Time: 3728.1663s\n",
            "#492 Loss: 3.8483 Acc: 59.3812% Time: 3735.7889s\n",
            "#493 Loss: 3.9859 Acc: 57.6846% Time: 3743.4284s\n",
            "#494 Loss: 5.1774 Acc: 54.9401% Time: 3751.0026s\n",
            "#495 Loss: 5.7275 Acc: 56.4371% Time: 3758.5965s\n",
            "#496 Loss: 5.0930 Acc: 55.7385% Time: 3766.1061s\n",
            "#497 Loss: 4.4822 Acc: 56.2874% Time: 3773.6344s\n",
            "#498 Loss: 4.9372 Acc: 55.9880% Time: 3781.1561s\n",
            "#499 Loss: 4.9560 Acc: 58.6327% Time: 3788.7612s\n",
            "#500 Loss: 4.8679 Acc: 55.9381% Time: 3796.3412s\n",
            "#501 Loss: 4.7201 Acc: 56.9860% Time: 3803.9169s\n",
            "#502 Loss: 4.6031 Acc: 57.4351% Time: 3811.4198s\n",
            "#503 Loss: 4.0302 Acc: 56.8363% Time: 3818.9365s\n",
            "#504 Loss: 6.0083 Acc: 55.1896% Time: 3826.3775s\n",
            "#505 Loss: 4.2571 Acc: 56.2874% Time: 3833.9042s\n",
            "#506 Loss: 4.4333 Acc: 57.6347% Time: 3841.5461s\n",
            "#507 Loss: 4.8364 Acc: 58.0838% Time: 3849.1450s\n",
            "#508 Loss: 4.3605 Acc: 57.2355% Time: 3856.7391s\n",
            "#509 Loss: 4.3452 Acc: 57.1856% Time: 3864.3668s\n",
            "#510 Loss: 4.7915 Acc: 57.8343% Time: 3871.9298s\n",
            "#511 Loss: 5.2308 Acc: 55.7884% Time: 3879.5714s\n",
            "#512 Loss: 5.0251 Acc: 54.8902% Time: 3887.1480s\n",
            "#513 Loss: 4.9100 Acc: 56.3872% Time: 3894.7324s\n",
            "#514 Loss: 5.0922 Acc: 55.4890% Time: 3902.1343s\n",
            "#515 Loss: 4.6034 Acc: 56.2874% Time: 3909.6845s\n",
            "#516 Loss: 4.7420 Acc: 57.5349% Time: 3917.1225s\n",
            "#517 Loss: 4.3775 Acc: 57.5349% Time: 3924.7312s\n",
            "#518 Loss: 5.0629 Acc: 56.1377% Time: 3932.2933s\n",
            "#519 Loss: 5.5230 Acc: 58.0339% Time: 3939.9095s\n",
            "#520 Loss: 5.0868 Acc: 55.2894% Time: 3947.3362s\n",
            "#521 Loss: 5.0556 Acc: 57.8343% Time: 3954.9519s\n",
            "#522 Loss: 5.4090 Acc: 56.7864% Time: 3962.5562s\n",
            "#523 Loss: 5.0560 Acc: 57.9341% Time: 3970.0727s\n",
            "#524 Loss: 4.7380 Acc: 58.0339% Time: 3977.5637s\n",
            "#525 Loss: 5.7290 Acc: 57.0359% Time: 3985.0535s\n",
            "#526 Loss: 5.1647 Acc: 56.6866% Time: 3992.5173s\n",
            "#527 Loss: 4.7497 Acc: 58.7325% Time: 4000.0840s\n",
            "#528 Loss: 4.6540 Acc: 57.3852% Time: 4007.7026s\n",
            "#529 Loss: 4.4669 Acc: 56.8862% Time: 4015.2407s\n",
            "#530 Loss: 4.5642 Acc: 56.9361% Time: 4022.7174s\n",
            "#531 Loss: 5.8039 Acc: 58.0838% Time: 4030.2872s\n",
            "#532 Loss: 5.5298 Acc: 57.2355% Time: 4037.8561s\n",
            "#533 Loss: 5.0933 Acc: 56.7864% Time: 4045.2990s\n",
            "#534 Loss: 4.3109 Acc: 58.4830% Time: 4052.8586s\n",
            "#535 Loss: 4.6123 Acc: 58.0838% Time: 4060.2842s\n",
            "#536 Loss: 4.0164 Acc: 58.3333% Time: 4067.8597s\n",
            "#537 Loss: 4.1968 Acc: 57.3852% Time: 4075.4687s\n",
            "#538 Loss: 4.5574 Acc: 57.4850% Time: 4083.0610s\n",
            "#539 Loss: 4.7260 Acc: 55.6886% Time: 4090.4882s\n",
            "#540 Loss: 4.4161 Acc: 57.2355% Time: 4098.0686s\n",
            "#541 Loss: 5.3672 Acc: 55.9880% Time: 4105.6237s\n",
            "#542 Loss: 5.7200 Acc: 55.8383% Time: 4113.2205s\n",
            "#543 Loss: 4.7916 Acc: 56.6866% Time: 4120.6590s\n",
            "#544 Loss: 4.4102 Acc: 56.8363% Time: 4128.2560s\n",
            "#545 Loss: 3.9877 Acc: 58.0339% Time: 4135.6822s\n",
            "#546 Loss: 4.0627 Acc: 59.2315% Time: 4143.2640s\n",
            "#547 Loss: 4.7307 Acc: 58.0838% Time: 4150.9461s\n",
            "#548 Loss: 4.8900 Acc: 56.9361% Time: 4158.4515s\n",
            "#549 Loss: 5.0471 Acc: 57.8343% Time: 4166.0436s\n",
            "#550 Loss: 4.4844 Acc: 58.5329% Time: 4173.6849s\n",
            "#551 Loss: 4.8937 Acc: 58.7325% Time: 4181.2106s\n",
            "#552 Loss: 5.0210 Acc: 56.5369% Time: 4188.6696s\n",
            "#553 Loss: 4.6066 Acc: 58.6826% Time: 4196.2537s\n",
            "#554 Loss: 4.4955 Acc: 57.4850% Time: 4203.8463s\n",
            "#555 Loss: 5.3140 Acc: 56.8862% Time: 4211.4080s\n",
            "#556 Loss: 4.2613 Acc: 59.3313% Time: 4219.0765s\n",
            "#557 Loss: 4.2107 Acc: 56.9361% Time: 4226.6378s\n",
            "#558 Loss: 4.7045 Acc: 56.5369% Time: 4234.2620s\n",
            "#559 Loss: 5.3337 Acc: 58.2335% Time: 4241.7662s\n",
            "#560 Loss: 4.6227 Acc: 59.5808% Time: 4249.2589s\n",
            "#561 Loss: 5.8608 Acc: 55.4391% Time: 4256.8033s\n",
            "#562 Loss: 6.1779 Acc: 55.2395% Time: 4264.4273s\n",
            "#563 Loss: 5.8897 Acc: 56.1876% Time: 4272.0130s\n",
            "#564 Loss: 4.9087 Acc: 55.9880% Time: 4279.4776s\n",
            "#565 Loss: 4.5670 Acc: 55.5888% Time: 4286.9861s\n",
            "#566 Loss: 4.9861 Acc: 56.5868% Time: 4294.5646s\n",
            "#567 Loss: 4.8165 Acc: 57.5848% Time: 4302.2113s\n",
            "#568 Loss: 5.7240 Acc: 56.9361% Time: 4309.7868s\n",
            "#569 Loss: 4.5510 Acc: 57.6347% Time: 4317.1975s\n",
            "#570 Loss: 5.3400 Acc: 55.5389% Time: 4324.7892s\n",
            "#571 Loss: 4.5067 Acc: 57.7844% Time: 4332.2006s\n",
            "#572 Loss: 4.9254 Acc: 57.9840% Time: 4339.5893s\n",
            "#573 Loss: 5.3351 Acc: 55.7385% Time: 4347.2469s\n",
            "#574 Loss: 5.7147 Acc: 54.6906% Time: 4354.7619s\n",
            "#575 Loss: 5.2320 Acc: 56.7365% Time: 4362.1481s\n",
            "#576 Loss: 5.8316 Acc: 53.2435% Time: 4369.7608s\n",
            "#577 Loss: 5.4464 Acc: 57.5349% Time: 4377.3891s\n",
            "#578 Loss: 5.2119 Acc: 56.8363% Time: 4384.9562s\n",
            "#579 Loss: 4.5183 Acc: 59.3313% Time: 4392.3716s\n",
            "#580 Loss: 5.0411 Acc: 57.4850% Time: 4399.8048s\n",
            "#581 Loss: 4.5056 Acc: 57.6347% Time: 4407.4034s\n",
            "#582 Loss: 4.8325 Acc: 58.1337% Time: 4414.9607s\n",
            "#583 Loss: 4.8911 Acc: 57.4351% Time: 4422.5881s\n",
            "#584 Loss: 4.7721 Acc: 56.0878% Time: 4430.0902s\n",
            "#585 Loss: 5.2372 Acc: 55.7884% Time: 4437.6316s\n",
            "#586 Loss: 5.9486 Acc: 56.5369% Time: 4445.2025s\n",
            "#587 Loss: 5.0134 Acc: 55.6387% Time: 4452.7578s\n",
            "#588 Loss: 4.5344 Acc: 58.4830% Time: 4460.3639s\n",
            "#589 Loss: 4.7472 Acc: 57.7844% Time: 4467.9742s\n",
            "#590 Loss: 3.9152 Acc: 56.5868% Time: 4475.5649s\n",
            "#591 Loss: 5.1223 Acc: 57.6347% Time: 4483.1768s\n",
            "#592 Loss: 5.3237 Acc: 56.9361% Time: 4490.7793s\n",
            "#593 Loss: 5.7733 Acc: 55.7884% Time: 4498.3466s\n",
            "#594 Loss: 4.7619 Acc: 58.8323% Time: 4505.9527s\n",
            "#595 Loss: 4.7136 Acc: 56.4870% Time: 4513.5174s\n",
            "#596 Loss: 5.3528 Acc: 57.2355% Time: 4521.1174s\n",
            "#597 Loss: 4.5775 Acc: 58.6826% Time: 4528.7035s\n",
            "#598 Loss: 5.8390 Acc: 57.2355% Time: 4536.1448s\n",
            "#599 Loss: 5.0930 Acc: 57.4850% Time: 4543.7594s\n",
            "#600 Loss: 4.4032 Acc: 58.4830% Time: 4551.2640s\n",
            "#601 Loss: 5.1041 Acc: 56.7365% Time: 4558.7194s\n",
            "#602 Loss: 5.3650 Acc: 56.6866% Time: 4566.3179s\n",
            "#603 Loss: 4.4016 Acc: 56.6866% Time: 4573.9694s\n",
            "#604 Loss: 3.8911 Acc: 58.1337% Time: 4581.5015s\n",
            "#605 Loss: 4.0331 Acc: 59.8802% Time: 4589.0659s\n",
            "#606 Loss: 5.1928 Acc: 54.9900% Time: 4596.5174s\n",
            "#607 Loss: 4.1961 Acc: 58.2335% Time: 4604.0909s\n",
            "#608 Loss: 5.3737 Acc: 57.8343% Time: 4611.6754s\n",
            "#609 Loss: 4.1464 Acc: 57.9341% Time: 4619.1567s\n",
            "#610 Loss: 5.2647 Acc: 55.3393% Time: 4626.7390s\n",
            "#611 Loss: 4.5449 Acc: 57.0858% Time: 4634.3283s\n",
            "#612 Loss: 4.5770 Acc: 56.1377% Time: 4641.8921s\n",
            "#613 Loss: 5.7801 Acc: 54.5409% Time: 4649.4881s\n",
            "#614 Loss: 4.9709 Acc: 58.2335% Time: 4657.1370s\n",
            "#615 Loss: 5.4191 Acc: 55.9381% Time: 4664.6617s\n",
            "#616 Loss: 5.0076 Acc: 56.3872% Time: 4672.2797s\n",
            "#617 Loss: 5.3416 Acc: 53.3433% Time: 4679.8537s\n",
            "#618 Loss: 5.4895 Acc: 57.5349% Time: 4687.3283s\n",
            "#619 Loss: 4.8837 Acc: 58.3333% Time: 4694.8855s\n",
            "#620 Loss: 4.6453 Acc: 56.7864% Time: 4702.4450s\n",
            "#621 Loss: 4.2000 Acc: 58.7325% Time: 4709.8546s\n",
            "#622 Loss: 5.5556 Acc: 56.0379% Time: 4717.5506s\n",
            "#623 Loss: 5.0610 Acc: 56.5868% Time: 4725.0582s\n",
            "#624 Loss: 4.7142 Acc: 58.2834% Time: 4732.7096s\n",
            "#625 Loss: 5.5230 Acc: 56.1876% Time: 4740.2611s\n",
            "#626 Loss: 4.9245 Acc: 57.7844% Time: 4747.8824s\n",
            "#627 Loss: 4.7426 Acc: 56.2375% Time: 4755.5398s\n",
            "#628 Loss: 4.5234 Acc: 58.3832% Time: 4763.0168s\n",
            "#629 Loss: 4.7815 Acc: 58.1337% Time: 4770.4778s\n",
            "#630 Loss: 5.3406 Acc: 55.4391% Time: 4778.0743s\n",
            "#631 Loss: 4.4731 Acc: 56.6866% Time: 4785.6746s\n",
            "#632 Loss: 5.3740 Acc: 56.4371% Time: 4793.2203s\n",
            "#633 Loss: 4.4969 Acc: 58.3333% Time: 4800.8484s\n",
            "#634 Loss: 5.2367 Acc: 56.9361% Time: 4808.3969s\n",
            "#635 Loss: 4.8712 Acc: 57.1856% Time: 4815.8704s\n",
            "#636 Loss: 6.2058 Acc: 55.5888% Time: 4823.4673s\n",
            "#637 Loss: 4.8052 Acc: 57.5848% Time: 4830.8722s\n",
            "#638 Loss: 4.9464 Acc: 58.2834% Time: 4838.4038s\n",
            "#639 Loss: 4.6611 Acc: 57.8343% Time: 4846.0182s\n",
            "#640 Loss: 5.2233 Acc: 57.2854% Time: 4853.5798s\n",
            "#641 Loss: 4.6911 Acc: 57.0858% Time: 4861.0807s\n",
            "#642 Loss: 5.1720 Acc: 55.6387% Time: 4868.6145s\n",
            "#643 Loss: 4.8648 Acc: 57.5848% Time: 4876.2122s\n",
            "#644 Loss: 4.8260 Acc: 56.2375% Time: 4883.6387s\n",
            "#645 Loss: 4.6299 Acc: 57.8842% Time: 4891.2565s\n",
            "#646 Loss: 5.1880 Acc: 55.7884% Time: 4898.7409s\n",
            "#647 Loss: 4.9380 Acc: 58.0339% Time: 4906.2358s\n",
            "#648 Loss: 4.1623 Acc: 57.5848% Time: 4913.7656s\n",
            "#649 Loss: 4.7225 Acc: 57.2355% Time: 4921.1989s\n",
            "#650 Loss: 4.5609 Acc: 56.7365% Time: 4928.7947s\n",
            "#651 Loss: 4.9281 Acc: 57.9840% Time: 4936.3823s\n",
            "#652 Loss: 4.4943 Acc: 57.1357% Time: 4943.8096s\n",
            "#653 Loss: 4.9832 Acc: 56.4371% Time: 4951.3358s\n",
            "#654 Loss: 4.5766 Acc: 56.9860% Time: 4958.9909s\n",
            "#655 Loss: 5.7200 Acc: 56.0379% Time: 4966.5938s\n",
            "#656 Loss: 4.6205 Acc: 57.8343% Time: 4974.1907s\n",
            "#657 Loss: 4.5726 Acc: 57.2854% Time: 4981.7877s\n",
            "#658 Loss: 5.9758 Acc: 56.9860% Time: 4989.3581s\n",
            "#659 Loss: 4.3782 Acc: 59.9800% Time: 4996.9282s\n",
            "#660 Loss: 4.5972 Acc: 58.4331% Time: 5004.4025s\n",
            "#661 Loss: 4.7845 Acc: 58.6327% Time: 5011.8985s\n",
            "#662 Loss: 4.0565 Acc: 58.4331% Time: 5019.4001s\n",
            "#663 Loss: 4.3684 Acc: 58.9321% Time: 5026.8933s\n",
            "#664 Loss: 5.9619 Acc: 56.7365% Time: 5034.4136s\n",
            "#665 Loss: 4.6762 Acc: 56.5369% Time: 5041.9469s\n",
            "#666 Loss: 4.6741 Acc: 57.5848% Time: 5049.5509s\n",
            "#667 Loss: 4.8518 Acc: 56.8363% Time: 5057.1367s\n",
            "#668 Loss: 4.1548 Acc: 58.9820% Time: 5064.7505s\n",
            "#669 Loss: 4.8838 Acc: 57.7345% Time: 5072.4439s\n",
            "#670 Loss: 5.2131 Acc: 55.1397% Time: 5080.0064s\n",
            "#671 Loss: 4.7515 Acc: 57.1856% Time: 5087.5957s\n",
            "#672 Loss: 4.7511 Acc: 55.9381% Time: 5095.1405s\n",
            "#673 Loss: 5.1335 Acc: 55.6886% Time: 5102.5543s\n",
            "#674 Loss: 4.7553 Acc: 55.8882% Time: 5110.1277s\n",
            "#675 Loss: 6.0537 Acc: 56.6866% Time: 5117.7320s\n",
            "#676 Loss: 4.0141 Acc: 58.8822% Time: 5125.2907s\n",
            "#677 Loss: 4.7560 Acc: 57.5848% Time: 5132.9120s\n",
            "#678 Loss: 5.1750 Acc: 56.8363% Time: 5140.4966s\n",
            "#679 Loss: 4.7951 Acc: 57.2355% Time: 5148.0886s\n",
            "#680 Loss: 5.7675 Acc: 56.8862% Time: 5155.7552s\n",
            "#681 Loss: 5.0886 Acc: 55.8383% Time: 5163.3207s\n",
            "#682 Loss: 4.7198 Acc: 57.5848% Time: 5170.8640s\n",
            "#683 Loss: 5.7227 Acc: 56.4870% Time: 5178.5091s\n",
            "#684 Loss: 5.5475 Acc: 56.8862% Time: 5186.1137s\n",
            "#685 Loss: 5.7028 Acc: 56.3373% Time: 5193.7372s\n",
            "#686 Loss: 4.3791 Acc: 57.3353% Time: 5201.3187s\n",
            "#687 Loss: 5.2312 Acc: 58.5329% Time: 5208.9279s\n",
            "#688 Loss: 5.2694 Acc: 55.8383% Time: 5216.4492s\n",
            "#689 Loss: 5.7160 Acc: 55.5888% Time: 5223.9593s\n",
            "#690 Loss: 3.7417 Acc: 59.5808% Time: 5231.4757s\n",
            "#691 Loss: 4.6167 Acc: 56.5868% Time: 5239.0861s\n",
            "#692 Loss: 4.5583 Acc: 56.4371% Time: 5246.7225s\n",
            "#693 Loss: 5.0170 Acc: 57.8842% Time: 5254.2946s\n",
            "#694 Loss: 4.6090 Acc: 57.5848% Time: 5261.9253s\n",
            "#695 Loss: 4.5872 Acc: 57.0858% Time: 5269.4614s\n",
            "#696 Loss: 4.4804 Acc: 59.0319% Time: 5277.0809s\n",
            "#697 Loss: 4.5057 Acc: 58.1337% Time: 5284.6979s\n",
            "#698 Loss: 4.3826 Acc: 57.5848% Time: 5292.2838s\n",
            "#699 Loss: 5.7260 Acc: 55.5888% Time: 5299.9487s\n",
            "#700 Loss: 5.1737 Acc: 55.6387% Time: 5307.4690s\n",
            "#701 Loss: 4.7098 Acc: 57.8343% Time: 5315.1012s\n",
            "#702 Loss: 4.6479 Acc: 56.8862% Time: 5322.6493s\n",
            "#703 Loss: 4.1948 Acc: 57.1357% Time: 5330.1165s\n",
            "#704 Loss: 4.9341 Acc: 57.4351% Time: 5337.6279s\n",
            "#705 Loss: 5.8457 Acc: 54.0918% Time: 5345.0916s\n",
            "#706 Loss: 6.0240 Acc: 54.9401% Time: 5352.6426s\n",
            "#707 Loss: 4.2283 Acc: 59.0818% Time: 5360.2471s\n",
            "#708 Loss: 4.6732 Acc: 57.1856% Time: 5367.8971s\n",
            "#709 Loss: 4.2256 Acc: 56.4371% Time: 5375.4129s\n",
            "#710 Loss: 4.8452 Acc: 55.6886% Time: 5382.8894s\n",
            "#711 Loss: 4.7152 Acc: 58.9321% Time: 5390.4427s\n",
            "#712 Loss: 5.2009 Acc: 56.7864% Time: 5398.0956s\n",
            "#713 Loss: 4.8160 Acc: 57.7844% Time: 5405.6906s\n",
            "#714 Loss: 5.3764 Acc: 56.2375% Time: 5413.2736s\n",
            "#715 Loss: 5.7519 Acc: 57.2854% Time: 5420.9013s\n",
            "#716 Loss: 5.4977 Acc: 57.2355% Time: 5428.4787s\n",
            "#717 Loss: 4.9953 Acc: 57.9840% Time: 5436.0624s\n",
            "#718 Loss: 4.2006 Acc: 57.5848% Time: 5443.5888s\n",
            "#719 Loss: 4.0385 Acc: 59.2315% Time: 5451.0805s\n",
            "#720 Loss: 5.1456 Acc: 56.4870% Time: 5458.5980s\n",
            "#721 Loss: 5.0713 Acc: 56.3373% Time: 5466.2359s\n",
            "#722 Loss: 4.2771 Acc: 57.3852% Time: 5473.7947s\n",
            "#723 Loss: 5.8454 Acc: 55.5888% Time: 5481.4522s\n",
            "#724 Loss: 5.3977 Acc: 56.6866% Time: 5488.9699s\n",
            "#725 Loss: 4.2965 Acc: 57.2854% Time: 5496.4308s\n",
            "#726 Loss: 4.7250 Acc: 58.4331% Time: 5503.7895s\n",
            "#727 Loss: 4.8311 Acc: 58.7325% Time: 5511.4119s\n",
            "#728 Loss: 4.7879 Acc: 57.6846% Time: 5518.9990s\n",
            "#729 Loss: 4.7855 Acc: 57.3353% Time: 5526.6022s\n",
            "#730 Loss: 5.4238 Acc: 58.0339% Time: 5534.3118s\n",
            "#731 Loss: 6.3792 Acc: 57.1856% Time: 5541.8361s\n",
            "#732 Loss: 6.8486 Acc: 58.7824% Time: 5549.3935s\n",
            "#733 Loss: 5.5987 Acc: 57.2854% Time: 5556.9718s\n",
            "#734 Loss: 4.6329 Acc: 58.7824% Time: 5564.3982s\n",
            "#735 Loss: 5.6576 Acc: 53.9421% Time: 5572.0229s\n",
            "#736 Loss: 5.0827 Acc: 57.1357% Time: 5579.6007s\n",
            "#737 Loss: 4.3350 Acc: 57.7844% Time: 5587.1482s\n",
            "#738 Loss: 5.1187 Acc: 56.5369% Time: 5594.7583s\n",
            "#739 Loss: 5.1789 Acc: 56.3872% Time: 5602.1875s\n",
            "#740 Loss: 4.9637 Acc: 55.8882% Time: 5609.7976s\n",
            "#741 Loss: 4.5013 Acc: 57.1856% Time: 5617.4394s\n",
            "#742 Loss: 4.4255 Acc: 57.1357% Time: 5624.9502s\n",
            "#743 Loss: 4.7464 Acc: 56.7864% Time: 5632.5788s\n",
            "#744 Loss: 5.0051 Acc: 56.0878% Time: 5640.1741s\n",
            "#745 Loss: 5.5240 Acc: 55.3393% Time: 5647.7611s\n",
            "#746 Loss: 5.1112 Acc: 57.8343% Time: 5655.4048s\n",
            "#747 Loss: 5.0914 Acc: 57.1856% Time: 5662.9638s\n",
            "#748 Loss: 4.9811 Acc: 56.9361% Time: 5670.5579s\n",
            "#749 Loss: 4.6154 Acc: 57.7844% Time: 5678.1258s\n",
            "#750 Loss: 4.5575 Acc: 55.2395% Time: 5685.8491s\n",
            "#751 Loss: 5.3416 Acc: 56.0878% Time: 5693.6288s\n",
            "#752 Loss: 4.4490 Acc: 58.0838% Time: 5701.1450s\n",
            "#753 Loss: 4.4476 Acc: 56.7365% Time: 5708.8497s\n",
            "#754 Loss: 4.7112 Acc: 55.8383% Time: 5716.7767s\n",
            "#755 Loss: 4.5682 Acc: 58.1337% Time: 5724.7537s\n",
            "#756 Loss: 4.7923 Acc: 56.1377% Time: 5732.6212s\n",
            "#757 Loss: 4.5491 Acc: 54.8902% Time: 5740.5474s\n",
            "#758 Loss: 4.0013 Acc: 58.1337% Time: 5748.1883s\n",
            "#759 Loss: 4.8083 Acc: 55.2894% Time: 5755.7798s\n",
            "#760 Loss: 4.0423 Acc: 57.2854% Time: 5763.6350s\n",
            "#761 Loss: 4.4050 Acc: 58.2834% Time: 5771.2004s\n",
            "#762 Loss: 5.3561 Acc: 55.8882% Time: 5778.9156s\n",
            "#763 Loss: 5.3041 Acc: 57.6846% Time: 5786.6297s\n",
            "#764 Loss: 5.0228 Acc: 57.5848% Time: 5794.0972s\n",
            "#765 Loss: 4.6496 Acc: 56.6367% Time: 5801.7516s\n",
            "#766 Loss: 5.3900 Acc: 55.6886% Time: 5809.2702s\n",
            "#767 Loss: 5.0642 Acc: 59.2814% Time: 5816.8851s\n",
            "#768 Loss: 5.1544 Acc: 57.8842% Time: 5824.4857s\n",
            "#769 Loss: 4.7629 Acc: 59.6307% Time: 5832.2726s\n",
            "#770 Loss: 5.0830 Acc: 56.4371% Time: 5839.7716s\n",
            "#771 Loss: 4.6788 Acc: 58.5329% Time: 5847.4039s\n",
            "#772 Loss: 4.9553 Acc: 57.2854% Time: 5855.2643s\n",
            "#773 Loss: 4.7432 Acc: 56.5868% Time: 5863.0035s\n",
            "#774 Loss: 4.4408 Acc: 57.0359% Time: 5870.7932s\n",
            "#775 Loss: 4.7273 Acc: 56.3872% Time: 5880.9112s\n",
            "#776 Loss: 5.4291 Acc: 56.1876% Time: 5890.8408s\n",
            "#777 Loss: 4.7630 Acc: 55.8882% Time: 5900.7081s\n",
            "#778 Loss: 3.8360 Acc: 58.6826% Time: 5910.6252s\n",
            "#779 Loss: 5.1438 Acc: 57.5848% Time: 5920.7248s\n",
            "#780 Loss: 4.3322 Acc: 59.9301% Time: 5930.5058s\n",
            "#781 Loss: 4.5572 Acc: 58.1337% Time: 5938.9860s\n",
            "#782 Loss: 4.7800 Acc: 55.6886% Time: 5946.8514s\n",
            "#783 Loss: 5.0161 Acc: 56.2375% Time: 5954.5120s\n",
            "#784 Loss: 4.8145 Acc: 57.5848% Time: 5962.2645s\n",
            "#785 Loss: 5.0978 Acc: 57.1856% Time: 5972.1410s\n",
            "#786 Loss: 5.5736 Acc: 56.3373% Time: 5980.0710s\n",
            "#787 Loss: 5.1155 Acc: 55.4890% Time: 5987.6598s\n",
            "#788 Loss: 4.3653 Acc: 58.4830% Time: 5995.3932s\n",
            "#789 Loss: 4.4986 Acc: 56.9860% Time: 6005.4925s\n",
            "#790 Loss: 4.9302 Acc: 56.0379% Time: 6013.1652s\n",
            "#791 Loss: 5.0420 Acc: 55.8882% Time: 6021.0396s\n",
            "#792 Loss: 4.7126 Acc: 56.8862% Time: 6028.7955s\n",
            "#793 Loss: 4.4986 Acc: 57.7345% Time: 6036.5617s\n",
            "#794 Loss: 5.3546 Acc: 55.3393% Time: 6044.2452s\n",
            "#795 Loss: 5.4530 Acc: 58.0339% Time: 6051.8560s\n",
            "#796 Loss: 4.3008 Acc: 55.4890% Time: 6059.4929s\n",
            "#797 Loss: 4.4718 Acc: 58.0339% Time: 6067.1722s\n",
            "#798 Loss: 4.6235 Acc: 57.7844% Time: 6075.0107s\n",
            "#799 Loss: 5.1393 Acc: 56.3872% Time: 6082.6548s\n",
            "#800 Loss: 4.6318 Acc: 57.1856% Time: 6090.3221s\n",
            "#801 Loss: 4.6648 Acc: 57.3353% Time: 6098.1615s\n",
            "#802 Loss: 4.8795 Acc: 57.0359% Time: 6108.1492s\n",
            "#803 Loss: 5.0308 Acc: 57.2355% Time: 6117.9166s\n",
            "#804 Loss: 5.4572 Acc: 56.0878% Time: 6127.6203s\n",
            "#805 Loss: 4.6052 Acc: 58.2834% Time: 6137.4378s\n",
            "#806 Loss: 4.7942 Acc: 56.9361% Time: 6147.4065s\n",
            "#807 Loss: 5.1417 Acc: 57.0858% Time: 6157.6764s\n",
            "#808 Loss: 5.2031 Acc: 55.0898% Time: 6167.5205s\n",
            "#809 Loss: 4.5760 Acc: 56.9361% Time: 6177.3188s\n",
            "#810 Loss: 5.7203 Acc: 57.3852% Time: 6185.7185s\n",
            "#811 Loss: 4.8013 Acc: 56.0379% Time: 6193.4709s\n",
            "#812 Loss: 4.8052 Acc: 58.6826% Time: 6201.0669s\n",
            "#813 Loss: 4.8879 Acc: 55.9880% Time: 6208.6099s\n",
            "#814 Loss: 4.6590 Acc: 58.2335% Time: 6216.3822s\n",
            "#815 Loss: 4.3754 Acc: 57.3852% Time: 6226.5765s\n",
            "#816 Loss: 4.1827 Acc: 58.0339% Time: 6236.7690s\n",
            "#817 Loss: 4.6708 Acc: 55.1397% Time: 6246.6592s\n",
            "#818 Loss: 4.5101 Acc: 56.7864% Time: 6254.2567s\n",
            "#819 Loss: 4.2700 Acc: 55.6387% Time: 6261.9348s\n",
            "#820 Loss: 4.5337 Acc: 56.7365% Time: 6269.4203s\n",
            "#821 Loss: 4.5293 Acc: 55.4391% Time: 6276.8946s\n",
            "#822 Loss: 4.4487 Acc: 58.1337% Time: 6284.3331s\n",
            "#823 Loss: 4.2815 Acc: 55.9880% Time: 6291.7979s\n",
            "#824 Loss: 4.8981 Acc: 56.6866% Time: 6299.2829s\n",
            "#825 Loss: 4.0678 Acc: 57.7345% Time: 6306.7849s\n",
            "#826 Loss: 5.0477 Acc: 56.8363% Time: 6314.3182s\n",
            "#827 Loss: 5.2428 Acc: 56.0878% Time: 6321.7355s\n",
            "#828 Loss: 4.6148 Acc: 55.9880% Time: 6329.3381s\n",
            "#829 Loss: 4.3960 Acc: 59.8303% Time: 6336.9022s\n",
            "#830 Loss: 4.6416 Acc: 56.9361% Time: 6344.5740s\n",
            "#831 Loss: 4.9757 Acc: 56.8862% Time: 6352.1280s\n",
            "#832 Loss: 4.5321 Acc: 56.1876% Time: 6359.7237s\n",
            "#833 Loss: 4.8996 Acc: 55.5888% Time: 6367.2870s\n",
            "#834 Loss: 4.8682 Acc: 57.1856% Time: 6374.9568s\n",
            "#835 Loss: 5.9191 Acc: 58.0838% Time: 6382.3301s\n",
            "#836 Loss: 4.6893 Acc: 57.9840% Time: 6389.9647s\n",
            "#837 Loss: 5.0864 Acc: 55.8383% Time: 6397.4200s\n",
            "#838 Loss: 5.3984 Acc: 56.2375% Time: 6404.9784s\n",
            "#839 Loss: 5.2208 Acc: 57.2854% Time: 6412.4788s\n",
            "#840 Loss: 5.5538 Acc: 56.2375% Time: 6419.9476s\n",
            "#841 Loss: 4.7824 Acc: 54.9401% Time: 6427.5020s\n",
            "#842 Loss: 4.1923 Acc: 58.2834% Time: 6435.0802s\n",
            "#843 Loss: 4.9984 Acc: 57.3353% Time: 6442.5402s\n",
            "#844 Loss: 5.3757 Acc: 59.1816% Time: 6450.1464s\n",
            "#845 Loss: 4.7194 Acc: 56.8862% Time: 6457.6814s\n",
            "#846 Loss: 5.0315 Acc: 57.2854% Time: 6465.1135s\n",
            "#847 Loss: 4.6188 Acc: 56.6866% Time: 6472.6617s\n",
            "#848 Loss: 4.9462 Acc: 57.2355% Time: 6480.2568s\n",
            "#849 Loss: 4.9346 Acc: 57.6347% Time: 6487.6975s\n",
            "#850 Loss: 5.4375 Acc: 57.9840% Time: 6495.3452s\n",
            "#851 Loss: 4.6956 Acc: 56.7864% Time: 6502.9117s\n",
            "#852 Loss: 4.1146 Acc: 58.1337% Time: 6510.2835s\n",
            "#853 Loss: 5.1505 Acc: 55.4391% Time: 6517.7638s\n",
            "#854 Loss: 4.9961 Acc: 59.8303% Time: 6525.3133s\n",
            "#855 Loss: 4.1888 Acc: 56.4371% Time: 6532.9717s\n",
            "#856 Loss: 5.0085 Acc: 57.4351% Time: 6540.4981s\n",
            "#857 Loss: 4.8919 Acc: 55.5389% Time: 6548.0699s\n",
            "#858 Loss: 3.9777 Acc: 59.2814% Time: 6555.5121s\n",
            "#859 Loss: 5.2300 Acc: 55.6387% Time: 6563.0492s\n",
            "#860 Loss: 5.6232 Acc: 56.5369% Time: 6570.6846s\n",
            "#861 Loss: 6.3661 Acc: 55.4391% Time: 6578.2817s\n",
            "#862 Loss: 5.3853 Acc: 57.4351% Time: 6585.8477s\n",
            "#863 Loss: 4.8305 Acc: 55.1397% Time: 6593.4868s\n",
            "#864 Loss: 5.8834 Acc: 56.9361% Time: 6601.0339s\n",
            "#865 Loss: 5.2414 Acc: 56.7864% Time: 6608.5975s\n",
            "#866 Loss: 4.8712 Acc: 57.0359% Time: 6616.2425s\n",
            "#867 Loss: 4.8712 Acc: 56.8862% Time: 6623.8484s\n",
            "#868 Loss: 5.1119 Acc: 57.7345% Time: 6631.2824s\n",
            "#869 Loss: 5.0954 Acc: 56.7365% Time: 6638.8397s\n",
            "#870 Loss: 4.7175 Acc: 57.7844% Time: 6646.2398s\n",
            "#871 Loss: 4.6183 Acc: 56.5868% Time: 6653.8145s\n",
            "#872 Loss: 5.2970 Acc: 58.0339% Time: 6661.2495s\n",
            "#873 Loss: 5.3090 Acc: 57.6846% Time: 6668.8583s\n",
            "#874 Loss: 4.4592 Acc: 56.8862% Time: 6676.4385s\n",
            "#875 Loss: 4.5075 Acc: 56.2375% Time: 6684.0436s\n",
            "#876 Loss: 5.3240 Acc: 54.6407% Time: 6691.6207s\n",
            "#877 Loss: 4.8790 Acc: 59.2315% Time: 6699.1876s\n",
            "#878 Loss: 5.0937 Acc: 54.8902% Time: 6706.6810s\n",
            "#879 Loss: 5.2138 Acc: 56.7365% Time: 6714.1741s\n",
            "#880 Loss: 5.3624 Acc: 56.6866% Time: 6721.6676s\n",
            "#881 Loss: 5.1975 Acc: 58.5329% Time: 6729.2227s\n",
            "#882 Loss: 4.7418 Acc: 58.2335% Time: 6736.8405s\n",
            "#883 Loss: 5.2973 Acc: 58.1337% Time: 6744.4063s\n",
            "#884 Loss: 4.1609 Acc: 57.6347% Time: 6751.8565s\n",
            "#885 Loss: 4.3076 Acc: 57.6846% Time: 6759.3855s\n",
            "#886 Loss: 6.0498 Acc: 55.2395% Time: 6766.8465s\n",
            "#887 Loss: 5.0422 Acc: 57.7345% Time: 6774.3862s\n",
            "#888 Loss: 4.6736 Acc: 57.9840% Time: 6781.9636s\n",
            "#889 Loss: 4.7032 Acc: 55.9880% Time: 6789.5515s\n",
            "#890 Loss: 4.9199 Acc: 56.6866% Time: 6797.1862s\n",
            "#891 Loss: 4.7089 Acc: 55.9381% Time: 6804.7019s\n",
            "#892 Loss: 4.1433 Acc: 56.6866% Time: 6812.3539s\n",
            "#893 Loss: 5.3210 Acc: 57.7844% Time: 6820.0002s\n",
            "#894 Loss: 4.8812 Acc: 56.0878% Time: 6827.5437s\n",
            "#895 Loss: 6.3879 Acc: 54.9401% Time: 6835.0354s\n",
            "#896 Loss: 5.8308 Acc: 57.2355% Time: 6842.5838s\n",
            "#897 Loss: 4.5843 Acc: 57.9341% Time: 6850.1765s\n",
            "#898 Loss: 4.8236 Acc: 57.0359% Time: 6857.6159s\n",
            "#899 Loss: 4.1992 Acc: 57.5848% Time: 6865.1947s\n",
            "#900 Loss: 4.2554 Acc: 58.1836% Time: 6872.5785s\n",
            "#901 Loss: 4.2953 Acc: 58.7325% Time: 6880.2190s\n",
            "#902 Loss: 4.1129 Acc: 57.8842% Time: 6887.7534s\n",
            "#903 Loss: 4.0485 Acc: 57.9840% Time: 6895.3407s\n",
            "#904 Loss: 4.4255 Acc: 56.3872% Time: 6902.7497s\n",
            "#905 Loss: 5.4211 Acc: 57.7844% Time: 6910.1630s\n",
            "#906 Loss: 5.6405 Acc: 57.0359% Time: 6917.7233s\n",
            "#907 Loss: 5.1412 Acc: 56.9361% Time: 6925.2042s\n",
            "#908 Loss: 4.8610 Acc: 57.2854% Time: 6932.7309s\n",
            "#909 Loss: 5.1122 Acc: 56.4870% Time: 6940.2300s\n",
            "#910 Loss: 5.0572 Acc: 57.4351% Time: 6947.7565s\n",
            "#911 Loss: 4.4597 Acc: 58.0838% Time: 6955.3154s\n",
            "#912 Loss: 4.9193 Acc: 57.8343% Time: 6962.9501s\n",
            "#913 Loss: 5.5376 Acc: 55.1896% Time: 6970.5378s\n",
            "#914 Loss: 4.6916 Acc: 55.5888% Time: 6978.0933s\n",
            "#915 Loss: 5.2960 Acc: 56.5868% Time: 6985.5445s\n",
            "#916 Loss: 4.7287 Acc: 57.4850% Time: 6993.0839s\n",
            "#917 Loss: 4.7338 Acc: 58.1836% Time: 7000.6877s\n",
            "#918 Loss: 4.5342 Acc: 55.9381% Time: 7008.1581s\n",
            "#919 Loss: 5.4240 Acc: 55.6387% Time: 7015.7564s\n",
            "#920 Loss: 4.9600 Acc: 54.9401% Time: 7023.3859s\n",
            "#921 Loss: 5.5177 Acc: 55.2894% Time: 7030.9238s\n",
            "#922 Loss: 4.2329 Acc: 60.5289% Time: 7038.5645s\n",
            "#923 Loss: 5.5593 Acc: 55.6886% Time: 7046.1072s\n",
            "#924 Loss: 4.6748 Acc: 57.3852% Time: 7053.6685s\n",
            "#925 Loss: 4.3662 Acc: 56.5369% Time: 7061.2716s\n",
            "#926 Loss: 5.5250 Acc: 57.1357% Time: 7068.7003s\n",
            "#927 Loss: 5.3234 Acc: 58.8822% Time: 7076.1184s\n",
            "#928 Loss: 4.1070 Acc: 58.7824% Time: 7083.7187s\n",
            "#929 Loss: 4.2614 Acc: 57.7345% Time: 7091.3139s\n",
            "#930 Loss: 4.7750 Acc: 56.3373% Time: 7098.8865s\n",
            "#931 Loss: 5.1653 Acc: 57.1357% Time: 7106.3977s\n",
            "#932 Loss: 4.7293 Acc: 56.3373% Time: 7113.9476s\n",
            "#933 Loss: 4.5767 Acc: 57.6846% Time: 7121.4261s\n",
            "#934 Loss: 5.7186 Acc: 56.6866% Time: 7129.0476s\n",
            "#935 Loss: 4.2221 Acc: 57.5848% Time: 7136.4853s\n",
            "#936 Loss: 4.7678 Acc: 56.8363% Time: 7144.1370s\n",
            "#937 Loss: 4.4034 Acc: 55.8882% Time: 7151.7204s\n",
            "#938 Loss: 4.4065 Acc: 57.2854% Time: 7159.2934s\n",
            "#939 Loss: 4.5349 Acc: 58.5828% Time: 7166.9115s\n",
            "#940 Loss: 5.1064 Acc: 56.3373% Time: 7174.4365s\n",
            "#941 Loss: 4.5277 Acc: 57.8842% Time: 7181.9013s\n",
            "#942 Loss: 4.5937 Acc: 57.1357% Time: 7189.4757s\n",
            "#943 Loss: 4.7905 Acc: 56.2874% Time: 7197.0585s\n",
            "#944 Loss: 5.2309 Acc: 55.4890% Time: 7204.4747s\n",
            "#945 Loss: 5.7297 Acc: 57.4351% Time: 7211.9944s\n",
            "#946 Loss: 5.2079 Acc: 56.7365% Time: 7219.4507s\n",
            "#947 Loss: 5.2255 Acc: 57.5848% Time: 7227.0472s\n",
            "#948 Loss: 5.3915 Acc: 56.3872% Time: 7234.6382s\n",
            "#949 Loss: 5.4530 Acc: 55.6886% Time: 7242.2256s\n",
            "#950 Loss: 5.2337 Acc: 55.4391% Time: 7249.7636s\n",
            "#951 Loss: 4.4789 Acc: 57.2355% Time: 7257.2504s\n",
            "#952 Loss: 5.3364 Acc: 56.1377% Time: 7264.7550s\n",
            "#953 Loss: 4.7182 Acc: 56.9860% Time: 7272.4246s\n",
            "#954 Loss: 5.0748 Acc: 55.7884% Time: 7280.0844s\n",
            "#955 Loss: 4.4264 Acc: 56.7864% Time: 7287.6153s\n",
            "#956 Loss: 4.0303 Acc: 56.8862% Time: 7295.2274s\n",
            "#957 Loss: 4.6666 Acc: 55.4890% Time: 7302.8033s\n",
            "#958 Loss: 3.7170 Acc: 57.6347% Time: 7310.2726s\n",
            "#959 Loss: 4.5866 Acc: 57.7345% Time: 7317.7872s\n",
            "#960 Loss: 4.4303 Acc: 56.4371% Time: 7325.4310s\n",
            "#961 Loss: 5.8378 Acc: 54.4910% Time: 7332.8392s\n",
            "#962 Loss: 4.6919 Acc: 57.2355% Time: 7340.4167s\n",
            "#963 Loss: 5.7535 Acc: 55.6886% Time: 7347.8126s\n",
            "#964 Loss: 4.9148 Acc: 57.4351% Time: 7355.3786s\n",
            "#965 Loss: 5.0746 Acc: 57.0858% Time: 7363.0147s\n",
            "#966 Loss: 4.2481 Acc: 58.9820% Time: 7370.5972s\n",
            "#967 Loss: 4.6513 Acc: 56.6866% Time: 7377.9900s\n",
            "#968 Loss: 4.2797 Acc: 57.6347% Time: 7385.5707s\n",
            "#969 Loss: 5.9452 Acc: 56.1377% Time: 7392.9965s\n",
            "#970 Loss: 4.9517 Acc: 56.8862% Time: 7400.3836s\n",
            "#971 Loss: 4.4356 Acc: 58.4830% Time: 7407.8133s\n",
            "#972 Loss: 4.1646 Acc: 56.7365% Time: 7415.5071s\n",
            "#973 Loss: 5.2204 Acc: 54.6407% Time: 7423.2927s\n",
            "#974 Loss: 4.5635 Acc: 56.2874% Time: 7431.0888s\n",
            "#975 Loss: 4.3706 Acc: 58.0339% Time: 7438.8620s\n",
            "#976 Loss: 4.4267 Acc: 56.6367% Time: 7446.6667s\n",
            "#977 Loss: 4.0403 Acc: 57.6347% Time: 7454.2053s\n",
            "#978 Loss: 5.2666 Acc: 55.6886% Time: 7461.7451s\n",
            "#979 Loss: 4.9416 Acc: 58.0339% Time: 7469.1639s\n",
            "#980 Loss: 5.0284 Acc: 58.8323% Time: 7476.7558s\n",
            "#981 Loss: 5.3545 Acc: 55.5389% Time: 7484.3586s\n",
            "#982 Loss: 4.0765 Acc: 57.1856% Time: 7491.7264s\n",
            "#983 Loss: 4.0790 Acc: 56.5868% Time: 7499.2029s\n",
            "#984 Loss: 4.3022 Acc: 55.5888% Time: 7506.8651s\n",
            "#985 Loss: 3.8509 Acc: 58.1836% Time: 7514.6179s\n",
            "#986 Loss: 4.1941 Acc: 55.9880% Time: 7522.4467s\n",
            "#987 Loss: 5.1287 Acc: 57.0858% Time: 7532.2113s\n",
            "#988 Loss: 4.7190 Acc: 56.7864% Time: 7541.7056s\n",
            "#989 Loss: 4.8283 Acc: 56.2874% Time: 7551.0786s\n",
            "#990 Loss: 4.5365 Acc: 56.9361% Time: 7560.7006s\n",
            "#991 Loss: 4.5856 Acc: 57.2854% Time: 7570.2811s\n",
            "#992 Loss: 4.1541 Acc: 59.4810% Time: 7579.6626s\n",
            "#993 Loss: 5.9981 Acc: 55.6387% Time: 7587.1310s\n",
            "#994 Loss: 4.7917 Acc: 55.5888% Time: 7594.6910s\n",
            "#995 Loss: 5.8413 Acc: 57.1856% Time: 7602.1398s\n",
            "#996 Loss: 4.6330 Acc: 57.8842% Time: 7609.7143s\n",
            "#997 Loss: 7.2556 Acc: 54.9900% Time: 7617.3690s\n",
            "#998 Loss: 4.5813 Acc: 57.7844% Time: 7626.5105s\n",
            "#999 Loss: 4.7870 Acc: 57.5848% Time: 7636.2361s\n"
          ]
        }
      ],
      "source": [
        "# num_epochs = 200\n",
        "# model.train()\n",
        "# start_time = time.time()\n",
        "# writer = SummaryWriter()\n",
        "\n",
        "# # 전체 반복(epoch) 수 만큼 반복하며\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.\n",
        "#     running_corrects = 0\n",
        "\n",
        "#     # 배치 단위로 학습 데이터 불러오기\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         # 모델에 입력(forward)하고 결과 계산\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss = running_loss / len(train_datasets)\n",
        "#     epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "#     writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"acc/train\", epoch_acc, epoch)\n",
        "#     writer.add_scalar(\"sum/train\", epoch_loss, epoch)\n",
        "#     # writer.add_scalar(\"sum/train\", epoch_acc/100, epoch)\n",
        "\n",
        "#     # 학습 과정 중에 결과 출력\n",
        "#     print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "# writer.flush()\n",
        "# writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Loss: 0.7131 Acc: 52.3952% Time: 11.1457s\n",
            "#1 Loss: 0.6968 Acc: 53.2934% Time: 17.9502s\n",
            "#2 Loss: 0.6983 Acc: 54.4411% Time: 24.9575s\n",
            "#3 Loss: 0.6846 Acc: 55.6886% Time: 31.8728s\n",
            "#4 Loss: 0.6783 Acc: 55.6387% Time: 38.7551s\n",
            "#5 Loss: 0.6689 Acc: 58.6826% Time: 45.6190s\n",
            "#6 Loss: 0.6736 Acc: 58.0339% Time: 52.4781s\n",
            "#7 Loss: 0.6630 Acc: 59.8802% Time: 59.3201s\n",
            "#8 Loss: 0.6585 Acc: 59.8802% Time: 66.1005s\n",
            "#9 Loss: 0.6569 Acc: 59.8802% Time: 72.9482s\n",
            "#10 Loss: 0.6542 Acc: 62.2256% Time: 79.8679s\n",
            "#11 Loss: 0.6508 Acc: 61.6267% Time: 86.9646s\n",
            "#12 Loss: 0.6462 Acc: 62.3253% Time: 93.8428s\n",
            "#13 Loss: 0.6449 Acc: 62.0758% Time: 100.7400s\n",
            "#14 Loss: 0.6454 Acc: 63.0240% Time: 107.7553s\n",
            "#15 Loss: 0.6388 Acc: 63.7226% Time: 114.5226s\n",
            "#16 Loss: 0.6490 Acc: 61.0778% Time: 121.5016s\n",
            "#17 Loss: 0.6354 Acc: 65.0699% Time: 128.4411s\n",
            "#18 Loss: 0.6390 Acc: 64.3713% Time: 135.6261s\n",
            "#19 Loss: 0.6351 Acc: 63.7725% Time: 142.5982s\n",
            "#20 Loss: 0.6326 Acc: 64.1717% Time: 149.5470s\n",
            "#21 Loss: 0.6347 Acc: 64.2715% Time: 156.4984s\n",
            "#22 Loss: 0.6360 Acc: 63.3234% Time: 163.7148s\n",
            "#23 Loss: 0.6322 Acc: 63.8224% Time: 170.5570s\n",
            "#24 Loss: 0.6272 Acc: 65.0200% Time: 177.5601s\n",
            "#25 Loss: 0.6308 Acc: 63.6727% Time: 184.7816s\n",
            "#26 Loss: 0.6246 Acc: 64.4711% Time: 191.8722s\n",
            "#27 Loss: 0.6223 Acc: 64.5709% Time: 198.7923s\n",
            "#28 Loss: 0.6202 Acc: 65.7186% Time: 205.7850s\n",
            "#29 Loss: 0.6290 Acc: 64.6208% Time: 213.0024s\n",
            "#30 Loss: 0.6211 Acc: 65.6188% Time: 219.9682s\n",
            "#31 Loss: 0.6134 Acc: 66.8164% Time: 227.2274s\n",
            "#32 Loss: 0.6143 Acc: 66.0679% Time: 234.1132s\n",
            "#33 Loss: 0.6203 Acc: 65.2695% Time: 241.0500s\n",
            "#34 Loss: 0.6123 Acc: 66.7665% Time: 247.8599s\n",
            "#35 Loss: 0.6216 Acc: 65.0699% Time: 254.7714s\n",
            "#36 Loss: 0.6174 Acc: 65.5689% Time: 261.7134s\n",
            "#37 Loss: 0.6148 Acc: 66.1677% Time: 268.7013s\n",
            "#38 Loss: 0.6098 Acc: 67.0659% Time: 275.6373s\n",
            "#39 Loss: 0.6165 Acc: 65.9681% Time: 282.6257s\n",
            "#40 Loss: 0.6115 Acc: 65.7186% Time: 289.5248s\n",
            "#41 Loss: 0.6101 Acc: 64.9202% Time: 296.5541s\n",
            "#42 Loss: 0.6013 Acc: 68.4132% Time: 303.5928s\n",
            "#43 Loss: 0.6096 Acc: 65.2695% Time: 310.7225s\n",
            "#44 Loss: 0.6074 Acc: 65.8184% Time: 317.9594s\n",
            "#45 Loss: 0.6021 Acc: 68.0639% Time: 324.8078s\n",
            "#46 Loss: 0.6019 Acc: 66.7166% Time: 331.7318s\n",
            "#47 Loss: 0.6114 Acc: 65.6687% Time: 338.6751s\n",
            "#48 Loss: 0.6081 Acc: 66.5669% Time: 345.6403s\n",
            "#49 Loss: 0.6042 Acc: 66.5170% Time: 352.6602s\n",
            "#50 Loss: 0.5994 Acc: 68.0639% Time: 359.8000s\n",
            "#51 Loss: 0.5948 Acc: 68.3633% Time: 366.7168s\n",
            "#52 Loss: 0.5988 Acc: 67.7645% Time: 373.8353s\n",
            "#53 Loss: 0.6067 Acc: 66.3174% Time: 380.9026s\n",
            "#54 Loss: 0.5982 Acc: 67.5150% Time: 387.8993s\n",
            "#55 Loss: 0.6017 Acc: 66.8663% Time: 395.2250s\n",
            "#56 Loss: 0.6009 Acc: 66.7166% Time: 402.3847s\n",
            "#57 Loss: 0.6033 Acc: 67.3653% Time: 409.5840s\n",
            "#58 Loss: 0.6001 Acc: 67.4152% Time: 416.8220s\n",
            "#59 Loss: 0.6026 Acc: 67.0659% Time: 423.7566s\n",
            "#60 Loss: 0.6066 Acc: 66.5170% Time: 430.5351s\n",
            "#61 Loss: 0.5979 Acc: 67.4651% Time: 437.6119s\n",
            "#62 Loss: 0.5991 Acc: 67.7146% Time: 444.6402s\n",
            "#63 Loss: 0.6029 Acc: 66.2176% Time: 451.9789s\n",
            "#64 Loss: 0.5996 Acc: 66.9661% Time: 459.0313s\n",
            "#65 Loss: 0.5967 Acc: 67.6647% Time: 466.1009s\n",
            "#66 Loss: 0.5929 Acc: 68.4631% Time: 473.1225s\n",
            "#67 Loss: 0.5955 Acc: 67.9641% Time: 480.0534s\n",
            "#68 Loss: 0.5970 Acc: 67.0160% Time: 487.1575s\n",
            "#69 Loss: 0.5930 Acc: 68.5130% Time: 494.2534s\n",
            "#70 Loss: 0.5998 Acc: 67.4651% Time: 501.2680s\n",
            "#71 Loss: 0.6003 Acc: 67.4152% Time: 508.2365s\n",
            "#72 Loss: 0.5962 Acc: 68.1138% Time: 515.2569s\n",
            "#73 Loss: 0.5971 Acc: 67.8643% Time: 522.2488s\n",
            "#74 Loss: 0.5951 Acc: 68.4132% Time: 529.1612s\n",
            "#75 Loss: 0.5950 Acc: 68.4631% Time: 536.2016s\n",
            "#76 Loss: 0.5919 Acc: 68.8124% Time: 543.1694s\n",
            "#77 Loss: 0.5929 Acc: 67.2655% Time: 550.1325s\n",
            "#78 Loss: 0.5984 Acc: 67.5150% Time: 556.9904s\n",
            "#79 Loss: 0.5896 Acc: 68.4631% Time: 564.0033s\n",
            "#80 Loss: 0.5855 Acc: 69.1118% Time: 570.6824s\n",
            "#81 Loss: 0.5825 Acc: 69.8603% Time: 577.5299s\n",
            "#82 Loss: 0.5932 Acc: 68.4132% Time: 584.5064s\n",
            "#83 Loss: 0.5955 Acc: 68.3134% Time: 591.3417s\n",
            "#84 Loss: 0.5833 Acc: 68.2635% Time: 598.1941s\n",
            "#85 Loss: 0.5853 Acc: 69.4112% Time: 605.1256s\n",
            "#86 Loss: 0.5901 Acc: 68.1637% Time: 612.1112s\n",
            "#87 Loss: 0.5901 Acc: 69.4611% Time: 619.3176s\n",
            "#88 Loss: 0.5965 Acc: 67.3154% Time: 626.4943s\n",
            "#89 Loss: 0.5820 Acc: 69.6607% Time: 633.6259s\n",
            "#90 Loss: 0.5920 Acc: 68.4132% Time: 640.7189s\n",
            "#91 Loss: 0.5808 Acc: 69.5110% Time: 647.8494s\n",
            "#92 Loss: 0.5910 Acc: 67.4651% Time: 654.9273s\n",
            "#93 Loss: 0.5894 Acc: 68.3134% Time: 662.0203s\n",
            "#94 Loss: 0.5823 Acc: 68.8124% Time: 668.8612s\n",
            "#95 Loss: 0.5842 Acc: 68.9621% Time: 675.7019s\n",
            "#96 Loss: 0.5877 Acc: 68.6128% Time: 682.5425s\n",
            "#97 Loss: 0.5939 Acc: 69.0619% Time: 689.3362s\n",
            "#98 Loss: 0.5783 Acc: 69.8104% Time: 696.1527s\n",
            "#99 Loss: 0.5857 Acc: 68.6128% Time: 702.9196s\n",
            "#100 Loss: 0.5872 Acc: 68.7126% Time: 709.7987s\n",
            "#101 Loss: 0.5828 Acc: 69.2116% Time: 716.7093s\n",
            "#102 Loss: 0.5797 Acc: 69.6607% Time: 723.4487s\n",
            "#103 Loss: 0.5911 Acc: 67.9641% Time: 730.3087s\n",
            "#104 Loss: 0.5757 Acc: 69.5609% Time: 737.2398s\n",
            "#105 Loss: 0.5849 Acc: 69.4611% Time: 744.0691s\n",
            "#106 Loss: 0.5907 Acc: 69.0120% Time: 750.9121s\n",
            "#107 Loss: 0.5815 Acc: 68.8623% Time: 757.6950s\n",
            "#108 Loss: 0.5864 Acc: 68.7625% Time: 764.4592s\n",
            "#109 Loss: 0.5781 Acc: 68.8623% Time: 771.3123s\n",
            "#110 Loss: 0.5826 Acc: 69.7106% Time: 778.2294s\n",
            "#111 Loss: 0.5806 Acc: 69.6108% Time: 785.3016s\n",
            "#112 Loss: 0.5826 Acc: 68.6627% Time: 792.2313s\n",
            "#113 Loss: 0.5844 Acc: 68.2635% Time: 799.0472s\n",
            "#114 Loss: 0.5826 Acc: 69.3114% Time: 805.7771s\n",
            "#115 Loss: 0.5751 Acc: 69.2116% Time: 812.4720s\n",
            "#116 Loss: 0.5882 Acc: 68.4631% Time: 819.5410s\n",
            "#117 Loss: 0.5859 Acc: 67.9641% Time: 826.6096s\n",
            "#118 Loss: 0.5821 Acc: 68.8623% Time: 833.6898s\n",
            "#119 Loss: 0.5783 Acc: 68.8124% Time: 840.6982s\n",
            "#120 Loss: 0.5759 Acc: 69.7106% Time: 847.8430s\n",
            "#121 Loss: 0.5762 Acc: 69.8104% Time: 854.6290s\n",
            "#122 Loss: 0.5796 Acc: 69.5609% Time: 861.4333s\n",
            "#123 Loss: 0.5708 Acc: 70.7585% Time: 868.1710s\n",
            "#124 Loss: 0.5800 Acc: 70.2096% Time: 875.0730s\n",
            "#125 Loss: 0.5735 Acc: 70.3094% Time: 881.7527s\n",
            "#126 Loss: 0.5722 Acc: 69.9102% Time: 888.6394s\n",
            "#127 Loss: 0.5816 Acc: 68.5130% Time: 895.5521s\n",
            "#128 Loss: 0.5855 Acc: 67.8643% Time: 902.5440s\n",
            "#129 Loss: 0.5766 Acc: 69.4611% Time: 909.4127s\n",
            "#130 Loss: 0.5787 Acc: 69.2116% Time: 916.2967s\n",
            "#131 Loss: 0.5778 Acc: 69.7605% Time: 923.2580s\n",
            "#132 Loss: 0.5746 Acc: 69.5609% Time: 930.0481s\n",
            "#133 Loss: 0.5751 Acc: 69.1617% Time: 936.7816s\n",
            "#134 Loss: 0.5789 Acc: 68.8623% Time: 943.6160s\n",
            "#135 Loss: 0.5843 Acc: 68.9122% Time: 950.4745s\n",
            "#136 Loss: 0.5696 Acc: 70.1597% Time: 957.4562s\n",
            "#137 Loss: 0.5744 Acc: 68.9621% Time: 964.1631s\n",
            "#138 Loss: 0.5795 Acc: 70.4591% Time: 971.1029s\n",
            "#139 Loss: 0.5662 Acc: 70.1597% Time: 977.9575s\n",
            "#140 Loss: 0.5760 Acc: 68.7126% Time: 984.6902s\n",
            "#141 Loss: 0.5733 Acc: 70.4092% Time: 991.6538s\n",
            "#142 Loss: 0.5697 Acc: 70.3593% Time: 998.6812s\n",
            "#143 Loss: 0.5709 Acc: 70.0599% Time: 1005.3249s\n",
            "#144 Loss: 0.5776 Acc: 69.7605% Time: 1012.2200s\n",
            "#145 Loss: 0.5759 Acc: 69.3114% Time: 1018.9435s\n",
            "#146 Loss: 0.5774 Acc: 69.7605% Time: 1025.7157s\n",
            "#147 Loss: 0.5683 Acc: 70.8084% Time: 1032.5411s\n",
            "#148 Loss: 0.5831 Acc: 67.9641% Time: 1039.2697s\n",
            "#149 Loss: 0.5706 Acc: 69.3613% Time: 1045.9568s\n",
            "#150 Loss: 0.5704 Acc: 72.0060% Time: 1052.8358s\n",
            "#151 Loss: 0.5698 Acc: 70.3094% Time: 1059.7567s\n",
            "#152 Loss: 0.5719 Acc: 69.9102% Time: 1066.7882s\n",
            "#153 Loss: 0.5697 Acc: 69.8603% Time: 1073.7346s\n",
            "#154 Loss: 0.5678 Acc: 70.4092% Time: 1080.7439s\n",
            "#155 Loss: 0.5734 Acc: 70.9581% Time: 1087.6880s\n",
            "#156 Loss: 0.5708 Acc: 69.5609% Time: 1094.9224s\n",
            "#157 Loss: 0.5677 Acc: 70.5589% Time: 1101.9030s\n",
            "#158 Loss: 0.5806 Acc: 69.3613% Time: 1109.0368s\n",
            "#159 Loss: 0.5795 Acc: 69.3613% Time: 1116.3441s\n",
            "#160 Loss: 0.5696 Acc: 69.2116% Time: 1123.5654s\n",
            "#161 Loss: 0.5655 Acc: 69.8104% Time: 1130.6666s\n",
            "#162 Loss: 0.5738 Acc: 69.2116% Time: 1137.5735s\n",
            "#163 Loss: 0.5813 Acc: 68.9621% Time: 1144.5165s\n",
            "#164 Loss: 0.5754 Acc: 69.6108% Time: 1151.3125s\n",
            "#165 Loss: 0.5725 Acc: 70.6587% Time: 1158.1680s\n",
            "#166 Loss: 0.5722 Acc: 70.0599% Time: 1165.2500s\n",
            "#167 Loss: 0.5641 Acc: 70.7086% Time: 1172.2360s\n",
            "#168 Loss: 0.5615 Acc: 69.7106% Time: 1179.2575s\n",
            "#169 Loss: 0.5753 Acc: 70.5589% Time: 1186.1873s\n",
            "#170 Loss: 0.5735 Acc: 69.7605% Time: 1193.1290s\n",
            "#171 Loss: 0.5573 Acc: 70.9082% Time: 1199.9706s\n",
            "#172 Loss: 0.5728 Acc: 70.0599% Time: 1206.8225s\n",
            "#173 Loss: 0.5625 Acc: 70.5090% Time: 1213.7796s\n",
            "#174 Loss: 0.5770 Acc: 70.1597% Time: 1220.7247s\n",
            "#175 Loss: 0.5774 Acc: 70.1098% Time: 1227.4433s\n",
            "#176 Loss: 0.5725 Acc: 70.5589% Time: 1234.2919s\n",
            "#177 Loss: 0.5625 Acc: 71.2076% Time: 1241.2835s\n",
            "#178 Loss: 0.5679 Acc: 70.2096% Time: 1248.0175s\n",
            "#179 Loss: 0.5639 Acc: 71.3074% Time: 1254.8610s\n",
            "#180 Loss: 0.5588 Acc: 70.6587% Time: 1261.6482s\n",
            "#181 Loss: 0.5747 Acc: 69.5609% Time: 1268.5822s\n",
            "#182 Loss: 0.5647 Acc: 70.6587% Time: 1275.6057s\n",
            "#183 Loss: 0.5701 Acc: 70.6088% Time: 1282.5201s\n",
            "#184 Loss: 0.5691 Acc: 69.8104% Time: 1289.5376s\n",
            "#185 Loss: 0.5717 Acc: 70.3094% Time: 1296.2495s\n",
            "#186 Loss: 0.5707 Acc: 69.6607% Time: 1302.8263s\n",
            "#187 Loss: 0.5631 Acc: 70.7585% Time: 1309.7441s\n",
            "#188 Loss: 0.5644 Acc: 70.4591% Time: 1316.4880s\n",
            "#189 Loss: 0.5667 Acc: 70.2096% Time: 1323.2294s\n",
            "#190 Loss: 0.5648 Acc: 70.6088% Time: 1330.1360s\n",
            "#191 Loss: 0.5671 Acc: 69.8104% Time: 1336.8040s\n",
            "#192 Loss: 0.5621 Acc: 71.5070% Time: 1343.4340s\n",
            "#193 Loss: 0.5660 Acc: 70.3593% Time: 1350.1923s\n",
            "#194 Loss: 0.5629 Acc: 70.5589% Time: 1356.8783s\n",
            "#195 Loss: 0.5798 Acc: 69.3613% Time: 1363.7803s\n",
            "#196 Loss: 0.5547 Acc: 71.5070% Time: 1370.8819s\n",
            "#197 Loss: 0.5653 Acc: 70.1098% Time: 1377.6990s\n",
            "#198 Loss: 0.5739 Acc: 71.2076% Time: 1384.5669s\n",
            "#199 Loss: 0.5657 Acc: 70.1597% Time: 1391.4577s\n",
            "#200 Loss: 0.5713 Acc: 69.3613% Time: 1398.2001s\n",
            "#201 Loss: 0.5672 Acc: 69.9601% Time: 1404.9298s\n",
            "#202 Loss: 0.5649 Acc: 70.7585% Time: 1411.5847s\n",
            "#203 Loss: 0.5576 Acc: 71.6068% Time: 1418.3151s\n",
            "#204 Loss: 0.5619 Acc: 70.2096% Time: 1424.9630s\n",
            "#205 Loss: 0.5690 Acc: 70.1098% Time: 1431.6444s\n",
            "#206 Loss: 0.5661 Acc: 70.1098% Time: 1438.3150s\n",
            "#207 Loss: 0.5673 Acc: 70.2096% Time: 1444.9594s\n",
            "#208 Loss: 0.5736 Acc: 69.7106% Time: 1451.5285s\n",
            "#209 Loss: 0.5716 Acc: 70.6587% Time: 1458.1453s\n",
            "#210 Loss: 0.5613 Acc: 72.0559% Time: 1464.9097s\n",
            "#211 Loss: 0.5691 Acc: 70.1597% Time: 1471.5791s\n",
            "#212 Loss: 0.5679 Acc: 69.6607% Time: 1478.2024s\n",
            "#213 Loss: 0.5673 Acc: 71.0579% Time: 1484.9921s\n",
            "#214 Loss: 0.5708 Acc: 69.5609% Time: 1491.7800s\n",
            "#215 Loss: 0.5612 Acc: 70.4591% Time: 1498.3533s\n",
            "#216 Loss: 0.5653 Acc: 70.4092% Time: 1505.0954s\n",
            "#217 Loss: 0.5634 Acc: 70.3094% Time: 1511.7278s\n",
            "#218 Loss: 0.5770 Acc: 69.3114% Time: 1518.3058s\n",
            "#219 Loss: 0.5613 Acc: 71.5569% Time: 1525.1510s\n",
            "#220 Loss: 0.5644 Acc: 71.4571% Time: 1531.9493s\n",
            "#221 Loss: 0.5786 Acc: 70.5090% Time: 1538.8425s\n",
            "#222 Loss: 0.5635 Acc: 69.8104% Time: 1545.5012s\n",
            "#223 Loss: 0.5689 Acc: 70.3593% Time: 1552.1623s\n",
            "#224 Loss: 0.5553 Acc: 71.8563% Time: 1559.2056s\n",
            "#225 Loss: 0.5667 Acc: 70.3593% Time: 1566.0822s\n",
            "#226 Loss: 0.5650 Acc: 70.8084% Time: 1572.8719s\n",
            "#227 Loss: 0.5649 Acc: 70.1597% Time: 1579.5248s\n",
            "#228 Loss: 0.5637 Acc: 70.4092% Time: 1586.3178s\n",
            "#229 Loss: 0.5641 Acc: 70.4092% Time: 1593.3491s\n",
            "#230 Loss: 0.5774 Acc: 69.2116% Time: 1600.1462s\n",
            "#231 Loss: 0.5691 Acc: 70.2096% Time: 1607.0195s\n",
            "#232 Loss: 0.5639 Acc: 70.7086% Time: 1613.9010s\n",
            "#233 Loss: 0.5665 Acc: 70.2595% Time: 1620.8262s\n",
            "#234 Loss: 0.5766 Acc: 69.0619% Time: 1627.9576s\n",
            "#235 Loss: 0.5647 Acc: 70.6587% Time: 1634.7665s\n",
            "#236 Loss: 0.5635 Acc: 70.4092% Time: 1641.7863s\n",
            "#237 Loss: 0.5628 Acc: 69.8104% Time: 1648.4667s\n",
            "#238 Loss: 0.5655 Acc: 71.4571% Time: 1655.3259s\n",
            "#239 Loss: 0.5605 Acc: 70.8084% Time: 1662.1503s\n",
            "#240 Loss: 0.5638 Acc: 71.7565% Time: 1668.8420s\n",
            "#241 Loss: 0.5672 Acc: 70.4092% Time: 1675.5635s\n",
            "#242 Loss: 0.5602 Acc: 70.5090% Time: 1682.2570s\n",
            "#243 Loss: 0.5562 Acc: 72.4052% Time: 1689.0065s\n",
            "#244 Loss: 0.5634 Acc: 70.4591% Time: 1695.8358s\n",
            "#245 Loss: 0.5654 Acc: 70.4092% Time: 1702.6375s\n",
            "#246 Loss: 0.5637 Acc: 70.3094% Time: 1709.5147s\n",
            "#247 Loss: 0.5617 Acc: 69.6607% Time: 1716.2777s\n",
            "#248 Loss: 0.5691 Acc: 69.3114% Time: 1723.0656s\n",
            "#249 Loss: 0.5637 Acc: 70.5090% Time: 1729.8751s\n",
            "#250 Loss: 0.5455 Acc: 73.2036% Time: 1736.6705s\n",
            "#251 Loss: 0.5596 Acc: 71.6567% Time: 1743.4257s\n",
            "#252 Loss: 0.5620 Acc: 69.9601% Time: 1750.0664s\n",
            "#253 Loss: 0.5690 Acc: 70.1098% Time: 1756.8115s\n",
            "#254 Loss: 0.5647 Acc: 70.7585% Time: 1763.5060s\n",
            "#255 Loss: 0.5579 Acc: 71.2575% Time: 1770.2971s\n",
            "#256 Loss: 0.5654 Acc: 71.1078% Time: 1777.0644s\n",
            "#257 Loss: 0.5583 Acc: 71.7565% Time: 1783.9485s\n",
            "#258 Loss: 0.5631 Acc: 71.8064% Time: 1790.7877s\n",
            "#259 Loss: 0.5615 Acc: 70.9581% Time: 1798.0248s\n",
            "#260 Loss: 0.5629 Acc: 70.9082% Time: 1804.8535s\n",
            "#261 Loss: 0.5636 Acc: 70.8084% Time: 1811.7698s\n",
            "#262 Loss: 0.5621 Acc: 70.9581% Time: 1818.8619s\n",
            "#263 Loss: 0.5577 Acc: 70.6088% Time: 1825.6935s\n",
            "#264 Loss: 0.5683 Acc: 70.0100% Time: 1832.3997s\n",
            "#265 Loss: 0.5612 Acc: 70.1597% Time: 1839.0946s\n",
            "#266 Loss: 0.5573 Acc: 71.7565% Time: 1845.9089s\n",
            "#267 Loss: 0.5638 Acc: 70.1098% Time: 1852.7235s\n",
            "#268 Loss: 0.5626 Acc: 71.0579% Time: 1859.4187s\n",
            "#269 Loss: 0.5568 Acc: 70.2096% Time: 1866.2396s\n",
            "#270 Loss: 0.5630 Acc: 70.7585% Time: 1873.0397s\n",
            "#271 Loss: 0.5667 Acc: 70.8084% Time: 1879.8090s\n",
            "#272 Loss: 0.5541 Acc: 71.4072% Time: 1886.4661s\n",
            "#273 Loss: 0.5638 Acc: 70.2595% Time: 1893.2137s\n",
            "#274 Loss: 0.5571 Acc: 71.4072% Time: 1899.9193s\n",
            "#275 Loss: 0.5538 Acc: 71.7565% Time: 1906.6576s\n",
            "#276 Loss: 0.5608 Acc: 70.5090% Time: 1913.3960s\n",
            "#277 Loss: 0.5603 Acc: 70.9581% Time: 1920.0201s\n",
            "#278 Loss: 0.5686 Acc: 69.9601% Time: 1926.8065s\n",
            "#279 Loss: 0.5570 Acc: 71.7066% Time: 1933.4497s\n",
            "#280 Loss: 0.5752 Acc: 69.9601% Time: 1940.2866s\n",
            "#281 Loss: 0.5705 Acc: 69.5110% Time: 1947.0647s\n",
            "#282 Loss: 0.5702 Acc: 70.1597% Time: 1953.8359s\n",
            "#283 Loss: 0.5548 Acc: 70.3593% Time: 1960.4438s\n",
            "#284 Loss: 0.5682 Acc: 69.7106% Time: 1967.1899s\n",
            "#285 Loss: 0.5838 Acc: 68.2635% Time: 1974.2706s\n",
            "#286 Loss: 0.5606 Acc: 70.9581% Time: 1981.2662s\n",
            "#287 Loss: 0.5558 Acc: 71.7066% Time: 1988.1883s\n",
            "#288 Loss: 0.5513 Acc: 70.6587% Time: 1995.1026s\n",
            "#289 Loss: 0.5498 Acc: 72.4551% Time: 2001.9934s\n",
            "#290 Loss: 0.5618 Acc: 70.5090% Time: 2008.9958s\n",
            "#291 Loss: 0.5576 Acc: 70.4591% Time: 2015.9827s\n",
            "#292 Loss: 0.5589 Acc: 71.4072% Time: 2023.1616s\n",
            "#293 Loss: 0.5553 Acc: 72.0559% Time: 2030.2520s\n",
            "#294 Loss: 0.5715 Acc: 70.2595% Time: 2037.0009s\n",
            "#295 Loss: 0.5550 Acc: 70.9581% Time: 2043.9670s\n",
            "#296 Loss: 0.5631 Acc: 70.9581% Time: 2050.8095s\n",
            "#297 Loss: 0.5536 Acc: 71.8563% Time: 2057.5995s\n",
            "#298 Loss: 0.5498 Acc: 71.6567% Time: 2064.3917s\n",
            "#299 Loss: 0.5650 Acc: 70.6088% Time: 2071.1917s\n",
            "#300 Loss: 0.5675 Acc: 71.4571% Time: 2077.9340s\n",
            "#301 Loss: 0.5513 Acc: 73.3533% Time: 2084.5818s\n",
            "#302 Loss: 0.5619 Acc: 70.6587% Time: 2091.3483s\n",
            "#303 Loss: 0.5531 Acc: 71.5070% Time: 2098.1906s\n",
            "#304 Loss: 0.5635 Acc: 71.7066% Time: 2104.9654s\n",
            "#305 Loss: 0.5634 Acc: 69.4611% Time: 2111.7875s\n",
            "#306 Loss: 0.5601 Acc: 71.5070% Time: 2118.5219s\n",
            "#307 Loss: 0.5606 Acc: 71.5569% Time: 2125.2778s\n",
            "#308 Loss: 0.5523 Acc: 72.2056% Time: 2132.0144s\n",
            "#309 Loss: 0.5633 Acc: 70.7086% Time: 2138.7033s\n",
            "#310 Loss: 0.5561 Acc: 71.3074% Time: 2145.4231s\n",
            "#311 Loss: 0.5566 Acc: 71.1078% Time: 2152.1402s\n",
            "#312 Loss: 0.5528 Acc: 71.3573% Time: 2158.9227s\n",
            "#313 Loss: 0.5598 Acc: 71.9561% Time: 2165.7709s\n",
            "#314 Loss: 0.5585 Acc: 70.8583% Time: 2172.5791s\n",
            "#315 Loss: 0.5556 Acc: 70.7585% Time: 2179.3567s\n",
            "#316 Loss: 0.5587 Acc: 70.7585% Time: 2186.1880s\n",
            "#317 Loss: 0.5522 Acc: 72.5050% Time: 2192.9958s\n",
            "#318 Loss: 0.5607 Acc: 71.0080% Time: 2199.7916s\n",
            "#319 Loss: 0.5518 Acc: 70.5589% Time: 2206.5399s\n",
            "#320 Loss: 0.5538 Acc: 72.1557% Time: 2213.5590s\n",
            "#321 Loss: 0.5568 Acc: 71.7565% Time: 2220.4040s\n",
            "#322 Loss: 0.5567 Acc: 71.6567% Time: 2227.2221s\n",
            "#323 Loss: 0.5571 Acc: 71.2575% Time: 2234.2213s\n",
            "#324 Loss: 0.5619 Acc: 70.6587% Time: 2241.1917s\n",
            "#325 Loss: 0.5587 Acc: 71.6567% Time: 2247.9833s\n",
            "#326 Loss: 0.5440 Acc: 72.2056% Time: 2254.7906s\n",
            "#327 Loss: 0.5502 Acc: 71.9062% Time: 2261.5670s\n",
            "#328 Loss: 0.5533 Acc: 71.2076% Time: 2268.5492s\n",
            "#329 Loss: 0.5548 Acc: 70.7585% Time: 2275.4087s\n",
            "#330 Loss: 0.5593 Acc: 70.5589% Time: 2282.2987s\n",
            "#331 Loss: 0.5510 Acc: 72.4551% Time: 2289.1556s\n",
            "#332 Loss: 0.5612 Acc: 70.4092% Time: 2295.9572s\n",
            "#333 Loss: 0.5557 Acc: 71.5070% Time: 2302.9653s\n",
            "#334 Loss: 0.5453 Acc: 72.0060% Time: 2309.7350s\n",
            "#335 Loss: 0.5623 Acc: 70.4591% Time: 2316.6410s\n",
            "#336 Loss: 0.5599 Acc: 71.0080% Time: 2323.5467s\n",
            "#337 Loss: 0.5598 Acc: 70.0100% Time: 2330.4110s\n",
            "#338 Loss: 0.5635 Acc: 70.2096% Time: 2337.3634s\n",
            "#339 Loss: 0.5696 Acc: 71.1078% Time: 2344.1944s\n",
            "#340 Loss: 0.5525 Acc: 71.1577% Time: 2351.1673s\n",
            "#341 Loss: 0.5525 Acc: 71.7565% Time: 2357.9422s\n",
            "#342 Loss: 0.5622 Acc: 70.4591% Time: 2364.8065s\n",
            "#343 Loss: 0.5542 Acc: 71.2076% Time: 2371.7204s\n",
            "#344 Loss: 0.5597 Acc: 71.6068% Time: 2378.7435s\n",
            "#345 Loss: 0.5510 Acc: 71.3074% Time: 2385.7517s\n",
            "#346 Loss: 0.5463 Acc: 72.7046% Time: 2392.7261s\n",
            "#347 Loss: 0.5515 Acc: 72.6048% Time: 2399.6395s\n",
            "#348 Loss: 0.5643 Acc: 69.1617% Time: 2406.5495s\n",
            "#349 Loss: 0.5638 Acc: 70.6587% Time: 2413.3351s\n",
            "#350 Loss: 0.5601 Acc: 71.2076% Time: 2420.1774s\n",
            "#351 Loss: 0.5561 Acc: 71.4571% Time: 2427.1539s\n",
            "#352 Loss: 0.5507 Acc: 71.9062% Time: 2433.9897s\n",
            "#353 Loss: 0.5650 Acc: 70.4092% Time: 2440.9032s\n",
            "#354 Loss: 0.5571 Acc: 70.8084% Time: 2447.7154s\n",
            "#355 Loss: 0.5543 Acc: 71.7066% Time: 2454.4938s\n",
            "#356 Loss: 0.5568 Acc: 70.8084% Time: 2461.5090s\n",
            "#357 Loss: 0.5531 Acc: 71.9062% Time: 2468.4459s\n",
            "#358 Loss: 0.5531 Acc: 71.0579% Time: 2475.2981s\n",
            "#359 Loss: 0.5638 Acc: 71.4571% Time: 2482.0937s\n",
            "#360 Loss: 0.5509 Acc: 71.3074% Time: 2488.9228s\n",
            "#361 Loss: 0.5538 Acc: 71.5569% Time: 2495.6259s\n",
            "#362 Loss: 0.5646 Acc: 70.1098% Time: 2502.5499s\n",
            "#363 Loss: 0.5611 Acc: 70.4092% Time: 2509.2657s\n",
            "#364 Loss: 0.5468 Acc: 72.2555% Time: 2516.0688s\n",
            "#365 Loss: 0.5588 Acc: 71.8064% Time: 2522.8825s\n",
            "#366 Loss: 0.5632 Acc: 70.7086% Time: 2529.8505s\n",
            "#367 Loss: 0.5570 Acc: 70.7086% Time: 2536.5876s\n",
            "#368 Loss: 0.5611 Acc: 70.3593% Time: 2543.3242s\n",
            "#369 Loss: 0.5525 Acc: 71.7066% Time: 2550.0686s\n",
            "#370 Loss: 0.5463 Acc: 72.7046% Time: 2556.8972s\n",
            "#371 Loss: 0.5559 Acc: 71.3074% Time: 2563.8379s\n",
            "#372 Loss: 0.5539 Acc: 71.2076% Time: 2570.6103s\n",
            "#373 Loss: 0.5553 Acc: 70.7585% Time: 2577.4761s\n",
            "#374 Loss: 0.5525 Acc: 70.8084% Time: 2584.2312s\n",
            "#375 Loss: 0.5558 Acc: 71.2076% Time: 2591.0285s\n",
            "#376 Loss: 0.5604 Acc: 71.6068% Time: 2597.8597s\n",
            "#377 Loss: 0.5565 Acc: 71.3573% Time: 2604.6121s\n",
            "#378 Loss: 0.5498 Acc: 71.6567% Time: 2611.4346s\n",
            "#379 Loss: 0.5457 Acc: 72.2056% Time: 2618.2548s\n",
            "#380 Loss: 0.5516 Acc: 71.8563% Time: 2625.0528s\n",
            "#381 Loss: 0.5424 Acc: 72.6547% Time: 2631.8789s\n",
            "#382 Loss: 0.5607 Acc: 70.5589% Time: 2638.6912s\n",
            "#383 Loss: 0.5534 Acc: 70.4591% Time: 2646.0685s\n",
            "#384 Loss: 0.5599 Acc: 69.8104% Time: 2656.7850s\n",
            "#385 Loss: 0.5596 Acc: 69.7106% Time: 2663.8605s\n",
            "#386 Loss: 0.5593 Acc: 71.3074% Time: 2671.0041s\n",
            "#387 Loss: 0.5438 Acc: 71.6567% Time: 2678.0438s\n",
            "#388 Loss: 0.5570 Acc: 71.2575% Time: 2685.1694s\n",
            "#389 Loss: 0.5599 Acc: 70.3593% Time: 2692.3584s\n",
            "#390 Loss: 0.5549 Acc: 71.7066% Time: 2699.6167s\n",
            "#391 Loss: 0.5500 Acc: 71.5070% Time: 2706.8253s\n",
            "#392 Loss: 0.5387 Acc: 73.4531% Time: 2713.9946s\n",
            "#393 Loss: 0.5498 Acc: 72.1557% Time: 2721.1657s\n",
            "#394 Loss: 0.5526 Acc: 71.9062% Time: 2728.3786s\n",
            "#395 Loss: 0.5540 Acc: 71.8064% Time: 2735.6194s\n",
            "#396 Loss: 0.5568 Acc: 71.0579% Time: 2742.8093s\n",
            "#397 Loss: 0.5522 Acc: 72.3553% Time: 2750.0326s\n",
            "#398 Loss: 0.5567 Acc: 70.4092% Time: 2757.2246s\n",
            "#399 Loss: 0.5464 Acc: 71.1577% Time: 2764.3983s\n",
            "#400 Loss: 0.5514 Acc: 71.3074% Time: 2771.5892s\n",
            "#401 Loss: 0.5601 Acc: 71.0080% Time: 2778.8441s\n",
            "#402 Loss: 0.5558 Acc: 71.1577% Time: 2785.9068s\n",
            "#403 Loss: 0.5509 Acc: 71.8563% Time: 2792.8869s\n",
            "#404 Loss: 0.5639 Acc: 70.3593% Time: 2799.8806s\n",
            "#405 Loss: 0.5687 Acc: 70.8583% Time: 2806.7381s\n",
            "#406 Loss: 0.5486 Acc: 72.0060% Time: 2813.6207s\n",
            "#407 Loss: 0.5608 Acc: 69.7106% Time: 2820.5291s\n",
            "#408 Loss: 0.5602 Acc: 70.3094% Time: 2827.4847s\n",
            "#409 Loss: 0.5681 Acc: 69.9601% Time: 2834.4664s\n",
            "#410 Loss: 0.5517 Acc: 71.2076% Time: 2841.3074s\n",
            "#411 Loss: 0.5656 Acc: 71.0080% Time: 2848.2521s\n",
            "#412 Loss: 0.5712 Acc: 69.3613% Time: 2855.1181s\n",
            "#413 Loss: 0.5468 Acc: 72.6048% Time: 2862.0312s\n",
            "#414 Loss: 0.5552 Acc: 70.5090% Time: 2868.8817s\n",
            "#415 Loss: 0.5570 Acc: 71.2076% Time: 2875.7333s\n",
            "#416 Loss: 0.5663 Acc: 70.0100% Time: 2882.6201s\n",
            "#417 Loss: 0.5479 Acc: 71.5569% Time: 2889.4521s\n",
            "#418 Loss: 0.5653 Acc: 70.3593% Time: 2896.2818s\n",
            "#419 Loss: 0.5557 Acc: 71.0080% Time: 2903.1819s\n",
            "#420 Loss: 0.5563 Acc: 72.0060% Time: 2910.0334s\n",
            "#421 Loss: 0.5387 Acc: 72.7545% Time: 2916.9983s\n",
            "#422 Loss: 0.5505 Acc: 71.6567% Time: 2923.8951s\n",
            "#423 Loss: 0.5568 Acc: 71.5070% Time: 2930.8550s\n",
            "#424 Loss: 0.5525 Acc: 71.0579% Time: 2937.7912s\n",
            "#425 Loss: 0.5478 Acc: 73.0040% Time: 2944.5854s\n",
            "#426 Loss: 0.5594 Acc: 71.2575% Time: 2951.5367s\n",
            "#427 Loss: 0.5580 Acc: 71.1078% Time: 2958.3552s\n",
            "#428 Loss: 0.5507 Acc: 71.0579% Time: 2965.2629s\n",
            "#429 Loss: 0.5441 Acc: 72.0060% Time: 2972.2104s\n",
            "#430 Loss: 0.5408 Acc: 73.0040% Time: 2979.1804s\n",
            "#431 Loss: 0.5539 Acc: 71.7565% Time: 2986.0149s\n",
            "#432 Loss: 0.5599 Acc: 71.0080% Time: 2992.9636s\n",
            "#433 Loss: 0.5595 Acc: 71.2076% Time: 2999.9090s\n",
            "#434 Loss: 0.5558 Acc: 72.1557% Time: 3006.8004s\n",
            "#435 Loss: 0.5580 Acc: 72.0060% Time: 3013.6013s\n",
            "#436 Loss: 0.5592 Acc: 71.8563% Time: 3020.5335s\n",
            "#437 Loss: 0.5510 Acc: 71.3573% Time: 3027.3962s\n",
            "#438 Loss: 0.5562 Acc: 72.2056% Time: 3034.3410s\n",
            "#439 Loss: 0.5469 Acc: 71.8064% Time: 3041.4001s\n",
            "#440 Loss: 0.5514 Acc: 71.6068% Time: 3048.2980s\n",
            "#441 Loss: 0.5552 Acc: 71.6567% Time: 3055.1669s\n",
            "#442 Loss: 0.5557 Acc: 71.6567% Time: 3062.0431s\n",
            "#443 Loss: 0.5497 Acc: 72.7545% Time: 3068.8555s\n",
            "#444 Loss: 0.5546 Acc: 70.8583% Time: 3075.7869s\n",
            "#445 Loss: 0.5497 Acc: 71.4571% Time: 3082.6579s\n",
            "#446 Loss: 0.5547 Acc: 71.8064% Time: 3089.5190s\n",
            "#447 Loss: 0.5515 Acc: 71.6567% Time: 3096.3174s\n",
            "#448 Loss: 0.5449 Acc: 72.9042% Time: 3103.3231s\n",
            "#449 Loss: 0.5580 Acc: 71.1577% Time: 3110.1944s\n",
            "#450 Loss: 0.5540 Acc: 71.7565% Time: 3117.0976s\n",
            "#451 Loss: 0.5453 Acc: 71.5569% Time: 3123.9621s\n",
            "#452 Loss: 0.5524 Acc: 71.9561% Time: 3130.8475s\n",
            "#453 Loss: 0.5396 Acc: 73.0040% Time: 3137.9422s\n",
            "#454 Loss: 0.5516 Acc: 72.8044% Time: 3144.9064s\n",
            "#455 Loss: 0.5539 Acc: 71.0579% Time: 3151.8908s\n",
            "#456 Loss: 0.5531 Acc: 71.6567% Time: 3158.7040s\n",
            "#457 Loss: 0.5532 Acc: 71.8064% Time: 3165.5169s\n",
            "#458 Loss: 0.5568 Acc: 70.6088% Time: 3172.4186s\n",
            "#459 Loss: 0.5435 Acc: 73.3533% Time: 3179.3457s\n",
            "#460 Loss: 0.5549 Acc: 70.5589% Time: 3186.3012s\n",
            "#461 Loss: 0.5432 Acc: 73.0539% Time: 3193.2197s\n",
            "#462 Loss: 0.5446 Acc: 72.1557% Time: 3200.0624s\n",
            "#463 Loss: 0.5470 Acc: 71.5569% Time: 3206.9097s\n",
            "#464 Loss: 0.5497 Acc: 71.2575% Time: 3213.7931s\n",
            "#465 Loss: 0.5454 Acc: 71.9561% Time: 3220.6554s\n",
            "#466 Loss: 0.5459 Acc: 72.3553% Time: 3227.4484s\n",
            "#467 Loss: 0.5541 Acc: 71.1577% Time: 3234.2729s\n",
            "#468 Loss: 0.5528 Acc: 72.8044% Time: 3241.2483s\n",
            "#469 Loss: 0.5497 Acc: 71.3573% Time: 3248.0368s\n",
            "#470 Loss: 0.5506 Acc: 71.7066% Time: 3254.8089s\n",
            "#471 Loss: 0.5574 Acc: 70.4591% Time: 3261.6076s\n",
            "#472 Loss: 0.5506 Acc: 72.5050% Time: 3268.4833s\n",
            "#473 Loss: 0.5631 Acc: 71.5569% Time: 3275.4264s\n",
            "#474 Loss: 0.5492 Acc: 71.9062% Time: 3282.3078s\n",
            "#475 Loss: 0.5500 Acc: 71.8563% Time: 3289.2730s\n",
            "#476 Loss: 0.5589 Acc: 71.0080% Time: 3296.0686s\n",
            "#477 Loss: 0.5486 Acc: 71.3573% Time: 3302.9361s\n",
            "#478 Loss: 0.5558 Acc: 69.7605% Time: 3309.8862s\n",
            "#479 Loss: 0.5462 Acc: 71.6068% Time: 3316.8162s\n",
            "#480 Loss: 0.5428 Acc: 72.2056% Time: 3323.7116s\n",
            "#481 Loss: 0.5416 Acc: 72.5549% Time: 3330.6390s\n",
            "#482 Loss: 0.5432 Acc: 72.3054% Time: 3337.4851s\n",
            "#483 Loss: 0.5468 Acc: 71.7565% Time: 3344.2933s\n",
            "#484 Loss: 0.5509 Acc: 71.0579% Time: 3351.2492s\n",
            "#485 Loss: 0.5587 Acc: 71.3074% Time: 3358.0911s\n",
            "#486 Loss: 0.5528 Acc: 70.8084% Time: 3365.0581s\n",
            "#487 Loss: 0.5523 Acc: 70.8583% Time: 3371.9828s\n",
            "#488 Loss: 0.5567 Acc: 71.7066% Time: 3378.9334s\n",
            "#489 Loss: 0.5528 Acc: 71.3074% Time: 3385.8318s\n",
            "#490 Loss: 0.5548 Acc: 71.5070% Time: 3392.7765s\n",
            "#491 Loss: 0.5503 Acc: 71.0080% Time: 3399.6020s\n",
            "#492 Loss: 0.5443 Acc: 72.8543% Time: 3406.3717s\n",
            "#493 Loss: 0.5423 Acc: 73.2036% Time: 3413.2585s\n",
            "#494 Loss: 0.5400 Acc: 72.3553% Time: 3420.0538s\n",
            "#495 Loss: 0.5550 Acc: 71.7066% Time: 3427.0918s\n",
            "#496 Loss: 0.5489 Acc: 71.2076% Time: 3434.0465s\n",
            "#497 Loss: 0.5572 Acc: 71.2076% Time: 3441.0331s\n",
            "#498 Loss: 0.5488 Acc: 71.6068% Time: 3447.8796s\n",
            "#499 Loss: 0.5468 Acc: 72.1557% Time: 3454.7709s\n",
            "#500 Loss: 0.5467 Acc: 72.6547% Time: 3461.5650s\n",
            "#501 Loss: 0.5535 Acc: 71.5070% Time: 3468.6072s\n",
            "#502 Loss: 0.5573 Acc: 70.5090% Time: 3475.5738s\n",
            "#503 Loss: 0.5527 Acc: 72.1557% Time: 3482.4164s\n",
            "#504 Loss: 0.5600 Acc: 70.3094% Time: 3489.3907s\n",
            "#505 Loss: 0.5388 Acc: 72.2555% Time: 3496.4028s\n",
            "#506 Loss: 0.5440 Acc: 71.2575% Time: 3503.2192s\n",
            "#507 Loss: 0.5500 Acc: 71.5070% Time: 3510.0797s\n",
            "#508 Loss: 0.5550 Acc: 70.4092% Time: 3517.0070s\n",
            "#509 Loss: 0.5481 Acc: 71.7565% Time: 3523.9568s\n",
            "#510 Loss: 0.5475 Acc: 72.8044% Time: 3530.7808s\n",
            "#511 Loss: 0.5406 Acc: 73.9022% Time: 3537.7079s\n",
            "#512 Loss: 0.5415 Acc: 71.6567% Time: 3544.6178s\n",
            "#513 Loss: 0.5407 Acc: 71.6567% Time: 3551.6154s\n",
            "#514 Loss: 0.5454 Acc: 72.2056% Time: 3558.6202s\n",
            "#515 Loss: 0.5473 Acc: 71.4571% Time: 3565.6143s\n",
            "#516 Loss: 0.5511 Acc: 72.5549% Time: 3572.5743s\n",
            "#517 Loss: 0.5473 Acc: 72.2056% Time: 3579.5279s\n",
            "#518 Loss: 0.5412 Acc: 72.1058% Time: 3586.4140s\n",
            "#519 Loss: 0.5398 Acc: 72.4052% Time: 3593.3327s\n",
            "#520 Loss: 0.5633 Acc: 71.7066% Time: 3600.2030s\n",
            "#521 Loss: 0.5428 Acc: 71.6567% Time: 3607.1589s\n",
            "#522 Loss: 0.5478 Acc: 71.4072% Time: 3614.0928s\n",
            "#523 Loss: 0.5466 Acc: 71.7066% Time: 3621.0020s\n",
            "#524 Loss: 0.5457 Acc: 71.9561% Time: 3627.9661s\n",
            "#525 Loss: 0.5551 Acc: 70.9082% Time: 3634.7847s\n",
            "#526 Loss: 0.5479 Acc: 71.9561% Time: 3641.7674s\n",
            "#527 Loss: 0.5456 Acc: 72.7545% Time: 3648.7811s\n",
            "#528 Loss: 0.5535 Acc: 71.6068% Time: 3655.5731s\n",
            "#529 Loss: 0.5595 Acc: 71.6567% Time: 3662.5412s\n",
            "#530 Loss: 0.5385 Acc: 73.4531% Time: 3669.5316s\n",
            "#531 Loss: 0.5478 Acc: 72.3054% Time: 3676.3669s\n",
            "#532 Loss: 0.5439 Acc: 72.1557% Time: 3683.1617s\n",
            "#533 Loss: 0.5488 Acc: 70.9581% Time: 3689.9421s\n",
            "#534 Loss: 0.5560 Acc: 72.7545% Time: 3696.7594s\n",
            "#535 Loss: 0.5409 Acc: 72.1058% Time: 3703.5422s\n",
            "#536 Loss: 0.5521 Acc: 71.9062% Time: 3710.3169s\n",
            "#537 Loss: 0.5381 Acc: 72.6048% Time: 3717.0821s\n",
            "#538 Loss: 0.5437 Acc: 72.6048% Time: 3723.8917s\n",
            "#539 Loss: 0.5471 Acc: 70.7086% Time: 3730.7299s\n",
            "#540 Loss: 0.5464 Acc: 71.4072% Time: 3737.5053s\n",
            "#541 Loss: 0.5547 Acc: 71.6567% Time: 3744.3594s\n",
            "#542 Loss: 0.5471 Acc: 72.3553% Time: 3751.1837s\n",
            "#543 Loss: 0.5525 Acc: 72.2555% Time: 3758.1060s\n",
            "#544 Loss: 0.5529 Acc: 70.6587% Time: 3764.8987s\n",
            "#545 Loss: 0.5455 Acc: 72.4052% Time: 3771.7174s\n",
            "#546 Loss: 0.5406 Acc: 73.1537% Time: 3778.4852s\n",
            "#547 Loss: 0.5609 Acc: 71.2076% Time: 3785.2753s\n",
            "#548 Loss: 0.5478 Acc: 72.2555% Time: 3792.0264s\n",
            "#549 Loss: 0.5459 Acc: 71.9062% Time: 3798.8880s\n",
            "#550 Loss: 0.5414 Acc: 72.8044% Time: 3805.7017s\n",
            "#551 Loss: 0.5568 Acc: 71.8064% Time: 3812.6670s\n",
            "#552 Loss: 0.5524 Acc: 72.0060% Time: 3819.5416s\n",
            "#553 Loss: 0.5435 Acc: 72.3553% Time: 3826.4535s\n",
            "#554 Loss: 0.5458 Acc: 71.1078% Time: 3833.2944s\n",
            "#555 Loss: 0.5456 Acc: 72.0559% Time: 3840.1569s\n",
            "#556 Loss: 0.5432 Acc: 72.1058% Time: 3847.0768s\n",
            "#557 Loss: 0.5463 Acc: 72.2056% Time: 3853.8996s\n",
            "#558 Loss: 0.5378 Acc: 72.7046% Time: 3860.8591s\n",
            "#559 Loss: 0.5538 Acc: 71.4571% Time: 3867.8745s\n",
            "#560 Loss: 0.5547 Acc: 71.4072% Time: 3874.6902s\n",
            "#561 Loss: 0.5415 Acc: 73.0539% Time: 3881.4992s\n",
            "#562 Loss: 0.5518 Acc: 71.8064% Time: 3888.3490s\n",
            "#563 Loss: 0.5419 Acc: 72.2056% Time: 3895.1868s\n",
            "#564 Loss: 0.5487 Acc: 71.3074% Time: 3902.0771s\n",
            "#565 Loss: 0.5502 Acc: 71.6068% Time: 3908.9341s\n",
            "#566 Loss: 0.5354 Acc: 72.1557% Time: 3915.9415s\n",
            "#567 Loss: 0.5407 Acc: 72.1557% Time: 3922.8487s\n",
            "#568 Loss: 0.5454 Acc: 72.2056% Time: 3929.6759s\n",
            "#569 Loss: 0.5522 Acc: 71.7066% Time: 3936.5448s\n",
            "#570 Loss: 0.5374 Acc: 72.5549% Time: 3943.4364s\n",
            "#571 Loss: 0.5397 Acc: 72.6048% Time: 3950.2911s\n",
            "#572 Loss: 0.5400 Acc: 73.7026% Time: 3957.2246s\n",
            "#573 Loss: 0.5626 Acc: 71.0080% Time: 3964.1023s\n",
            "#574 Loss: 0.5518 Acc: 71.8064% Time: 3970.9796s\n",
            "#575 Loss: 0.5482 Acc: 72.0559% Time: 3977.8610s\n",
            "#576 Loss: 0.5573 Acc: 71.4072% Time: 3984.6445s\n",
            "#577 Loss: 0.5515 Acc: 71.2076% Time: 3991.6109s\n",
            "#578 Loss: 0.5482 Acc: 70.9581% Time: 3998.5831s\n",
            "#579 Loss: 0.5608 Acc: 71.0080% Time: 4005.4645s\n",
            "#580 Loss: 0.5479 Acc: 71.7565% Time: 4012.4185s\n",
            "#581 Loss: 0.5408 Acc: 72.3553% Time: 4019.3322s\n",
            "#582 Loss: 0.5655 Acc: 70.1597% Time: 4026.2100s\n",
            "#583 Loss: 0.5471 Acc: 72.8044% Time: 4032.9735s\n",
            "#584 Loss: 0.5492 Acc: 71.2575% Time: 4039.8583s\n",
            "#585 Loss: 0.5575 Acc: 71.5569% Time: 4046.7724s\n",
            "#586 Loss: 0.5494 Acc: 72.2056% Time: 4053.5583s\n",
            "#587 Loss: 0.5478 Acc: 72.2056% Time: 4060.4741s\n",
            "#588 Loss: 0.5521 Acc: 71.4571% Time: 4067.3949s\n",
            "#589 Loss: 0.5436 Acc: 72.0060% Time: 4074.3173s\n",
            "#590 Loss: 0.5489 Acc: 71.2575% Time: 4081.2272s\n",
            "#591 Loss: 0.5538 Acc: 71.3573% Time: 4088.2540s\n",
            "#592 Loss: 0.5518 Acc: 72.5549% Time: 4095.0651s\n",
            "#593 Loss: 0.5500 Acc: 71.6567% Time: 4102.0213s\n",
            "#594 Loss: 0.5517 Acc: 71.4072% Time: 4108.9927s\n",
            "#595 Loss: 0.5483 Acc: 72.6547% Time: 4115.9376s\n",
            "#596 Loss: 0.5461 Acc: 72.3054% Time: 4122.8185s\n",
            "#597 Loss: 0.5501 Acc: 71.9561% Time: 4129.8201s\n",
            "#598 Loss: 0.5510 Acc: 70.9581% Time: 4136.6432s\n",
            "#599 Loss: 0.5634 Acc: 71.1577% Time: 4143.6389s\n",
            "#600 Loss: 0.5450 Acc: 71.5569% Time: 4150.5667s\n",
            "#601 Loss: 0.5416 Acc: 72.4551% Time: 4157.4050s\n",
            "#602 Loss: 0.5462 Acc: 72.1058% Time: 4164.3857s\n",
            "#603 Loss: 0.5495 Acc: 71.4571% Time: 4171.2229s\n",
            "#604 Loss: 0.5403 Acc: 72.0060% Time: 4178.0919s\n",
            "#605 Loss: 0.5403 Acc: 72.5050% Time: 4184.9753s\n",
            "#606 Loss: 0.5468 Acc: 72.9541% Time: 4191.7335s\n",
            "#607 Loss: 0.5521 Acc: 72.4052% Time: 4198.7523s\n",
            "#608 Loss: 0.5348 Acc: 72.9541% Time: 4205.6922s\n",
            "#609 Loss: 0.5541 Acc: 71.4072% Time: 4212.5148s\n",
            "#610 Loss: 0.5433 Acc: 73.0040% Time: 4219.4169s\n",
            "#611 Loss: 0.5528 Acc: 71.3573% Time: 4226.3817s\n",
            "#612 Loss: 0.5415 Acc: 73.5529% Time: 4233.1600s\n",
            "#613 Loss: 0.5538 Acc: 72.4052% Time: 4239.9846s\n",
            "#614 Loss: 0.5464 Acc: 72.4551% Time: 4246.9260s\n",
            "#615 Loss: 0.5496 Acc: 71.2575% Time: 4253.8007s\n",
            "#616 Loss: 0.5425 Acc: 72.4052% Time: 4260.7456s\n",
            "#617 Loss: 0.5477 Acc: 71.9561% Time: 4267.6774s\n",
            "#618 Loss: 0.5489 Acc: 71.0080% Time: 4274.5788s\n",
            "#619 Loss: 0.5401 Acc: 72.6048% Time: 4281.5207s\n",
            "#620 Loss: 0.5449 Acc: 72.3553% Time: 4288.3576s\n",
            "#621 Loss: 0.5571 Acc: 70.6088% Time: 4295.3640s\n",
            "#622 Loss: 0.5494 Acc: 71.4571% Time: 4302.3048s\n",
            "#623 Loss: 0.5358 Acc: 73.3533% Time: 4309.1407s\n",
            "#624 Loss: 0.5427 Acc: 72.7545% Time: 4315.9249s\n",
            "#625 Loss: 0.5576 Acc: 70.8583% Time: 4322.8914s\n",
            "#626 Loss: 0.5500 Acc: 72.7046% Time: 4329.7435s\n",
            "#627 Loss: 0.5563 Acc: 71.2076% Time: 4336.7252s\n",
            "#628 Loss: 0.5481 Acc: 71.6567% Time: 4343.5489s\n",
            "#629 Loss: 0.5419 Acc: 72.6547% Time: 4350.4889s\n",
            "#630 Loss: 0.5429 Acc: 72.2056% Time: 4357.3792s\n",
            "#631 Loss: 0.5476 Acc: 71.7565% Time: 4364.0316s\n",
            "#632 Loss: 0.5467 Acc: 73.6028% Time: 4370.9179s\n",
            "#633 Loss: 0.5550 Acc: 70.2096% Time: 4377.7334s\n",
            "#634 Loss: 0.5438 Acc: 71.9062% Time: 4384.5230s\n",
            "#635 Loss: 0.5480 Acc: 71.9561% Time: 4391.5213s\n",
            "#636 Loss: 0.5503 Acc: 71.9062% Time: 4398.5151s\n",
            "#637 Loss: 0.5443 Acc: 72.5050% Time: 4405.3341s\n",
            "#638 Loss: 0.5527 Acc: 71.1078% Time: 4412.3019s\n",
            "#639 Loss: 0.5406 Acc: 73.6028% Time: 4419.1509s\n",
            "#640 Loss: 0.5620 Acc: 70.4092% Time: 4425.9341s\n",
            "#641 Loss: 0.5504 Acc: 71.4571% Time: 4432.9031s\n",
            "#642 Loss: 0.5532 Acc: 71.1078% Time: 4439.6997s\n",
            "#643 Loss: 0.5381 Acc: 72.5549% Time: 4446.6176s\n",
            "#644 Loss: 0.5424 Acc: 71.7066% Time: 4453.4972s\n",
            "#645 Loss: 0.5413 Acc: 72.6048% Time: 4460.5117s\n",
            "#646 Loss: 0.5382 Acc: 72.9042% Time: 4467.4209s\n",
            "#647 Loss: 0.5440 Acc: 71.0579% Time: 4474.3598s\n",
            "#648 Loss: 0.5350 Acc: 73.5030% Time: 4481.2265s\n",
            "#649 Loss: 0.5393 Acc: 73.0040% Time: 4488.1396s\n",
            "#650 Loss: 0.5351 Acc: 73.2535% Time: 4495.0811s\n",
            "#651 Loss: 0.5557 Acc: 70.7086% Time: 4502.0503s\n",
            "#652 Loss: 0.5547 Acc: 71.0579% Time: 4508.9109s\n",
            "#653 Loss: 0.5412 Acc: 71.8064% Time: 4515.9144s\n",
            "#654 Loss: 0.5600 Acc: 70.7086% Time: 4522.6520s\n",
            "#655 Loss: 0.5436 Acc: 72.0559% Time: 4529.5889s\n",
            "#656 Loss: 0.5428 Acc: 72.5050% Time: 4536.4886s\n",
            "#657 Loss: 0.5502 Acc: 71.1078% Time: 4543.2472s\n",
            "#658 Loss: 0.5483 Acc: 70.8084% Time: 4550.1101s\n",
            "#659 Loss: 0.5520 Acc: 72.1058% Time: 4557.1012s\n",
            "#660 Loss: 0.5485 Acc: 71.9561% Time: 4564.0296s\n",
            "#661 Loss: 0.5515 Acc: 72.0060% Time: 4570.9159s\n",
            "#662 Loss: 0.5543 Acc: 71.0579% Time: 4577.8425s\n",
            "#663 Loss: 0.5450 Acc: 73.0539% Time: 4584.8853s\n",
            "#664 Loss: 0.5536 Acc: 71.5070% Time: 4591.8626s\n",
            "#665 Loss: 0.5453 Acc: 71.9062% Time: 4598.7990s\n",
            "#666 Loss: 0.5493 Acc: 71.9062% Time: 4605.6421s\n",
            "#667 Loss: 0.5526 Acc: 71.2076% Time: 4612.5014s\n",
            "#668 Loss: 0.5482 Acc: 72.1557% Time: 4619.2628s\n",
            "#669 Loss: 0.5587 Acc: 71.2575% Time: 4626.1728s\n",
            "#670 Loss: 0.5552 Acc: 71.3074% Time: 4633.1978s\n",
            "#671 Loss: 0.5556 Acc: 70.9581% Time: 4640.0383s\n",
            "#672 Loss: 0.5510 Acc: 72.6048% Time: 4646.8833s\n",
            "#673 Loss: 0.5489 Acc: 70.7585% Time: 4654.0051s\n",
            "#674 Loss: 0.5520 Acc: 71.3573% Time: 4660.9864s\n",
            "#675 Loss: 0.5525 Acc: 71.5569% Time: 4667.8866s\n",
            "#676 Loss: 0.5598 Acc: 70.0100% Time: 4674.7618s\n",
            "#677 Loss: 0.5535 Acc: 71.7565% Time: 4681.5732s\n",
            "#678 Loss: 0.5411 Acc: 72.6547% Time: 4688.4215s\n",
            "#679 Loss: 0.5408 Acc: 72.5050% Time: 4695.4041s\n",
            "#680 Loss: 0.5397 Acc: 72.9042% Time: 4702.2441s\n",
            "#681 Loss: 0.5359 Acc: 73.7026% Time: 4709.1117s\n",
            "#682 Loss: 0.5446 Acc: 72.2555% Time: 4715.9926s\n",
            "#683 Loss: 0.5517 Acc: 72.2555% Time: 4723.0219s\n",
            "#684 Loss: 0.5497 Acc: 72.3553% Time: 4729.8841s\n",
            "#685 Loss: 0.5479 Acc: 72.5549% Time: 4736.7748s\n",
            "#686 Loss: 0.5444 Acc: 72.4052% Time: 4743.6030s\n",
            "#687 Loss: 0.5525 Acc: 72.3553% Time: 4750.3340s\n",
            "#688 Loss: 0.5441 Acc: 72.0559% Time: 4757.1761s\n",
            "#689 Loss: 0.5474 Acc: 71.8563% Time: 4763.9702s\n",
            "#690 Loss: 0.5495 Acc: 71.5569% Time: 4770.7555s\n",
            "#691 Loss: 0.5562 Acc: 70.6088% Time: 4777.6484s\n",
            "#692 Loss: 0.5330 Acc: 74.2016% Time: 4784.4955s\n",
            "#693 Loss: 0.5384 Acc: 73.0539% Time: 4791.3744s\n",
            "#694 Loss: 0.5492 Acc: 71.5070% Time: 4798.1645s\n",
            "#695 Loss: 0.5354 Acc: 73.2036% Time: 4804.9909s\n",
            "#696 Loss: 0.5496 Acc: 71.4072% Time: 4811.8145s\n",
            "#697 Loss: 0.5416 Acc: 71.7565% Time: 4818.6450s\n",
            "#698 Loss: 0.5452 Acc: 72.4052% Time: 4825.5538s\n",
            "#699 Loss: 0.5532 Acc: 71.1577% Time: 4832.3002s\n",
            "#700 Loss: 0.5418 Acc: 72.7545% Time: 4839.1435s\n",
            "#701 Loss: 0.5461 Acc: 72.0060% Time: 4845.9345s\n",
            "#702 Loss: 0.5398 Acc: 72.2555% Time: 4852.7198s\n",
            "#703 Loss: 0.5515 Acc: 70.4591% Time: 4859.4833s\n",
            "#704 Loss: 0.5437 Acc: 72.1557% Time: 4866.3035s\n",
            "#705 Loss: 0.5500 Acc: 71.5569% Time: 4873.1267s\n",
            "#706 Loss: 0.5516 Acc: 72.4551% Time: 4879.8972s\n",
            "#707 Loss: 0.5426 Acc: 72.1058% Time: 4886.7932s\n",
            "#708 Loss: 0.5463 Acc: 71.6567% Time: 4893.6403s\n",
            "#709 Loss: 0.5408 Acc: 73.0040% Time: 4900.5902s\n",
            "#710 Loss: 0.5523 Acc: 71.4072% Time: 4907.5550s\n",
            "#711 Loss: 0.5529 Acc: 72.0559% Time: 4914.4921s\n",
            "#712 Loss: 0.5570 Acc: 72.0060% Time: 4921.3939s\n",
            "#713 Loss: 0.5532 Acc: 71.0579% Time: 4928.2909s\n",
            "#714 Loss: 0.5508 Acc: 71.9062% Time: 4935.1247s\n",
            "#715 Loss: 0.5401 Acc: 71.5569% Time: 4942.0024s\n",
            "#716 Loss: 0.5547 Acc: 70.4591% Time: 4948.9437s\n",
            "#717 Loss: 0.5443 Acc: 71.3074% Time: 4955.8907s\n",
            "#718 Loss: 0.5533 Acc: 71.0080% Time: 4962.7340s\n",
            "#719 Loss: 0.5339 Acc: 72.2555% Time: 4969.7286s\n",
            "#720 Loss: 0.5444 Acc: 72.1058% Time: 4976.5482s\n",
            "#721 Loss: 0.5413 Acc: 72.1557% Time: 4983.3285s\n",
            "#722 Loss: 0.5459 Acc: 71.5569% Time: 4990.1775s\n",
            "#723 Loss: 0.5479 Acc: 71.7565% Time: 4997.0848s\n",
            "#724 Loss: 0.5401 Acc: 73.1537% Time: 5003.9706s\n",
            "#725 Loss: 0.5325 Acc: 74.2016% Time: 5010.9207s\n",
            "#726 Loss: 0.5445 Acc: 72.7545% Time: 5017.9205s\n",
            "#727 Loss: 0.5482 Acc: 72.2056% Time: 5024.8391s\n",
            "#728 Loss: 0.5402 Acc: 70.9082% Time: 5031.7298s\n",
            "#729 Loss: 0.5470 Acc: 72.4052% Time: 5038.5961s\n",
            "#730 Loss: 0.5597 Acc: 70.4591% Time: 5045.5199s\n",
            "#731 Loss: 0.5607 Acc: 70.6088% Time: 5052.4504s\n",
            "#732 Loss: 0.5462 Acc: 71.7565% Time: 5059.4363s\n",
            "#733 Loss: 0.5415 Acc: 72.2555% Time: 5066.3047s\n",
            "#734 Loss: 0.5392 Acc: 72.5549% Time: 5073.1474s\n",
            "#735 Loss: 0.5500 Acc: 71.5070% Time: 5080.0880s\n",
            "#736 Loss: 0.5475 Acc: 71.3573% Time: 5087.1218s\n",
            "#737 Loss: 0.5402 Acc: 71.8064% Time: 5094.0884s\n",
            "#738 Loss: 0.5465 Acc: 71.5070% Time: 5101.1034s\n",
            "#739 Loss: 0.5404 Acc: 72.9541% Time: 5108.1017s\n",
            "#740 Loss: 0.5474 Acc: 72.5050% Time: 5115.0701s\n",
            "#741 Loss: 0.5346 Acc: 71.9561% Time: 5122.0980s\n",
            "#742 Loss: 0.5422 Acc: 72.1058% Time: 5128.8552s\n",
            "#743 Loss: 0.5518 Acc: 71.6567% Time: 5135.7613s\n",
            "#744 Loss: 0.5419 Acc: 72.0060% Time: 5142.6738s\n",
            "#745 Loss: 0.5390 Acc: 72.0060% Time: 5149.6462s\n",
            "#746 Loss: 0.5416 Acc: 71.3573% Time: 5156.5222s\n",
            "#747 Loss: 0.5428 Acc: 72.9541% Time: 5163.4512s\n",
            "#748 Loss: 0.5441 Acc: 71.6567% Time: 5170.4256s\n",
            "#749 Loss: 0.5415 Acc: 72.5050% Time: 5177.2725s\n",
            "#750 Loss: 0.5441 Acc: 71.9561% Time: 5184.0650s\n",
            "#751 Loss: 0.5405 Acc: 71.9561% Time: 5191.0503s\n",
            "#752 Loss: 0.5396 Acc: 71.9062% Time: 5198.0080s\n",
            "#753 Loss: 0.5386 Acc: 73.5529% Time: 5205.0356s\n",
            "#754 Loss: 0.5395 Acc: 72.6048% Time: 5211.8736s\n",
            "#755 Loss: 0.5387 Acc: 72.4052% Time: 5218.8292s\n",
            "#756 Loss: 0.5465 Acc: 72.4052% Time: 5225.6339s\n",
            "#757 Loss: 0.5474 Acc: 72.5050% Time: 5232.4888s\n",
            "#758 Loss: 0.5422 Acc: 71.9561% Time: 5239.2730s\n",
            "#759 Loss: 0.5383 Acc: 71.7565% Time: 5246.1837s\n",
            "#760 Loss: 0.5409 Acc: 72.4551% Time: 5253.2145s\n",
            "#761 Loss: 0.5470 Acc: 72.7046% Time: 5260.0873s\n",
            "#762 Loss: 0.5459 Acc: 71.7565% Time: 5266.8650s\n",
            "#763 Loss: 0.5465 Acc: 71.9561% Time: 5273.7851s\n",
            "#764 Loss: 0.5432 Acc: 72.5050% Time: 5280.5162s\n",
            "#765 Loss: 0.5439 Acc: 72.9042% Time: 5287.3861s\n",
            "#766 Loss: 0.5601 Acc: 69.9102% Time: 5294.2291s\n",
            "#767 Loss: 0.5546 Acc: 70.5090% Time: 5301.0101s\n",
            "#768 Loss: 0.5479 Acc: 71.3074% Time: 5307.8399s\n",
            "#769 Loss: 0.5466 Acc: 71.3573% Time: 5314.6858s\n",
            "#770 Loss: 0.5526 Acc: 71.5070% Time: 5321.5973s\n",
            "#771 Loss: 0.5418 Acc: 71.9561% Time: 5328.6390s\n",
            "#772 Loss: 0.5367 Acc: 74.0020% Time: 5335.3776s\n",
            "#773 Loss: 0.5392 Acc: 72.6547% Time: 5342.1956s\n",
            "#774 Loss: 0.5443 Acc: 71.7565% Time: 5349.0374s\n",
            "#775 Loss: 0.5507 Acc: 72.1557% Time: 5355.9648s\n",
            "#776 Loss: 0.5563 Acc: 70.7585% Time: 5362.7954s\n",
            "#777 Loss: 0.5395 Acc: 73.0040% Time: 5369.6375s\n",
            "#778 Loss: 0.5531 Acc: 71.0579% Time: 5376.5704s\n",
            "#779 Loss: 0.5446 Acc: 71.8064% Time: 5383.4297s\n",
            "#780 Loss: 0.5430 Acc: 72.0060% Time: 5390.3544s\n",
            "#781 Loss: 0.5509 Acc: 71.7565% Time: 5397.3832s\n",
            "#782 Loss: 0.5395 Acc: 72.6048% Time: 5404.3991s\n",
            "#783 Loss: 0.5475 Acc: 72.0559% Time: 5411.3700s\n",
            "#784 Loss: 0.5417 Acc: 72.4551% Time: 5418.2180s\n",
            "#785 Loss: 0.5419 Acc: 72.2056% Time: 5425.0128s\n",
            "#786 Loss: 0.5453 Acc: 71.9561% Time: 5431.9267s\n",
            "#787 Loss: 0.5467 Acc: 71.5070% Time: 5438.7044s\n",
            "#788 Loss: 0.5439 Acc: 72.1058% Time: 5445.5330s\n",
            "#789 Loss: 0.5404 Acc: 72.8044% Time: 5452.3881s\n",
            "#790 Loss: 0.5475 Acc: 71.1078% Time: 5459.3584s\n",
            "#791 Loss: 0.5427 Acc: 72.4551% Time: 5466.3601s\n",
            "#792 Loss: 0.5381 Acc: 72.7046% Time: 5473.1544s\n",
            "#793 Loss: 0.5435 Acc: 72.5050% Time: 5480.0094s\n",
            "#794 Loss: 0.5409 Acc: 72.8044% Time: 5486.9935s\n",
            "#795 Loss: 0.5305 Acc: 72.9541% Time: 5493.9916s\n",
            "#796 Loss: 0.5512 Acc: 71.8064% Time: 5500.6938s\n",
            "#797 Loss: 0.5393 Acc: 72.1058% Time: 5507.5500s\n",
            "#798 Loss: 0.5448 Acc: 72.1557% Time: 5514.5837s\n",
            "#799 Loss: 0.5451 Acc: 72.3054% Time: 5521.2882s\n",
            "#800 Loss: 0.5407 Acc: 72.4551% Time: 5528.1197s\n",
            "#801 Loss: 0.5497 Acc: 71.4072% Time: 5534.9870s\n",
            "#802 Loss: 0.5334 Acc: 73.4032% Time: 5541.8813s\n",
            "#803 Loss: 0.5445 Acc: 72.0060% Time: 5548.7928s\n",
            "#804 Loss: 0.5527 Acc: 72.1058% Time: 5555.7697s\n",
            "#805 Loss: 0.5442 Acc: 72.1557% Time: 5562.6015s\n",
            "#806 Loss: 0.5479 Acc: 73.0539% Time: 5569.3742s\n",
            "#807 Loss: 0.5438 Acc: 72.4551% Time: 5576.3539s\n",
            "#808 Loss: 0.5366 Acc: 72.5050% Time: 5583.4016s\n",
            "#809 Loss: 0.5512 Acc: 71.3074% Time: 5590.2908s\n",
            "#810 Loss: 0.5532 Acc: 72.3553% Time: 5597.1140s\n",
            "#811 Loss: 0.5353 Acc: 72.6048% Time: 5603.9935s\n",
            "#812 Loss: 0.5482 Acc: 72.0559% Time: 5610.8469s\n",
            "#813 Loss: 0.5426 Acc: 71.4571% Time: 5617.7820s\n",
            "#814 Loss: 0.5415 Acc: 72.3553% Time: 5624.7489s\n",
            "#815 Loss: 0.5481 Acc: 72.0559% Time: 5631.7275s\n",
            "#816 Loss: 0.5505 Acc: 72.1058% Time: 5638.5375s\n",
            "#817 Loss: 0.5379 Acc: 72.4052% Time: 5645.3564s\n",
            "#818 Loss: 0.5501 Acc: 70.9581% Time: 5652.2960s\n",
            "#819 Loss: 0.5389 Acc: 72.7545% Time: 5659.1117s\n",
            "#820 Loss: 0.5470 Acc: 72.1557% Time: 5665.9156s\n",
            "#821 Loss: 0.5430 Acc: 72.8543% Time: 5672.8737s\n",
            "#822 Loss: 0.5482 Acc: 71.4072% Time: 5679.7453s\n",
            "#823 Loss: 0.5369 Acc: 72.7545% Time: 5686.7051s\n",
            "#824 Loss: 0.5472 Acc: 71.3074% Time: 5693.7147s\n",
            "#825 Loss: 0.5356 Acc: 72.7545% Time: 5700.7276s\n",
            "#826 Loss: 0.5414 Acc: 72.9042% Time: 5707.6597s\n",
            "#827 Loss: 0.5392 Acc: 73.4531% Time: 5714.4821s\n",
            "#828 Loss: 0.5366 Acc: 72.1557% Time: 5721.4364s\n",
            "#829 Loss: 0.5503 Acc: 71.3074% Time: 5728.3359s\n",
            "#830 Loss: 0.5496 Acc: 72.3054% Time: 5735.2823s\n",
            "#831 Loss: 0.5353 Acc: 73.4531% Time: 5742.0827s\n",
            "#832 Loss: 0.5391 Acc: 73.0539% Time: 5749.0580s\n",
            "#833 Loss: 0.5404 Acc: 73.5529% Time: 5755.8624s\n",
            "#834 Loss: 0.5462 Acc: 71.7066% Time: 5762.6129s\n",
            "#835 Loss: 0.5591 Acc: 70.6587% Time: 5769.4629s\n",
            "#836 Loss: 0.5440 Acc: 71.8064% Time: 5776.4124s\n",
            "#837 Loss: 0.5489 Acc: 71.3573% Time: 5783.2066s\n",
            "#838 Loss: 0.5496 Acc: 71.8563% Time: 5790.0996s\n",
            "#839 Loss: 0.5438 Acc: 71.6068% Time: 5797.0780s\n",
            "#840 Loss: 0.5441 Acc: 71.0080% Time: 5803.8777s\n",
            "#841 Loss: 0.5532 Acc: 71.6068% Time: 5810.8620s\n",
            "#842 Loss: 0.5377 Acc: 72.2056% Time: 5817.7920s\n",
            "#843 Loss: 0.5541 Acc: 70.7086% Time: 5824.6940s\n",
            "#844 Loss: 0.5464 Acc: 72.2056% Time: 5831.5969s\n",
            "#845 Loss: 0.5440 Acc: 72.4052% Time: 5838.5138s\n",
            "#846 Loss: 0.5448 Acc: 72.8044% Time: 5845.4555s\n",
            "#847 Loss: 0.5415 Acc: 73.0040% Time: 5852.3225s\n",
            "#848 Loss: 0.5498 Acc: 71.6068% Time: 5859.2768s\n",
            "#849 Loss: 0.5339 Acc: 72.9541% Time: 5866.1775s\n",
            "#850 Loss: 0.5408 Acc: 72.8543% Time: 5873.0399s\n",
            "#851 Loss: 0.5421 Acc: 72.4551% Time: 5879.8742s\n",
            "#852 Loss: 0.5373 Acc: 72.8543% Time: 5886.8466s\n",
            "#853 Loss: 0.5443 Acc: 72.1557% Time: 5893.8007s\n",
            "#854 Loss: 0.5365 Acc: 72.3054% Time: 5900.6122s\n",
            "#855 Loss: 0.5438 Acc: 71.8064% Time: 5907.4593s\n",
            "#856 Loss: 0.5437 Acc: 72.0060% Time: 5914.3591s\n",
            "#857 Loss: 0.5476 Acc: 72.1058% Time: 5921.2248s\n",
            "#858 Loss: 0.5434 Acc: 71.8563% Time: 5928.0456s\n",
            "#859 Loss: 0.5290 Acc: 72.9541% Time: 5935.3053s\n",
            "#860 Loss: 0.5439 Acc: 71.4571% Time: 5942.5521s\n",
            "#861 Loss: 0.5514 Acc: 70.7585% Time: 5949.4214s\n",
            "#862 Loss: 0.5374 Acc: 72.6048% Time: 5956.3792s\n",
            "#863 Loss: 0.5420 Acc: 72.5050% Time: 5963.2133s\n",
            "#864 Loss: 0.5415 Acc: 72.3054% Time: 5970.2211s\n",
            "#865 Loss: 0.5453 Acc: 72.5549% Time: 5977.2873s\n",
            "#866 Loss: 0.5434 Acc: 72.3054% Time: 5984.2678s\n",
            "#867 Loss: 0.5345 Acc: 72.9042% Time: 5991.2765s\n",
            "#868 Loss: 0.5439 Acc: 71.6068% Time: 5998.2270s\n",
            "#869 Loss: 0.5464 Acc: 72.9042% Time: 6005.0369s\n",
            "#870 Loss: 0.5388 Acc: 73.0539% Time: 6011.9395s\n",
            "#871 Loss: 0.5459 Acc: 72.4551% Time: 6018.8363s\n",
            "#872 Loss: 0.5460 Acc: 72.3054% Time: 6025.6261s\n",
            "#873 Loss: 0.5543 Acc: 71.4571% Time: 6032.5187s\n",
            "#874 Loss: 0.5428 Acc: 73.1038% Time: 6039.3594s\n",
            "#875 Loss: 0.5518 Acc: 71.1078% Time: 6046.2001s\n",
            "#876 Loss: 0.5460 Acc: 72.5050% Time: 6053.0167s\n",
            "#877 Loss: 0.5401 Acc: 73.1537% Time: 6059.9355s\n",
            "#878 Loss: 0.5409 Acc: 72.5050% Time: 6066.7394s\n",
            "#879 Loss: 0.5471 Acc: 71.8563% Time: 6073.6286s\n",
            "#880 Loss: 0.5458 Acc: 71.3074% Time: 6080.4946s\n",
            "#881 Loss: 0.5402 Acc: 73.1537% Time: 6087.3359s\n",
            "#882 Loss: 0.5444 Acc: 72.4551% Time: 6094.1745s\n",
            "#883 Loss: 0.5504 Acc: 72.3553% Time: 6100.9210s\n",
            "#884 Loss: 0.5395 Acc: 71.3573% Time: 6107.7873s\n",
            "#885 Loss: 0.5483 Acc: 72.2555% Time: 6114.5509s\n",
            "#886 Loss: 0.5556 Acc: 71.6068% Time: 6121.3818s\n",
            "#887 Loss: 0.5382 Acc: 73.0040% Time: 6128.1263s\n",
            "#888 Loss: 0.5489 Acc: 72.4551% Time: 6134.9305s\n",
            "#889 Loss: 0.5425 Acc: 72.9541% Time: 6141.9036s\n",
            "#890 Loss: 0.5426 Acc: 72.4551% Time: 6148.6966s\n",
            "#891 Loss: 0.5452 Acc: 71.4072% Time: 6155.6067s\n",
            "#892 Loss: 0.5418 Acc: 71.9062% Time: 6162.3382s\n",
            "#893 Loss: 0.5502 Acc: 72.2555% Time: 6169.1364s\n",
            "#894 Loss: 0.5372 Acc: 72.6547% Time: 6176.1249s\n",
            "#895 Loss: 0.5405 Acc: 73.3034% Time: 6182.9587s\n",
            "#896 Loss: 0.5432 Acc: 72.6547% Time: 6189.8815s\n",
            "#897 Loss: 0.5425 Acc: 72.2555% Time: 6196.7172s\n",
            "#898 Loss: 0.5440 Acc: 72.8044% Time: 6203.5153s\n",
            "#899 Loss: 0.5428 Acc: 72.2056% Time: 6210.3256s\n",
            "#900 Loss: 0.5450 Acc: 72.3054% Time: 6217.0779s\n",
            "#901 Loss: 0.5453 Acc: 71.7565% Time: 6223.9701s\n",
            "#902 Loss: 0.5351 Acc: 73.1038% Time: 6230.9063s\n",
            "#903 Loss: 0.5470 Acc: 72.3054% Time: 6237.6919s\n",
            "#904 Loss: 0.5488 Acc: 71.2575% Time: 6244.7032s\n",
            "#905 Loss: 0.5467 Acc: 72.2056% Time: 6251.5477s\n",
            "#906 Loss: 0.5589 Acc: 70.8583% Time: 6258.3791s\n",
            "#907 Loss: 0.5461 Acc: 71.4072% Time: 6265.3020s\n",
            "#908 Loss: 0.5377 Acc: 72.8044% Time: 6272.2770s\n",
            "#909 Loss: 0.5348 Acc: 72.5050% Time: 6279.1921s\n",
            "#910 Loss: 0.5424 Acc: 72.4551% Time: 6286.0641s\n",
            "#911 Loss: 0.5484 Acc: 71.3573% Time: 6292.8408s\n",
            "#912 Loss: 0.5479 Acc: 70.4591% Time: 6299.8217s\n",
            "#913 Loss: 0.5389 Acc: 72.4052% Time: 6306.7452s\n",
            "#914 Loss: 0.5414 Acc: 72.0559% Time: 6313.6021s\n",
            "#915 Loss: 0.5467 Acc: 73.1038% Time: 6320.5119s\n",
            "#916 Loss: 0.5330 Acc: 72.4052% Time: 6327.4943s\n",
            "#917 Loss: 0.5362 Acc: 72.8044% Time: 6334.3033s\n",
            "#918 Loss: 0.5389 Acc: 72.3553% Time: 6341.2463s\n",
            "#919 Loss: 0.5371 Acc: 74.1517% Time: 6348.1263s\n",
            "#920 Loss: 0.5378 Acc: 73.0539% Time: 6355.1084s\n",
            "#921 Loss: 0.5532 Acc: 72.2056% Time: 6362.0196s\n",
            "#922 Loss: 0.5455 Acc: 72.1058% Time: 6368.8888s\n",
            "#923 Loss: 0.5498 Acc: 71.9561% Time: 6375.6344s\n",
            "#924 Loss: 0.5540 Acc: 71.4571% Time: 6382.5874s\n",
            "#925 Loss: 0.5504 Acc: 72.1058% Time: 6389.4719s\n",
            "#926 Loss: 0.5412 Acc: 73.0040% Time: 6396.3565s\n",
            "#927 Loss: 0.5503 Acc: 72.6547% Time: 6403.2513s\n",
            "#928 Loss: 0.5476 Acc: 72.3553% Time: 6410.0478s\n",
            "#929 Loss: 0.5363 Acc: 72.2555% Time: 6416.8589s\n",
            "#930 Loss: 0.5433 Acc: 72.6547% Time: 6423.8862s\n",
            "#931 Loss: 0.5469 Acc: 72.1557% Time: 6430.8530s\n",
            "#932 Loss: 0.5462 Acc: 72.7046% Time: 6437.6514s\n",
            "#933 Loss: 0.5336 Acc: 72.7046% Time: 6444.4635s\n",
            "#934 Loss: 0.5382 Acc: 72.4551% Time: 6451.2840s\n",
            "#935 Loss: 0.5452 Acc: 72.2555% Time: 6458.2269s\n",
            "#936 Loss: 0.5454 Acc: 70.6088% Time: 6465.0635s\n",
            "#937 Loss: 0.5393 Acc: 73.0539% Time: 6471.9861s\n",
            "#938 Loss: 0.5420 Acc: 71.7066% Time: 6478.8603s\n",
            "#939 Loss: 0.5433 Acc: 72.6048% Time: 6485.8167s\n",
            "#940 Loss: 0.5498 Acc: 71.8064% Time: 6492.7104s\n",
            "#941 Loss: 0.5403 Acc: 72.1557% Time: 6499.6356s\n",
            "#942 Loss: 0.5410 Acc: 73.0539% Time: 6506.4268s\n",
            "#943 Loss: 0.5508 Acc: 71.2575% Time: 6513.3695s\n",
            "#944 Loss: 0.5357 Acc: 72.8044% Time: 6520.2296s\n",
            "#945 Loss: 0.5494 Acc: 71.3074% Time: 6527.2196s\n",
            "#946 Loss: 0.5377 Acc: 72.7046% Time: 6534.0157s\n",
            "#947 Loss: 0.5510 Acc: 72.2555% Time: 6540.8167s\n",
            "#948 Loss: 0.5547 Acc: 71.3573% Time: 6547.7154s\n",
            "#949 Loss: 0.5376 Acc: 73.1537% Time: 6554.6088s\n",
            "#950 Loss: 0.5463 Acc: 71.2575% Time: 6561.6146s\n",
            "#951 Loss: 0.5382 Acc: 71.9062% Time: 6568.6046s\n",
            "#952 Loss: 0.5402 Acc: 72.5549% Time: 6575.4322s\n",
            "#953 Loss: 0.5389 Acc: 72.2555% Time: 6582.4015s\n",
            "#954 Loss: 0.5342 Acc: 72.9541% Time: 6589.1250s\n",
            "#955 Loss: 0.5418 Acc: 72.3553% Time: 6596.0062s\n",
            "#956 Loss: 0.5366 Acc: 73.3533% Time: 6602.8520s\n",
            "#957 Loss: 0.5467 Acc: 71.9561% Time: 6609.8339s\n",
            "#958 Loss: 0.5401 Acc: 72.0060% Time: 6616.7704s\n",
            "#959 Loss: 0.5581 Acc: 70.1597% Time: 6623.6443s\n",
            "#960 Loss: 0.5381 Acc: 72.6547% Time: 6630.5692s\n",
            "#961 Loss: 0.5458 Acc: 71.5569% Time: 6637.3668s\n",
            "#962 Loss: 0.5487 Acc: 72.6547% Time: 6644.2330s\n",
            "#963 Loss: 0.5498 Acc: 71.4571% Time: 6651.0706s\n",
            "#964 Loss: 0.5407 Acc: 72.5549% Time: 6657.9438s\n",
            "#965 Loss: 0.5419 Acc: 72.0559% Time: 6664.8051s\n",
            "#966 Loss: 0.5335 Acc: 73.3034% Time: 6671.7687s\n",
            "#967 Loss: 0.5475 Acc: 71.4571% Time: 6678.7290s\n",
            "#968 Loss: 0.5387 Acc: 72.4551% Time: 6685.5938s\n",
            "#969 Loss: 0.5421 Acc: 73.1537% Time: 6692.5874s\n",
            "#970 Loss: 0.5354 Acc: 73.0539% Time: 6699.5684s\n",
            "#971 Loss: 0.5434 Acc: 72.6048% Time: 6706.2997s\n",
            "#972 Loss: 0.5429 Acc: 72.1557% Time: 6713.1606s\n",
            "#973 Loss: 0.5380 Acc: 72.1557% Time: 6719.9861s\n",
            "#974 Loss: 0.5424 Acc: 72.1557% Time: 6726.9923s\n",
            "#975 Loss: 0.5413 Acc: 72.7046% Time: 6734.0028s\n",
            "#976 Loss: 0.5273 Acc: 74.5010% Time: 6740.9813s\n",
            "#977 Loss: 0.5299 Acc: 73.3533% Time: 6747.7826s\n",
            "#978 Loss: 0.5612 Acc: 70.5589% Time: 6754.5708s\n",
            "#979 Loss: 0.5411 Acc: 72.5549% Time: 6761.4127s\n",
            "#980 Loss: 0.5450 Acc: 71.8563% Time: 6768.4203s\n",
            "#981 Loss: 0.5415 Acc: 72.1557% Time: 6775.3373s\n",
            "#982 Loss: 0.5525 Acc: 72.0060% Time: 6782.2973s\n",
            "#983 Loss: 0.5549 Acc: 71.8563% Time: 6789.1218s\n",
            "#984 Loss: 0.5437 Acc: 73.3533% Time: 6795.9953s\n",
            "#985 Loss: 0.5455 Acc: 71.9062% Time: 6802.9551s\n",
            "#986 Loss: 0.5352 Acc: 73.8523% Time: 6809.7815s\n",
            "#987 Loss: 0.5368 Acc: 71.5569% Time: 6816.7418s\n",
            "#988 Loss: 0.5403 Acc: 71.5569% Time: 6823.6563s\n",
            "#989 Loss: 0.5306 Acc: 73.6028% Time: 6830.6011s\n",
            "#990 Loss: 0.5439 Acc: 72.1058% Time: 6837.5170s\n",
            "#991 Loss: 0.5511 Acc: 72.2056% Time: 6844.4674s\n",
            "#992 Loss: 0.5460 Acc: 72.8044% Time: 6851.3485s\n",
            "#993 Loss: 0.5555 Acc: 71.1577% Time: 6858.2848s\n",
            "#994 Loss: 0.5449 Acc: 71.6567% Time: 6865.1388s\n",
            "#995 Loss: 0.5598 Acc: 69.9102% Time: 6872.0990s\n",
            "#996 Loss: 0.5392 Acc: 72.7545% Time: 6879.0608s\n",
            "#997 Loss: 0.5568 Acc: 70.9581% Time: 6885.9199s\n",
            "#998 Loss: 0.5491 Acc: 71.5070% Time: 6892.8487s\n",
            "#999 Loss: 0.5465 Acc: 72.4052% Time: 6899.6943s\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1000\n",
        "# model.train()\n",
        "start_time = time.time()\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# 전체 반복(epoch) 수 만큼 반복하며\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    # 배치 단위로 학습 데이터 불러오기\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # 모델에 입력(forward)하고 결과 계산\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    # 학습 과정 중에 결과 출력\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    #save\n",
        "    torch.save(model.state_dict(), f'C:/team3/resnet/models/#22 resnet_models/resnet_dict{epoch}.pth')\n",
        "\n",
        "# writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 [Test Phase] Loss: 0.7677 Acc: 42.8242% Time: 11.8239s\n",
            "#1 [Test Phase] Loss: 0.8208 Acc: 36.7018% Time: 21.3061s\n",
            "#2 [Test Phase] Loss: 0.6857 Acc: 55.2666% Time: 30.0462s\n",
            "#3 [Test Phase] Loss: 0.7165 Acc: 56.4845% Time: 38.7664s\n",
            "#4 [Test Phase] Loss: 0.6286 Acc: 65.3720% Time: 47.5752s\n",
            "#5 [Test Phase] Loss: 0.6204 Acc: 65.9315% Time: 56.4480s\n",
            "#6 [Test Phase] Loss: 0.6083 Acc: 67.3469% Time: 65.2373s\n",
            "#7 [Test Phase] Loss: 0.7986 Acc: 49.9671% Time: 74.1928s\n",
            "#8 [Test Phase] Loss: 0.7214 Acc: 56.3199% Time: 83.2719s\n",
            "#9 [Test Phase] Loss: 0.6912 Acc: 59.0849% Time: 92.2952s\n",
            "#10 [Test Phase] Loss: 0.6781 Acc: 60.4674% Time: 101.2252s\n",
            "#11 [Test Phase] Loss: 0.7587 Acc: 53.7525% Time: 110.0499s\n",
            "#12 [Test Phase] Loss: 0.7255 Acc: 56.0237% Time: 118.8697s\n",
            "#13 [Test Phase] Loss: 0.7395 Acc: 47.5971% Time: 127.8255s\n",
            "#14 [Test Phase] Loss: 0.6458 Acc: 63.7920% Time: 136.7498s\n",
            "#15 [Test Phase] Loss: 0.6688 Acc: 60.9612% Time: 145.7922s\n",
            "#16 [Test Phase] Loss: 0.7066 Acc: 58.2620% Time: 154.5599s\n",
            "#17 [Test Phase] Loss: 0.6724 Acc: 60.9282% Time: 163.5333s\n",
            "#18 [Test Phase] Loss: 0.6899 Acc: 59.6445% Time: 172.2882s\n",
            "#19 [Test Phase] Loss: 0.6775 Acc: 60.1053% Time: 181.1799s\n",
            "#20 [Test Phase] Loss: 0.5818 Acc: 69.7169% Time: 190.0329s\n",
            "#21 [Test Phase] Loss: 0.6681 Acc: 61.1916% Time: 198.8030s\n",
            "#22 [Test Phase] Loss: 0.7098 Acc: 57.3733% Time: 207.6745s\n",
            "#23 [Test Phase] Loss: 0.7140 Acc: 57.8999% Time: 216.5601s\n",
            "#24 [Test Phase] Loss: 0.6888 Acc: 55.1679% Time: 225.5338s\n",
            "#25 [Test Phase] Loss: 0.6713 Acc: 61.7512% Time: 234.3652s\n",
            "#26 [Test Phase] Loss: 0.6261 Acc: 65.5365% Time: 243.2442s\n",
            "#27 [Test Phase] Loss: 0.6842 Acc: 60.5332% Time: 252.2000s\n",
            "#28 [Test Phase] Loss: 0.6946 Acc: 59.6445% Time: 261.0252s\n",
            "#29 [Test Phase] Loss: 0.7633 Acc: 54.2791% Time: 269.8751s\n",
            "#30 [Test Phase] Loss: 0.5785 Acc: 70.0790% Time: 278.7035s\n",
            "#31 [Test Phase] Loss: 0.8809 Acc: 45.9184% Time: 287.6801s\n",
            "#32 [Test Phase] Loss: 0.6301 Acc: 64.9111% Time: 296.5762s\n",
            "#33 [Test Phase] Loss: 0.5895 Acc: 69.4207% Time: 305.3427s\n",
            "#34 [Test Phase] Loss: 0.6885 Acc: 60.4674% Time: 314.0797s\n",
            "#35 [Test Phase] Loss: 0.7497 Acc: 47.3338% Time: 322.6815s\n",
            "#36 [Test Phase] Loss: 0.6995 Acc: 59.2824% Time: 331.2727s\n",
            "#37 [Test Phase] Loss: 0.7571 Acc: 54.4437% Time: 339.9329s\n",
            "#38 [Test Phase] Loss: 0.7379 Acc: 55.7275% Time: 348.6203s\n",
            "#39 [Test Phase] Loss: 0.6911 Acc: 60.2370% Time: 357.2201s\n",
            "#40 [Test Phase] Loss: 0.6923 Acc: 59.8749% Time: 365.7686s\n",
            "#41 [Test Phase] Loss: 0.6416 Acc: 64.3515% Time: 374.3296s\n",
            "#42 [Test Phase] Loss: 0.6498 Acc: 63.8907% Time: 382.9004s\n",
            "#43 [Test Phase] Loss: 0.6968 Acc: 59.2495% Time: 391.6251s\n",
            "#44 [Test Phase] Loss: 0.5991 Acc: 68.3344% Time: 400.1933s\n",
            "#45 [Test Phase] Loss: 0.5887 Acc: 68.7953% Time: 408.7444s\n",
            "#46 [Test Phase] Loss: 0.6327 Acc: 62.4095% Time: 417.2537s\n",
            "#47 [Test Phase] Loss: 0.7444 Acc: 55.8920% Time: 425.7251s\n",
            "#48 [Test Phase] Loss: 0.6479 Acc: 63.5616% Time: 434.2972s\n",
            "#49 [Test Phase] Loss: 0.6870 Acc: 60.6320% Time: 442.9006s\n",
            "#50 [Test Phase] Loss: 0.5825 Acc: 69.7828% Time: 451.4402s\n",
            "#51 [Test Phase] Loss: 0.6263 Acc: 65.5365% Time: 459.8591s\n",
            "#52 [Test Phase] Loss: 0.7287 Acc: 57.3404% Time: 468.3183s\n",
            "#53 [Test Phase] Loss: 0.7050 Acc: 58.6570% Time: 476.7044s\n",
            "#54 [Test Phase] Loss: 0.5840 Acc: 69.7169% Time: 485.1671s\n",
            "#55 [Test Phase] Loss: 0.6748 Acc: 62.1132% Time: 493.6714s\n",
            "#56 [Test Phase] Loss: 0.6166 Acc: 66.2607% Time: 502.2952s\n",
            "#57 [Test Phase] Loss: 0.5874 Acc: 68.3015% Time: 510.8386s\n",
            "#58 [Test Phase] Loss: 0.7380 Acc: 56.1883% Time: 519.2891s\n",
            "#59 [Test Phase] Loss: 0.8845 Acc: 47.0046% Time: 527.7900s\n",
            "#60 [Test Phase] Loss: 0.7390 Acc: 56.8795% Time: 536.2538s\n",
            "#61 [Test Phase] Loss: 0.6901 Acc: 60.5991% Time: 544.9356s\n",
            "#62 [Test Phase] Loss: 0.7588 Acc: 55.2995% Time: 553.4123s\n",
            "#63 [Test Phase] Loss: 0.5354 Acc: 73.8644% Time: 561.9936s\n",
            "#64 [Test Phase] Loss: 0.6444 Acc: 63.7920% Time: 570.3491s\n",
            "#65 [Test Phase] Loss: 0.6488 Acc: 64.1211% Time: 578.8379s\n",
            "#66 [Test Phase] Loss: 0.6606 Acc: 63.0349% Time: 587.3867s\n",
            "#67 [Test Phase] Loss: 0.8124 Acc: 51.0533% Time: 595.8373s\n",
            "#68 [Test Phase] Loss: 0.8036 Acc: 42.0342% Time: 604.4254s\n",
            "#69 [Test Phase] Loss: 0.8557 Acc: 48.8150% Time: 612.9054s\n",
            "#70 [Test Phase] Loss: 0.5691 Acc: 70.4740% Time: 621.3491s\n",
            "#71 [Test Phase] Loss: 0.6158 Acc: 67.4128% Time: 629.8103s\n",
            "#72 [Test Phase] Loss: 0.7397 Acc: 56.6491% Time: 638.4219s\n",
            "#73 [Test Phase] Loss: 0.6726 Acc: 62.3436% Time: 647.0452s\n",
            "#74 [Test Phase] Loss: 0.7293 Acc: 57.4720% Time: 655.5801s\n",
            "#75 [Test Phase] Loss: 0.7374 Acc: 56.6820% Time: 664.0386s\n",
            "#76 [Test Phase] Loss: 0.7835 Acc: 54.2791% Time: 672.6169s\n",
            "#77 [Test Phase] Loss: 0.8217 Acc: 51.2508% Time: 681.0140s\n",
            "#78 [Test Phase] Loss: 0.7252 Acc: 57.9658% Time: 689.6057s\n",
            "#79 [Test Phase] Loss: 0.8307 Acc: 40.0922% Time: 698.1483s\n",
            "#80 [Test Phase] Loss: 0.6358 Acc: 65.8657% Time: 706.6739s\n",
            "#81 [Test Phase] Loss: 0.7624 Acc: 55.1020% Time: 715.3193s\n",
            "#82 [Test Phase] Loss: 0.7482 Acc: 56.8137% Time: 723.7770s\n",
            "#83 [Test Phase] Loss: 0.6879 Acc: 61.1257% Time: 732.3749s\n",
            "#84 [Test Phase] Loss: 0.6765 Acc: 61.8828% Time: 741.0480s\n",
            "#85 [Test Phase] Loss: 0.6255 Acc: 65.8328% Time: 749.5810s\n",
            "#86 [Test Phase] Loss: 0.7651 Acc: 55.1020% Time: 757.9541s\n",
            "#87 [Test Phase] Loss: 0.7631 Acc: 55.4970% Time: 766.4979s\n",
            "#88 [Test Phase] Loss: 0.5796 Acc: 70.4082% Time: 775.0205s\n",
            "#89 [Test Phase] Loss: 0.7779 Acc: 54.1804% Time: 783.6443s\n",
            "#90 [Test Phase] Loss: 0.7885 Acc: 43.7459% Time: 792.2458s\n",
            "#91 [Test Phase] Loss: 0.6763 Acc: 62.1132% Time: 800.7834s\n",
            "#92 [Test Phase] Loss: 0.6667 Acc: 62.6399% Time: 809.3603s\n",
            "#93 [Test Phase] Loss: 0.7570 Acc: 55.8591% Time: 817.9370s\n",
            "#94 [Test Phase] Loss: 0.7266 Acc: 57.8341% Time: 826.4133s\n",
            "#95 [Test Phase] Loss: 0.6255 Acc: 66.1290% Time: 834.9055s\n",
            "#96 [Test Phase] Loss: 0.7526 Acc: 56.3529% Time: 843.3726s\n",
            "#97 [Test Phase] Loss: 0.7845 Acc: 53.4233% Time: 851.7593s\n",
            "#98 [Test Phase] Loss: 0.5440 Acc: 72.9756% Time: 860.3094s\n",
            "#99 [Test Phase] Loss: 0.5265 Acc: 74.4898% Time: 869.0714s\n",
            "#100 [Test Phase] Loss: 0.6910 Acc: 61.8170% Time: 877.5220s\n",
            "#101 [Test Phase] Loss: 0.7009 Acc: 54.2133% Time: 885.9854s\n",
            "#102 [Test Phase] Loss: 0.6270 Acc: 66.1290% Time: 894.4440s\n",
            "#103 [Test Phase] Loss: 0.8199 Acc: 51.3167% Time: 902.9338s\n",
            "#104 [Test Phase] Loss: 0.7103 Acc: 59.8091% Time: 911.5060s\n",
            "#105 [Test Phase] Loss: 0.6863 Acc: 61.7182% Time: 919.9859s\n",
            "#106 [Test Phase] Loss: 0.6185 Acc: 67.1494% Time: 928.5901s\n",
            "#107 [Test Phase] Loss: 0.6125 Acc: 67.7419% Time: 937.0891s\n",
            "#108 [Test Phase] Loss: 0.5771 Acc: 70.3094% Time: 945.5220s\n",
            "#109 [Test Phase] Loss: 0.8688 Acc: 49.5392% Time: 953.9226s\n",
            "#110 [Test Phase] Loss: 0.6464 Acc: 64.5820% Time: 962.5642s\n",
            "#111 [Test Phase] Loss: 0.5967 Acc: 68.7623% Time: 971.0273s\n",
            "#112 [Test Phase] Loss: 0.7782 Acc: 42.5609% Time: 979.5506s\n",
            "#113 [Test Phase] Loss: 0.6202 Acc: 63.3970% Time: 987.9654s\n",
            "#114 [Test Phase] Loss: 0.6134 Acc: 67.5773% Time: 996.5062s\n",
            "#115 [Test Phase] Loss: 0.6500 Acc: 64.6807% Time: 1005.0921s\n",
            "#116 [Test Phase] Loss: 0.8017 Acc: 52.4029% Time: 1013.7014s\n",
            "#117 [Test Phase] Loss: 0.6759 Acc: 62.4095% Time: 1022.3276s\n",
            "#118 [Test Phase] Loss: 0.7937 Acc: 53.8841% Time: 1030.7154s\n",
            "#119 [Test Phase] Loss: 0.7354 Acc: 58.2620% Time: 1039.3136s\n",
            "#120 [Test Phase] Loss: 0.6769 Acc: 62.3766% Time: 1047.8567s\n",
            "#121 [Test Phase] Loss: 0.7272 Acc: 58.3608% Time: 1056.4116s\n",
            "#122 [Test Phase] Loss: 0.7709 Acc: 55.2337% Time: 1064.8799s\n",
            "#123 [Test Phase] Loss: 0.6435 Acc: 65.4707% Time: 1073.3263s\n",
            "#124 [Test Phase] Loss: 0.6307 Acc: 63.2982% Time: 1081.8375s\n",
            "#125 [Test Phase] Loss: 0.6596 Acc: 64.2528% Time: 1090.2839s\n",
            "#126 [Test Phase] Loss: 0.6232 Acc: 66.9849% Time: 1098.5532s\n",
            "#127 [Test Phase] Loss: 0.6325 Acc: 66.2607% Time: 1106.9729s\n",
            "#128 [Test Phase] Loss: 0.6945 Acc: 60.8295% Time: 1115.4571s\n",
            "#129 [Test Phase] Loss: 0.7490 Acc: 56.7149% Time: 1123.9519s\n",
            "#130 [Test Phase] Loss: 0.8641 Acc: 49.3417% Time: 1132.4731s\n",
            "#131 [Test Phase] Loss: 0.6787 Acc: 62.6728% Time: 1141.0338s\n",
            "#132 [Test Phase] Loss: 0.6385 Acc: 65.6353% Time: 1149.6208s\n",
            "#133 [Test Phase] Loss: 0.7927 Acc: 54.3120% Time: 1158.0888s\n",
            "#134 [Test Phase] Loss: 0.7394 Acc: 57.8341% Time: 1166.6116s\n",
            "#135 [Test Phase] Loss: 0.7556 Acc: 48.5846% Time: 1175.0090s\n",
            "#136 [Test Phase] Loss: 0.7086 Acc: 59.8420% Time: 1183.5219s\n",
            "#137 [Test Phase] Loss: 0.6663 Acc: 63.5616% Time: 1191.9780s\n",
            "#138 [Test Phase] Loss: 0.6863 Acc: 61.8499% Time: 1200.5248s\n",
            "#139 [Test Phase] Loss: 0.6707 Acc: 63.1995% Time: 1209.0354s\n",
            "#140 [Test Phase] Loss: 0.7360 Acc: 58.2620% Time: 1217.4912s\n",
            "#141 [Test Phase] Loss: 0.5623 Acc: 71.4615% Time: 1225.9536s\n",
            "#142 [Test Phase] Loss: 0.6332 Acc: 66.0632% Time: 1234.3760s\n",
            "#143 [Test Phase] Loss: 0.6210 Acc: 67.4457% Time: 1242.9351s\n",
            "#144 [Test Phase] Loss: 0.7427 Acc: 57.9328% Time: 1251.4617s\n",
            "#145 [Test Phase] Loss: 0.6480 Acc: 65.2732% Time: 1259.9845s\n",
            "#146 [Test Phase] Loss: 0.7789 Acc: 44.9638% Time: 1268.3687s\n",
            "#147 [Test Phase] Loss: 0.5752 Acc: 70.9348% Time: 1276.8661s\n",
            "#148 [Test Phase] Loss: 0.6718 Acc: 63.5616% Time: 1285.3945s\n",
            "#149 [Test Phase] Loss: 0.6079 Acc: 68.5978% Time: 1293.7822s\n",
            "#150 [Test Phase] Loss: 0.7563 Acc: 57.0770% Time: 1302.1643s\n",
            "#151 [Test Phase] Loss: 0.6998 Acc: 61.5866% Time: 1310.4514s\n",
            "#152 [Test Phase] Loss: 0.7064 Acc: 60.7307% Time: 1319.0512s\n",
            "#153 [Test Phase] Loss: 0.8538 Acc: 50.0329% Time: 1327.5560s\n",
            "#154 [Test Phase] Loss: 0.6001 Acc: 68.9269% Time: 1336.1340s\n",
            "#155 [Test Phase] Loss: 0.7167 Acc: 59.3812% Time: 1344.7056s\n",
            "#156 [Test Phase] Loss: 0.6659 Acc: 63.3641% Time: 1353.1442s\n",
            "#157 [Test Phase] Loss: 0.7792 Acc: 45.0296% Time: 1361.5760s\n",
            "#158 [Test Phase] Loss: 0.6089 Acc: 68.7294% Time: 1369.9770s\n",
            "#159 [Test Phase] Loss: 0.6836 Acc: 61.9487% Time: 1378.3815s\n",
            "#160 [Test Phase] Loss: 0.7277 Acc: 58.6570% Time: 1386.7484s\n",
            "#161 [Test Phase] Loss: 0.6283 Acc: 66.6886% Time: 1395.1720s\n",
            "#162 [Test Phase] Loss: 0.5554 Acc: 71.4615% Time: 1403.7865s\n",
            "#163 [Test Phase] Loss: 0.6187 Acc: 67.8078% Time: 1412.3901s\n",
            "#164 [Test Phase] Loss: 0.6980 Acc: 61.2903% Time: 1420.8862s\n",
            "#165 [Test Phase] Loss: 0.6382 Acc: 65.9315% Time: 1429.3418s\n",
            "#166 [Test Phase] Loss: 0.7550 Acc: 56.9124% Time: 1437.8326s\n",
            "#167 [Test Phase] Loss: 0.7695 Acc: 55.9579% Time: 1446.3214s\n",
            "#168 [Test Phase] Loss: 0.5264 Acc: 75.6419% Time: 1454.7657s\n",
            "#169 [Test Phase] Loss: 0.7517 Acc: 57.5379% Time: 1463.1478s\n",
            "#170 [Test Phase] Loss: 0.7898 Acc: 54.8387% Time: 1471.6075s\n",
            "#171 [Test Phase] Loss: 0.7279 Acc: 58.8874% Time: 1480.2241s\n",
            "#172 [Test Phase] Loss: 0.6687 Acc: 63.2653% Time: 1488.7544s\n",
            "#173 [Test Phase] Loss: 0.6482 Acc: 65.3390% Time: 1497.3112s\n",
            "#174 [Test Phase] Loss: 0.7552 Acc: 57.4720% Time: 1505.7826s\n",
            "#175 [Test Phase] Loss: 0.6961 Acc: 61.2245% Time: 1514.1834s\n",
            "#176 [Test Phase] Loss: 0.6156 Acc: 67.4128% Time: 1522.5345s\n",
            "#177 [Test Phase] Loss: 0.7153 Acc: 59.9078% Time: 1530.9999s\n",
            "#178 [Test Phase] Loss: 0.6397 Acc: 65.8328% Time: 1539.4714s\n",
            "#179 [Test Phase] Loss: 0.5543 Acc: 72.0540% Time: 1547.9401s\n",
            "#180 [Test Phase] Loss: 0.6709 Acc: 63.9895% Time: 1556.4808s\n",
            "#181 [Test Phase] Loss: 0.6037 Acc: 68.7953% Time: 1564.9719s\n",
            "#182 [Test Phase] Loss: 0.7647 Acc: 56.4516% Time: 1573.5028s\n",
            "#183 [Test Phase] Loss: 0.6393 Acc: 65.7999% Time: 1581.9887s\n",
            "#184 [Test Phase] Loss: 0.6753 Acc: 62.8374% Time: 1590.5714s\n",
            "#185 [Test Phase] Loss: 0.8862 Acc: 48.5846% Time: 1599.1141s\n",
            "#186 [Test Phase] Loss: 0.6460 Acc: 65.5695% Time: 1607.7046s\n",
            "#187 [Test Phase] Loss: 0.5332 Acc: 73.6998% Time: 1616.1633s\n",
            "#188 [Test Phase] Loss: 0.6268 Acc: 66.3265% Time: 1624.5275s\n",
            "#189 [Test Phase] Loss: 0.6504 Acc: 65.3061% Time: 1632.8344s\n",
            "#190 [Test Phase] Loss: 0.6211 Acc: 64.3186% Time: 1641.2860s\n",
            "#191 [Test Phase] Loss: 0.7262 Acc: 59.1178% Time: 1649.7331s\n",
            "#192 [Test Phase] Loss: 0.7993 Acc: 54.3779% Time: 1658.2448s\n",
            "#193 [Test Phase] Loss: 0.7034 Acc: 60.5662% Time: 1666.7196s\n",
            "#194 [Test Phase] Loss: 0.8383 Acc: 52.2712% Time: 1675.1442s\n",
            "#195 [Test Phase] Loss: 0.5796 Acc: 70.4740% Time: 1683.7215s\n",
            "#196 [Test Phase] Loss: 0.6385 Acc: 65.9644% Time: 1692.1977s\n",
            "#197 [Test Phase] Loss: 0.6995 Acc: 61.6524% Time: 1700.5918s\n",
            "#198 [Test Phase] Loss: 0.8027 Acc: 54.4437% Time: 1709.1136s\n",
            "#199 [Test Phase] Loss: 0.7781 Acc: 55.4641% Time: 1717.7159s\n",
            "#200 [Test Phase] Loss: 0.7164 Acc: 59.9737% Time: 1726.2509s\n",
            "#201 [Test Phase] Loss: 0.6470 Acc: 60.5662% Time: 1734.7806s\n",
            "#202 [Test Phase] Loss: 0.5896 Acc: 69.8815% Time: 1743.2759s\n",
            "#203 [Test Phase] Loss: 0.7358 Acc: 58.6570% Time: 1751.9042s\n",
            "#204 [Test Phase] Loss: 0.7396 Acc: 58.2620% Time: 1760.4551s\n",
            "#205 [Test Phase] Loss: 0.7052 Acc: 61.3891% Time: 1768.9988s\n",
            "#206 [Test Phase] Loss: 0.7398 Acc: 58.4595% Time: 1777.4184s\n",
            "#207 [Test Phase] Loss: 0.6741 Acc: 63.3311% Time: 1785.8590s\n",
            "#208 [Test Phase] Loss: 0.6426 Acc: 65.8986% Time: 1794.4064s\n",
            "#209 [Test Phase] Loss: 0.7877 Acc: 55.1679% Time: 1802.8372s\n",
            "#210 [Test Phase] Loss: 0.7175 Acc: 59.5457% Time: 1811.3711s\n",
            "#211 [Test Phase] Loss: 0.6123 Acc: 67.9724% Time: 1819.9299s\n",
            "#212 [Test Phase] Loss: 0.7967 Acc: 44.3713% Time: 1828.4308s\n",
            "#213 [Test Phase] Loss: 0.7465 Acc: 58.2291% Time: 1837.0707s\n",
            "#214 [Test Phase] Loss: 0.7201 Acc: 60.3028% Time: 1845.5968s\n",
            "#215 [Test Phase] Loss: 0.6023 Acc: 69.1244% Time: 1853.9947s\n",
            "#216 [Test Phase] Loss: 0.6973 Acc: 61.6853% Time: 1862.4216s\n",
            "#217 [Test Phase] Loss: 0.6127 Acc: 67.3140% Time: 1870.7741s\n",
            "#218 [Test Phase] Loss: 0.6801 Acc: 62.9361% Time: 1879.2824s\n",
            "#219 [Test Phase] Loss: 0.7380 Acc: 58.5253% Time: 1887.7177s\n",
            "#220 [Test Phase] Loss: 0.8107 Acc: 54.0816% Time: 1896.2558s\n",
            "#221 [Test Phase] Loss: 0.7806 Acc: 55.5629% Time: 1904.7620s\n",
            "#222 [Test Phase] Loss: 0.6819 Acc: 62.8045% Time: 1913.1613s\n",
            "#223 [Test Phase] Loss: 0.6400 Acc: 62.0145% Time: 1921.5406s\n",
            "#224 [Test Phase] Loss: 0.7420 Acc: 50.2633% Time: 1929.8783s\n",
            "#225 [Test Phase] Loss: 0.6905 Acc: 62.0803% Time: 1938.3868s\n",
            "#226 [Test Phase] Loss: 0.6612 Acc: 64.2528% Time: 1946.9318s\n",
            "#227 [Test Phase] Loss: 0.7760 Acc: 56.2541% Time: 1955.4656s\n",
            "#228 [Test Phase] Loss: 0.6982 Acc: 61.4878% Time: 1963.8511s\n",
            "#229 [Test Phase] Loss: 0.6858 Acc: 62.6728% Time: 1972.3167s\n",
            "#230 [Test Phase] Loss: 0.6201 Acc: 68.0711% Time: 1980.7665s\n",
            "#231 [Test Phase] Loss: 0.7263 Acc: 60.1382% Time: 1989.2110s\n",
            "#232 [Test Phase] Loss: 0.7078 Acc: 61.4549% Time: 1997.7221s\n",
            "#233 [Test Phase] Loss: 0.6050 Acc: 68.4661% Time: 2006.1975s\n",
            "#234 [Test Phase] Loss: 0.8010 Acc: 54.6083% Time: 2014.7393s\n",
            "#235 [Test Phase] Loss: 0.7622 Acc: 48.5188% Time: 2023.2608s\n",
            "#236 [Test Phase] Loss: 0.5778 Acc: 70.2765% Time: 2031.7102s\n",
            "#237 [Test Phase] Loss: 0.6892 Acc: 62.2778% Time: 2040.2340s\n",
            "#238 [Test Phase] Loss: 0.6629 Acc: 64.2528% Time: 2048.7213s\n",
            "#239 [Test Phase] Loss: 0.6541 Acc: 64.9111% Time: 2057.0959s\n",
            "#240 [Test Phase] Loss: 0.6740 Acc: 63.1995% Time: 2065.5724s\n",
            "#241 [Test Phase] Loss: 0.5956 Acc: 68.8611% Time: 2073.9542s\n",
            "#242 [Test Phase] Loss: 0.6818 Acc: 62.5741% Time: 2082.2731s\n",
            "#243 [Test Phase] Loss: 0.7572 Acc: 58.1633% Time: 2090.7280s\n",
            "#244 [Test Phase] Loss: 0.8520 Acc: 51.1192% Time: 2099.2443s\n",
            "#245 [Test Phase] Loss: 0.6878 Acc: 62.2778% Time: 2107.7178s\n",
            "#246 [Test Phase] Loss: 0.7029 Acc: 54.3779% Time: 2116.3240s\n",
            "#247 [Test Phase] Loss: 0.8147 Acc: 54.0487% Time: 2124.7936s\n",
            "#248 [Test Phase] Loss: 0.6775 Acc: 63.6603% Time: 2133.3388s\n",
            "#249 [Test Phase] Loss: 0.5920 Acc: 69.6511% Time: 2141.7497s\n",
            "#250 [Test Phase] Loss: 0.6560 Acc: 65.4707% Time: 2150.0897s\n",
            "#251 [Test Phase] Loss: 0.7906 Acc: 55.2008% Time: 2158.4887s\n",
            "#252 [Test Phase] Loss: 0.7690 Acc: 57.4720% Time: 2167.1907s\n",
            "#253 [Test Phase] Loss: 0.6812 Acc: 63.2982% Time: 2175.8654s\n",
            "#254 [Test Phase] Loss: 0.7075 Acc: 61.5207% Time: 2184.2844s\n",
            "#255 [Test Phase] Loss: 0.6567 Acc: 65.0099% Time: 2192.7757s\n",
            "#256 [Test Phase] Loss: 0.6872 Acc: 63.2982% Time: 2201.3174s\n",
            "#257 [Test Phase] Loss: 0.6397 Acc: 62.2449% Time: 2209.9232s\n",
            "#258 [Test Phase] Loss: 0.6931 Acc: 62.5411% Time: 2218.4080s\n",
            "#259 [Test Phase] Loss: 0.7932 Acc: 55.2666% Time: 2226.8836s\n",
            "#260 [Test Phase] Loss: 0.7557 Acc: 57.9987% Time: 2235.3453s\n",
            "#261 [Test Phase] Loss: 0.6979 Acc: 61.6524% Time: 2243.6818s\n",
            "#262 [Test Phase] Loss: 0.8003 Acc: 55.2995% Time: 2252.3138s\n",
            "#263 [Test Phase] Loss: 0.6780 Acc: 63.6274% Time: 2260.8717s\n",
            "#264 [Test Phase] Loss: 0.7177 Acc: 60.5991% Time: 2269.5677s\n",
            "#265 [Test Phase] Loss: 0.6325 Acc: 66.4911% Time: 2278.1416s\n",
            "#266 [Test Phase] Loss: 0.7265 Acc: 60.3028% Time: 2286.7112s\n",
            "#267 [Test Phase] Loss: 0.6865 Acc: 63.5286% Time: 2295.1495s\n",
            "#268 [Test Phase] Loss: 0.5469 Acc: 73.1402% Time: 2303.7215s\n",
            "#269 [Test Phase] Loss: 0.7076 Acc: 62.2449% Time: 2312.1806s\n",
            "#270 [Test Phase] Loss: 0.7003 Acc: 62.3107% Time: 2320.7297s\n",
            "#271 [Test Phase] Loss: 0.7978 Acc: 54.9704% Time: 2329.2193s\n",
            "#272 [Test Phase] Loss: 0.8202 Acc: 53.7196% Time: 2337.6686s\n",
            "#273 [Test Phase] Loss: 0.6805 Acc: 63.5945% Time: 2346.0767s\n",
            "#274 [Test Phase] Loss: 0.6272 Acc: 67.0836% Time: 2354.5036s\n",
            "#275 [Test Phase] Loss: 0.9108 Acc: 47.5642% Time: 2362.9799s\n",
            "#276 [Test Phase] Loss: 0.7612 Acc: 57.4720% Time: 2371.4175s\n",
            "#277 [Test Phase] Loss: 0.8498 Acc: 51.3167% Time: 2379.8192s\n",
            "#278 [Test Phase] Loss: 0.7126 Acc: 61.5207% Time: 2388.2152s\n",
            "#279 [Test Phase] Loss: 0.7822 Acc: 46.7413% Time: 2396.6985s\n",
            "#280 [Test Phase] Loss: 0.6882 Acc: 63.0349% Time: 2405.1929s\n",
            "#281 [Test Phase] Loss: 0.7463 Acc: 58.8874% Time: 2413.8811s\n",
            "#282 [Test Phase] Loss: 0.7046 Acc: 61.7512% Time: 2422.5198s\n",
            "#283 [Test Phase] Loss: 0.6962 Acc: 62.4424% Time: 2431.0757s\n",
            "#284 [Test Phase] Loss: 0.7927 Acc: 56.1883% Time: 2439.4692s\n",
            "#285 [Test Phase] Loss: 0.6100 Acc: 68.1369% Time: 2447.9163s\n",
            "#286 [Test Phase] Loss: 0.7651 Acc: 57.2416% Time: 2456.4368s\n",
            "#287 [Test Phase] Loss: 0.6419 Acc: 65.9315% Time: 2464.9598s\n",
            "#288 [Test Phase] Loss: 0.7595 Acc: 58.3278% Time: 2473.3550s\n",
            "#289 [Test Phase] Loss: 0.6751 Acc: 63.9895% Time: 2481.8055s\n",
            "#290 [Test Phase] Loss: 0.6244 Acc: 63.4299% Time: 2490.2316s\n",
            "#291 [Test Phase] Loss: 0.6828 Acc: 63.3311% Time: 2498.7542s\n",
            "#292 [Test Phase] Loss: 0.9540 Acc: 45.6221% Time: 2507.2167s\n",
            "#293 [Test Phase] Loss: 0.7581 Acc: 58.3608% Time: 2515.6020s\n",
            "#294 [Test Phase] Loss: 0.7348 Acc: 60.1382% Time: 2524.0506s\n",
            "#295 [Test Phase] Loss: 0.7302 Acc: 60.4016% Time: 2532.3914s\n",
            "#296 [Test Phase] Loss: 0.6432 Acc: 66.4911% Time: 2540.7556s\n",
            "#297 [Test Phase] Loss: 0.7691 Acc: 57.5049% Time: 2549.2625s\n",
            "#298 [Test Phase] Loss: 0.8209 Acc: 54.1475% Time: 2557.7236s\n",
            "#299 [Test Phase] Loss: 0.7356 Acc: 59.2495% Time: 2566.1544s\n",
            "#300 [Test Phase] Loss: 0.8219 Acc: 53.9170% Time: 2574.6775s\n",
            "#301 [Test Phase] Loss: 0.7452 Acc: 50.7571% Time: 2583.0556s\n",
            "#302 [Test Phase] Loss: 0.5905 Acc: 70.3094% Time: 2591.6029s\n",
            "#303 [Test Phase] Loss: 0.7647 Acc: 57.8999% Time: 2600.0639s\n",
            "#304 [Test Phase] Loss: 0.6728 Acc: 64.1540% Time: 2608.5238s\n",
            "#305 [Test Phase] Loss: 0.6159 Acc: 68.1369% Time: 2616.9885s\n",
            "#306 [Test Phase] Loss: 0.7611 Acc: 58.0974% Time: 2625.5442s\n",
            "#307 [Test Phase] Loss: 0.7271 Acc: 60.3028% Time: 2633.9970s\n",
            "#308 [Test Phase] Loss: 0.7702 Acc: 58.3278% Time: 2642.3852s\n",
            "#309 [Test Phase] Loss: 0.8982 Acc: 48.4529% Time: 2650.9469s\n",
            "#310 [Test Phase] Loss: 0.7980 Acc: 55.1679% Time: 2659.2338s\n",
            "#311 [Test Phase] Loss: 0.6989 Acc: 62.3436% Time: 2667.6671s\n",
            "#312 [Test Phase] Loss: 0.7089 Acc: 54.5425% Time: 2676.0598s\n",
            "#313 [Test Phase] Loss: 0.6242 Acc: 67.1165% Time: 2684.6090s\n",
            "#314 [Test Phase] Loss: 0.6244 Acc: 67.4457% Time: 2693.1842s\n",
            "#315 [Test Phase] Loss: 0.7287 Acc: 60.5991% Time: 2701.5793s\n",
            "#316 [Test Phase] Loss: 0.6668 Acc: 64.5490% Time: 2710.0763s\n",
            "#317 [Test Phase] Loss: 0.7011 Acc: 62.2120% Time: 2718.5740s\n",
            "#318 [Test Phase] Loss: 0.6803 Acc: 63.8578% Time: 2727.1000s\n",
            "#319 [Test Phase] Loss: 0.6966 Acc: 62.3436% Time: 2735.5429s\n",
            "#320 [Test Phase] Loss: 0.8192 Acc: 54.0487% Time: 2743.9408s\n",
            "#321 [Test Phase] Loss: 0.8098 Acc: 54.9045% Time: 2752.3138s\n",
            "#322 [Test Phase] Loss: 0.7403 Acc: 59.3812% Time: 2760.7521s\n",
            "#323 [Test Phase] Loss: 0.6728 Acc: 57.6037% Time: 2769.1394s\n",
            "#324 [Test Phase] Loss: 0.7856 Acc: 56.3858% Time: 2777.5068s\n",
            "#325 [Test Phase] Loss: 0.6574 Acc: 65.1086% Time: 2785.9793s\n",
            "#326 [Test Phase] Loss: 0.6425 Acc: 66.4582% Time: 2794.5210s\n",
            "#327 [Test Phase] Loss: 0.6762 Acc: 64.3515% Time: 2802.9519s\n",
            "#328 [Test Phase] Loss: 0.7357 Acc: 59.2495% Time: 2811.3750s\n",
            "#329 [Test Phase] Loss: 0.8924 Acc: 49.5721% Time: 2819.8209s\n",
            "#330 [Test Phase] Loss: 0.6338 Acc: 66.4911% Time: 2828.3013s\n",
            "#331 [Test Phase] Loss: 0.8196 Acc: 54.2791% Time: 2836.6872s\n",
            "#332 [Test Phase] Loss: 0.7262 Acc: 60.7966% Time: 2845.1780s\n",
            "#333 [Test Phase] Loss: 0.8641 Acc: 51.5142% Time: 2853.6001s\n",
            "#334 [Test Phase] Loss: 0.6420 Acc: 62.0145% Time: 2861.9803s\n",
            "#335 [Test Phase] Loss: 0.6052 Acc: 66.6228% Time: 2870.4513s\n",
            "#336 [Test Phase] Loss: 0.8487 Acc: 52.3700% Time: 2878.8338s\n",
            "#337 [Test Phase] Loss: 0.7942 Acc: 56.3858% Time: 2887.3367s\n",
            "#338 [Test Phase] Loss: 0.7330 Acc: 59.7762% Time: 2895.8849s\n",
            "#339 [Test Phase] Loss: 0.7538 Acc: 58.3937% Time: 2904.3766s\n",
            "#340 [Test Phase] Loss: 0.7062 Acc: 62.3107% Time: 2912.7250s\n",
            "#341 [Test Phase] Loss: 0.6623 Acc: 65.1745% Time: 2921.3344s\n",
            "#342 [Test Phase] Loss: 0.6840 Acc: 63.1336% Time: 2929.9099s\n",
            "#343 [Test Phase] Loss: 0.7363 Acc: 59.7103% Time: 2938.4442s\n",
            "#344 [Test Phase] Loss: 0.6137 Acc: 68.2686% Time: 2946.8443s\n",
            "#345 [Test Phase] Loss: 0.6627 Acc: 64.4174% Time: 2955.3441s\n",
            "#346 [Test Phase] Loss: 0.6879 Acc: 56.9454% Time: 2963.8372s\n",
            "#347 [Test Phase] Loss: 0.7369 Acc: 59.1837% Time: 2972.4836s\n",
            "#348 [Test Phase] Loss: 0.6588 Acc: 65.2732% Time: 2981.0608s\n",
            "#349 [Test Phase] Loss: 0.7171 Acc: 61.1916% Time: 2989.4565s\n",
            "#350 [Test Phase] Loss: 0.7372 Acc: 59.8749% Time: 2997.8864s\n",
            "#351 [Test Phase] Loss: 0.6777 Acc: 63.8578% Time: 3006.1708s\n",
            "#352 [Test Phase] Loss: 0.8300 Acc: 53.7854% Time: 3014.5463s\n",
            "#353 [Test Phase] Loss: 0.7709 Acc: 57.2416% Time: 3023.1169s\n",
            "#354 [Test Phase] Loss: 0.8680 Acc: 51.4812% Time: 3031.6483s\n",
            "#355 [Test Phase] Loss: 0.7092 Acc: 61.8828% Time: 3040.0456s\n",
            "#356 [Test Phase] Loss: 0.8233 Acc: 54.2133% Time: 3048.4907s\n",
            "#357 [Test Phase] Loss: 0.8157 Acc: 44.8321% Time: 3056.9695s\n",
            "#358 [Test Phase] Loss: 0.7869 Acc: 56.2212% Time: 3065.3573s\n",
            "#359 [Test Phase] Loss: 0.6970 Acc: 62.5741% Time: 3073.9933s\n",
            "#360 [Test Phase] Loss: 0.7179 Acc: 61.2574% Time: 3082.4698s\n",
            "#361 [Test Phase] Loss: 0.6783 Acc: 63.9895% Time: 3091.0479s\n",
            "#362 [Test Phase] Loss: 0.6922 Acc: 63.2653% Time: 3099.4682s\n",
            "#363 [Test Phase] Loss: 0.9431 Acc: 46.0500% Time: 3107.8961s\n",
            "#364 [Test Phase] Loss: 0.8955 Acc: 50.2633% Time: 3116.3622s\n",
            "#365 [Test Phase] Loss: 0.8427 Acc: 52.8966% Time: 3124.7535s\n",
            "#366 [Test Phase] Loss: 0.7841 Acc: 56.7149% Time: 3133.1740s\n",
            "#367 [Test Phase] Loss: 0.7372 Acc: 59.8749% Time: 3141.7441s\n",
            "#368 [Test Phase] Loss: 0.8234 Acc: 44.4700% Time: 3150.1536s\n",
            "#369 [Test Phase] Loss: 0.6087 Acc: 69.0257% Time: 3158.6146s\n",
            "#370 [Test Phase] Loss: 0.7731 Acc: 57.6695% Time: 3167.0022s\n",
            "#371 [Test Phase] Loss: 0.6991 Acc: 62.3766% Time: 3175.3343s\n",
            "#372 [Test Phase] Loss: 0.7301 Acc: 60.4016% Time: 3183.8719s\n",
            "#373 [Test Phase] Loss: 0.7068 Acc: 61.7512% Time: 3192.3569s\n",
            "#374 [Test Phase] Loss: 0.8482 Acc: 51.9750% Time: 3200.8000s\n",
            "#375 [Test Phase] Loss: 0.6992 Acc: 62.0145% Time: 3209.2403s\n",
            "#376 [Test Phase] Loss: 0.9198 Acc: 47.7617% Time: 3217.8879s\n",
            "#377 [Test Phase] Loss: 0.7195 Acc: 60.8953% Time: 3226.3772s\n",
            "#378 [Test Phase] Loss: 0.6949 Acc: 63.0678% Time: 3234.7557s\n",
            "#379 [Test Phase] Loss: 0.5271 Acc: 74.8190% Time: 3243.1752s\n",
            "#380 [Test Phase] Loss: 0.6488 Acc: 65.7669% Time: 3251.5402s\n",
            "#381 [Test Phase] Loss: 0.7236 Acc: 60.7966% Time: 3259.7882s\n",
            "#382 [Test Phase] Loss: 0.8476 Acc: 52.2054% Time: 3268.1052s\n",
            "#383 [Test Phase] Loss: 0.7986 Acc: 56.1224% Time: 3276.5940s\n",
            "#384 [Test Phase] Loss: 0.7393 Acc: 59.4470% Time: 3284.8355s\n",
            "#385 [Test Phase] Loss: 0.7352 Acc: 60.5003% Time: 3293.1702s\n",
            "#386 [Test Phase] Loss: 0.7797 Acc: 57.1758% Time: 3301.6312s\n",
            "#387 [Test Phase] Loss: 0.6172 Acc: 67.6103% Time: 3310.1316s\n",
            "#388 [Test Phase] Loss: 0.8176 Acc: 54.6083% Time: 3318.8113s\n",
            "#389 [Test Phase] Loss: 0.7256 Acc: 60.5662% Time: 3327.2824s\n",
            "#390 [Test Phase] Loss: 0.6683 Acc: 59.2824% Time: 3335.7110s\n",
            "#391 [Test Phase] Loss: 0.7371 Acc: 60.0395% Time: 3344.1430s\n",
            "#392 [Test Phase] Loss: 0.7684 Acc: 57.9328% Time: 3352.5546s\n",
            "#393 [Test Phase] Loss: 0.7008 Acc: 62.5411% Time: 3360.8515s\n",
            "#394 [Test Phase] Loss: 0.7453 Acc: 59.4799% Time: 3369.3423s\n",
            "#395 [Test Phase] Loss: 0.7186 Acc: 61.3232% Time: 3377.8437s\n",
            "#396 [Test Phase] Loss: 0.6467 Acc: 65.8328% Time: 3386.3269s\n",
            "#397 [Test Phase] Loss: 0.7708 Acc: 58.2620% Time: 3394.7004s\n",
            "#398 [Test Phase] Loss: 0.5736 Acc: 70.9019% Time: 3403.4132s\n",
            "#399 [Test Phase] Loss: 0.6272 Acc: 67.2811% Time: 3411.9199s\n",
            "#400 [Test Phase] Loss: 0.7012 Acc: 62.0803% Time: 3420.3671s\n",
            "#401 [Test Phase] Loss: 0.6129 Acc: 65.5036% Time: 3428.8425s\n",
            "#402 [Test Phase] Loss: 0.6888 Acc: 63.7261% Time: 3437.3971s\n",
            "#403 [Test Phase] Loss: 0.6792 Acc: 64.0224% Time: 3445.9410s\n",
            "#404 [Test Phase] Loss: 0.7177 Acc: 61.5207% Time: 3454.4885s\n",
            "#405 [Test Phase] Loss: 0.8633 Acc: 51.5142% Time: 3462.8624s\n",
            "#406 [Test Phase] Loss: 0.6953 Acc: 63.3641% Time: 3471.2681s\n",
            "#407 [Test Phase] Loss: 0.8990 Acc: 50.2962% Time: 3479.7366s\n",
            "#408 [Test Phase] Loss: 0.8397 Acc: 53.9500% Time: 3488.2712s\n",
            "#409 [Test Phase] Loss: 0.7339 Acc: 60.6978% Time: 3496.8015s\n",
            "#410 [Test Phase] Loss: 0.7362 Acc: 60.2370% Time: 3505.1890s\n",
            "#411 [Test Phase] Loss: 0.6475 Acc: 65.7011% Time: 3513.6518s\n",
            "#412 [Test Phase] Loss: 0.6771 Acc: 58.8874% Time: 3522.1296s\n",
            "#413 [Test Phase] Loss: 0.6765 Acc: 63.9895% Time: 3530.6571s\n",
            "#414 [Test Phase] Loss: 0.7015 Acc: 62.9361% Time: 3539.1327s\n",
            "#415 [Test Phase] Loss: 0.7458 Acc: 59.7762% Time: 3547.6562s\n",
            "#416 [Test Phase] Loss: 0.7945 Acc: 56.9454% Time: 3556.0634s\n",
            "#417 [Test Phase] Loss: 0.7237 Acc: 60.8295% Time: 3564.3411s\n",
            "#418 [Test Phase] Loss: 0.7419 Acc: 60.4345% Time: 3572.8710s\n",
            "#419 [Test Phase] Loss: 0.6506 Acc: 65.6353% Time: 3581.2212s\n",
            "#420 [Test Phase] Loss: 0.8378 Acc: 53.7854% Time: 3589.6492s\n",
            "#421 [Test Phase] Loss: 0.7315 Acc: 60.6649% Time: 3598.0747s\n",
            "#422 [Test Phase] Loss: 0.6898 Acc: 63.0349% Time: 3606.5842s\n",
            "#423 [Test Phase] Loss: 0.7364 Acc: 52.4687% Time: 3615.1040s\n",
            "#424 [Test Phase] Loss: 0.7299 Acc: 60.7966% Time: 3623.6682s\n",
            "#425 [Test Phase] Loss: 0.7055 Acc: 62.5741% Time: 3632.0583s\n",
            "#426 [Test Phase] Loss: 0.6554 Acc: 65.9315% Time: 3640.5049s\n",
            "#427 [Test Phase] Loss: 0.7584 Acc: 58.8545% Time: 3649.0279s\n",
            "#428 [Test Phase] Loss: 0.6702 Acc: 64.7465% Time: 3657.4202s\n",
            "#429 [Test Phase] Loss: 0.6851 Acc: 63.4299% Time: 3665.9689s\n",
            "#430 [Test Phase] Loss: 0.7009 Acc: 62.7057% Time: 3674.4704s\n",
            "#431 [Test Phase] Loss: 0.7248 Acc: 61.0270% Time: 3683.0426s\n",
            "#432 [Test Phase] Loss: 0.7403 Acc: 59.9408% Time: 3691.7352s\n",
            "#433 [Test Phase] Loss: 0.8746 Acc: 50.8558% Time: 3700.1751s\n",
            "#434 [Test Phase] Loss: 0.6986 Acc: 56.9124% Time: 3708.6462s\n",
            "#435 [Test Phase] Loss: 0.7298 Acc: 60.7307% Time: 3717.1124s\n",
            "#436 [Test Phase] Loss: 0.7494 Acc: 60.0395% Time: 3725.6354s\n",
            "#437 [Test Phase] Loss: 0.7221 Acc: 60.9612% Time: 3734.0009s\n",
            "#438 [Test Phase] Loss: 0.6174 Acc: 68.2028% Time: 3742.6765s\n",
            "#439 [Test Phase] Loss: 0.9089 Acc: 49.2758% Time: 3751.2248s\n",
            "#440 [Test Phase] Loss: 0.7654 Acc: 58.0316% Time: 3759.8061s\n",
            "#441 [Test Phase] Loss: 0.6570 Acc: 65.4049% Time: 3768.2057s\n",
            "#442 [Test Phase] Loss: 0.6841 Acc: 64.0224% Time: 3776.6910s\n",
            "#443 [Test Phase] Loss: 0.6699 Acc: 64.4174% Time: 3785.1903s\n",
            "#444 [Test Phase] Loss: 0.9141 Acc: 48.8808% Time: 3793.7565s\n",
            "#445 [Test Phase] Loss: 0.6151 Acc: 65.7669% Time: 3802.2519s\n",
            "#446 [Test Phase] Loss: 0.6291 Acc: 63.8249% Time: 3810.7868s\n",
            "#447 [Test Phase] Loss: 0.7822 Acc: 57.2416% Time: 3819.4389s\n",
            "#448 [Test Phase] Loss: 0.8608 Acc: 52.4358% Time: 3827.8911s\n",
            "#449 [Test Phase] Loss: 0.6050 Acc: 69.4207% Time: 3836.3697s\n",
            "#450 [Test Phase] Loss: 0.7624 Acc: 58.9862% Time: 3844.7493s\n",
            "#451 [Test Phase] Loss: 0.7623 Acc: 58.6241% Time: 3853.2025s\n",
            "#452 [Test Phase] Loss: 0.6470 Acc: 66.1290% Time: 3861.7493s\n",
            "#453 [Test Phase] Loss: 0.7163 Acc: 61.7512% Time: 3870.1530s\n",
            "#454 [Test Phase] Loss: 0.7369 Acc: 60.7966% Time: 3878.5389s\n",
            "#455 [Test Phase] Loss: 0.8340 Acc: 55.1020% Time: 3886.9735s\n",
            "#456 [Test Phase] Loss: 0.7312 Acc: 60.5662% Time: 3895.6125s\n",
            "#457 [Test Phase] Loss: 0.6815 Acc: 58.0645% Time: 3903.9518s\n",
            "#458 [Test Phase] Loss: 0.7186 Acc: 61.8170% Time: 3912.3435s\n",
            "#459 [Test Phase] Loss: 0.8194 Acc: 55.1679% Time: 3920.7626s\n",
            "#460 [Test Phase] Loss: 0.7450 Acc: 60.4016% Time: 3929.0406s\n",
            "#461 [Test Phase] Loss: 0.7357 Acc: 60.5003% Time: 3937.5280s\n",
            "#462 [Test Phase] Loss: 0.6766 Acc: 64.4503% Time: 3945.9902s\n",
            "#463 [Test Phase] Loss: 0.7901 Acc: 57.7683% Time: 3954.4459s\n",
            "#464 [Test Phase] Loss: 0.7991 Acc: 56.1224% Time: 3962.9271s\n",
            "#465 [Test Phase] Loss: 0.6641 Acc: 65.5365% Time: 3971.2754s\n",
            "#466 [Test Phase] Loss: 0.6753 Acc: 64.3515% Time: 3979.7793s\n",
            "#467 [Test Phase] Loss: 0.8620 Acc: 52.3371% Time: 3988.4014s\n",
            "#468 [Test Phase] Loss: 0.7806 Acc: 49.3746% Time: 3996.8335s\n",
            "#469 [Test Phase] Loss: 0.8983 Acc: 50.9217% Time: 4005.3269s\n",
            "#470 [Test Phase] Loss: 0.7196 Acc: 61.7841% Time: 4013.7027s\n",
            "#471 [Test Phase] Loss: 0.6818 Acc: 64.2199% Time: 4022.0905s\n",
            "#472 [Test Phase] Loss: 0.6923 Acc: 63.1336% Time: 4030.6913s\n",
            "#473 [Test Phase] Loss: 0.6963 Acc: 63.3970% Time: 4039.3024s\n",
            "#474 [Test Phase] Loss: 0.7365 Acc: 59.7103% Time: 4047.6816s\n",
            "#475 [Test Phase] Loss: 0.7036 Acc: 62.8703% Time: 4056.1333s\n",
            "#476 [Test Phase] Loss: 0.7561 Acc: 59.6445% Time: 4064.6952s\n",
            "#477 [Test Phase] Loss: 0.7408 Acc: 60.0395% Time: 4073.3027s\n",
            "#478 [Test Phase] Loss: 0.7061 Acc: 62.7057% Time: 4082.0199s\n",
            "#479 [Test Phase] Loss: 0.6689 Acc: 59.6774% Time: 4090.4530s\n",
            "#480 [Test Phase] Loss: 0.7872 Acc: 57.4720% Time: 4098.9737s\n",
            "#481 [Test Phase] Loss: 0.6666 Acc: 65.2074% Time: 4107.4553s\n",
            "#482 [Test Phase] Loss: 0.8329 Acc: 54.4766% Time: 4115.8298s\n",
            "#483 [Test Phase] Loss: 0.6606 Acc: 65.6682% Time: 4124.2489s\n",
            "#484 [Test Phase] Loss: 0.8115 Acc: 56.7149% Time: 4132.8491s\n",
            "#485 [Test Phase] Loss: 0.7314 Acc: 60.3687% Time: 4141.2219s\n",
            "#486 [Test Phase] Loss: 0.6515 Acc: 66.4911% Time: 4149.8295s\n",
            "#487 [Test Phase] Loss: 0.6856 Acc: 64.3515% Time: 4158.2828s\n",
            "#488 [Test Phase] Loss: 0.8584 Acc: 53.8841% Time: 4166.6454s\n",
            "#489 [Test Phase] Loss: 0.7352 Acc: 60.6649% Time: 4175.1679s\n",
            "#490 [Test Phase] Loss: 0.7577 Acc: 50.5925% Time: 4183.6312s\n",
            "#491 [Test Phase] Loss: 0.8383 Acc: 54.3450% Time: 4192.0812s\n",
            "#492 [Test Phase] Loss: 0.8016 Acc: 56.5174% Time: 4200.4953s\n",
            "#493 [Test Phase] Loss: 0.8797 Acc: 51.9750% Time: 4208.8716s\n",
            "#494 [Test Phase] Loss: 0.6087 Acc: 68.4332% Time: 4217.4100s\n",
            "#495 [Test Phase] Loss: 0.7119 Acc: 62.3107% Time: 4225.8830s\n",
            "#496 [Test Phase] Loss: 0.6525 Acc: 66.4911% Time: 4234.4354s\n",
            "#497 [Test Phase] Loss: 0.7720 Acc: 58.5253% Time: 4242.8983s\n",
            "#498 [Test Phase] Loss: 0.5696 Acc: 71.0994% Time: 4251.4845s\n",
            "#499 [Test Phase] Loss: 0.7411 Acc: 60.2370% Time: 4260.0557s\n",
            "#500 [Test Phase] Loss: 0.7184 Acc: 62.0474% Time: 4268.7390s\n",
            "#501 [Test Phase] Loss: 0.7399 Acc: 53.1600% Time: 4277.2874s\n",
            "#502 [Test Phase] Loss: 0.7117 Acc: 62.5082% Time: 4285.6853s\n",
            "#503 [Test Phase] Loss: 0.6555 Acc: 66.1949% Time: 4294.1299s\n",
            "#504 [Test Phase] Loss: 0.7196 Acc: 62.2120% Time: 4302.6155s\n",
            "#505 [Test Phase] Loss: 0.8681 Acc: 52.6004% Time: 4311.0480s\n",
            "#506 [Test Phase] Loss: 0.6715 Acc: 64.7795% Time: 4319.4476s\n",
            "#507 [Test Phase] Loss: 0.6996 Acc: 63.4957% Time: 4328.0749s\n",
            "#508 [Test Phase] Loss: 0.7460 Acc: 60.6649% Time: 4336.5318s\n",
            "#509 [Test Phase] Loss: 0.6048 Acc: 68.7953% Time: 4345.0200s\n",
            "#510 [Test Phase] Loss: 0.6456 Acc: 66.3594% Time: 4353.3993s\n",
            "#511 [Test Phase] Loss: 0.7344 Acc: 61.2245% Time: 4361.8196s\n",
            "#512 [Test Phase] Loss: 0.7163 Acc: 55.0033% Time: 4370.4061s\n",
            "#513 [Test Phase] Loss: 0.7528 Acc: 59.2495% Time: 4379.0218s\n",
            "#514 [Test Phase] Loss: 0.8665 Acc: 52.0408% Time: 4387.4672s\n",
            "#515 [Test Phase] Loss: 0.7423 Acc: 60.5662% Time: 4395.9844s\n",
            "#516 [Test Phase] Loss: 0.7755 Acc: 58.6570% Time: 4404.6352s\n",
            "#517 [Test Phase] Loss: 0.7372 Acc: 61.1587% Time: 4413.2243s\n",
            "#518 [Test Phase] Loss: 0.7239 Acc: 61.5536% Time: 4421.7023s\n",
            "#519 [Test Phase] Loss: 0.6835 Acc: 64.1540% Time: 4430.1897s\n",
            "#520 [Test Phase] Loss: 0.7432 Acc: 60.2699% Time: 4438.6063s\n",
            "#521 [Test Phase] Loss: 0.7365 Acc: 60.5991% Time: 4446.9798s\n",
            "#522 [Test Phase] Loss: 0.7357 Acc: 60.9612% Time: 4455.3229s\n",
            "#523 [Test Phase] Loss: 0.7262 Acc: 54.1804% Time: 4463.8708s\n",
            "#524 [Test Phase] Loss: 0.7580 Acc: 58.9533% Time: 4472.4205s\n",
            "#525 [Test Phase] Loss: 0.6442 Acc: 66.6886% Time: 4480.9718s\n",
            "#526 [Test Phase] Loss: 0.8306 Acc: 55.1350% Time: 4489.3370s\n",
            "#527 [Test Phase] Loss: 0.7800 Acc: 58.1962% Time: 4497.7033s\n",
            "#528 [Test Phase] Loss: 0.7537 Acc: 59.8091% Time: 4506.1133s\n",
            "#529 [Test Phase] Loss: 0.7905 Acc: 57.6695% Time: 4514.6011s\n",
            "#530 [Test Phase] Loss: 0.8072 Acc: 56.6820% Time: 4523.0051s\n",
            "#531 [Test Phase] Loss: 0.8238 Acc: 55.2337% Time: 4531.5396s\n",
            "#532 [Test Phase] Loss: 0.6297 Acc: 67.6432% Time: 4540.2267s\n",
            "#533 [Test Phase] Loss: 0.6611 Acc: 65.7999% Time: 4548.7995s\n",
            "#534 [Test Phase] Loss: 0.6832 Acc: 58.7887% Time: 4557.3292s\n",
            "#535 [Test Phase] Loss: 0.7126 Acc: 62.7716% Time: 4565.6753s\n",
            "#536 [Test Phase] Loss: 0.6591 Acc: 66.2936% Time: 4574.0338s\n",
            "#537 [Test Phase] Loss: 0.6879 Acc: 64.1211% Time: 4582.5031s\n",
            "#538 [Test Phase] Loss: 0.6337 Acc: 67.4786% Time: 4590.8929s\n",
            "#539 [Test Phase] Loss: 0.6421 Acc: 66.6228% Time: 4599.2774s\n",
            "#540 [Test Phase] Loss: 0.8445 Acc: 54.3450% Time: 4607.7927s\n",
            "#541 [Test Phase] Loss: 0.9083 Acc: 49.8025% Time: 4616.2965s\n",
            "#542 [Test Phase] Loss: 0.6139 Acc: 68.9598% Time: 4624.7025s\n",
            "#543 [Test Phase] Loss: 0.7525 Acc: 59.6774% Time: 4633.0169s\n",
            "#544 [Test Phase] Loss: 0.7023 Acc: 62.7057% Time: 4641.4688s\n",
            "#545 [Test Phase] Loss: 0.8280 Acc: 45.5563% Time: 4649.9931s\n",
            "#546 [Test Phase] Loss: 0.6384 Acc: 66.6886% Time: 4658.5078s\n",
            "#547 [Test Phase] Loss: 0.7288 Acc: 61.8170% Time: 4666.8796s\n",
            "#548 [Test Phase] Loss: 0.8160 Acc: 55.2666% Time: 4675.2809s\n",
            "#549 [Test Phase] Loss: 0.8049 Acc: 56.7808% Time: 4683.7549s\n",
            "#550 [Test Phase] Loss: 0.6774 Acc: 64.6149% Time: 4692.3058s\n",
            "#551 [Test Phase] Loss: 0.6285 Acc: 67.9065% Time: 4700.9629s\n",
            "#552 [Test Phase] Loss: 0.7081 Acc: 62.9361% Time: 4709.5364s\n",
            "#553 [Test Phase] Loss: 0.7264 Acc: 61.5207% Time: 4717.8895s\n",
            "#554 [Test Phase] Loss: 0.9659 Acc: 47.1363% Time: 4726.3361s\n",
            "#555 [Test Phase] Loss: 0.7258 Acc: 61.7512% Time: 4734.6507s\n",
            "#556 [Test Phase] Loss: 0.7157 Acc: 51.6787% Time: 4743.1136s\n",
            "#557 [Test Phase] Loss: 0.7607 Acc: 51.5471% Time: 4751.7778s\n",
            "#558 [Test Phase] Loss: 0.6691 Acc: 65.2403% Time: 4760.3391s\n",
            "#559 [Test Phase] Loss: 0.7494 Acc: 59.8420% Time: 4768.8826s\n",
            "#560 [Test Phase] Loss: 0.6942 Acc: 63.6603% Time: 4777.2304s\n",
            "#561 [Test Phase] Loss: 0.6773 Acc: 64.7465% Time: 4785.5911s\n",
            "#562 [Test Phase] Loss: 0.7147 Acc: 62.2449% Time: 4794.0009s\n",
            "#563 [Test Phase] Loss: 0.7715 Acc: 59.3153% Time: 4802.4392s\n",
            "#564 [Test Phase] Loss: 0.7083 Acc: 62.4753% Time: 4810.8185s\n",
            "#565 [Test Phase] Loss: 0.8082 Acc: 56.4516% Time: 4819.2817s\n",
            "#566 [Test Phase] Loss: 0.6721 Acc: 65.3061% Time: 4827.7177s\n",
            "#567 [Test Phase] Loss: 0.7909 Acc: 58.0645% Time: 4836.2276s\n",
            "#568 [Test Phase] Loss: 0.6471 Acc: 62.4095% Time: 4844.6388s\n",
            "#569 [Test Phase] Loss: 0.6514 Acc: 66.5240% Time: 4853.0909s\n",
            "#570 [Test Phase] Loss: 0.8317 Acc: 55.5958% Time: 4861.6129s\n",
            "#571 [Test Phase] Loss: 0.7839 Acc: 58.2291% Time: 4870.0774s\n",
            "#572 [Test Phase] Loss: 0.7217 Acc: 62.2120% Time: 4878.3200s\n",
            "#573 [Test Phase] Loss: 0.7292 Acc: 61.4549% Time: 4886.7691s\n",
            "#574 [Test Phase] Loss: 0.6713 Acc: 65.1086% Time: 4895.2319s\n",
            "#575 [Test Phase] Loss: 0.7474 Acc: 60.5332% Time: 4903.6974s\n",
            "#576 [Test Phase] Loss: 0.6616 Acc: 65.3390% Time: 4912.2860s\n",
            "#577 [Test Phase] Loss: 0.7084 Acc: 63.1666% Time: 4920.7838s\n",
            "#578 [Test Phase] Loss: 0.6927 Acc: 63.7920% Time: 4929.2751s\n",
            "#579 [Test Phase] Loss: 0.8531 Acc: 44.2396% Time: 4937.7157s\n",
            "#580 [Test Phase] Loss: 0.7058 Acc: 63.1007% Time: 4946.1583s\n",
            "#581 [Test Phase] Loss: 0.7221 Acc: 62.0145% Time: 4954.6242s\n",
            "#582 [Test Phase] Loss: 0.5993 Acc: 69.1573% Time: 4962.9660s\n",
            "#583 [Test Phase] Loss: 0.7349 Acc: 60.7966% Time: 4971.4774s\n",
            "#584 [Test Phase] Loss: 0.7161 Acc: 63.1336% Time: 4979.9989s\n",
            "#585 [Test Phase] Loss: 0.7342 Acc: 61.3562% Time: 4988.4466s\n",
            "#586 [Test Phase] Loss: 0.7312 Acc: 61.4220% Time: 4997.1015s\n",
            "#587 [Test Phase] Loss: 0.7547 Acc: 59.9408% Time: 5005.6587s\n",
            "#588 [Test Phase] Loss: 0.7247 Acc: 61.6524% Time: 5014.2001s\n",
            "#589 [Test Phase] Loss: 0.8193 Acc: 55.9579% Time: 5022.7268s\n",
            "#590 [Test Phase] Loss: 0.6591 Acc: 60.6649% Time: 5031.1919s\n",
            "#591 [Test Phase] Loss: 0.6819 Acc: 64.2857% Time: 5039.5381s\n",
            "#592 [Test Phase] Loss: 0.7606 Acc: 59.4141% Time: 5047.9140s\n",
            "#593 [Test Phase] Loss: 0.7663 Acc: 59.3483% Time: 5056.3762s\n",
            "#594 [Test Phase] Loss: 0.7186 Acc: 62.2778% Time: 5064.9212s\n",
            "#595 [Test Phase] Loss: 0.7292 Acc: 62.0474% Time: 5073.3800s\n",
            "#596 [Test Phase] Loss: 0.5718 Acc: 71.5602% Time: 5081.7580s\n",
            "#597 [Test Phase] Loss: 0.8100 Acc: 56.8137% Time: 5090.2748s\n",
            "#598 [Test Phase] Loss: 0.6775 Acc: 65.1086% Time: 5098.8289s\n",
            "#599 [Test Phase] Loss: 0.8342 Acc: 54.8716% Time: 5107.2497s\n",
            "#600 [Test Phase] Loss: 0.7785 Acc: 58.6570% Time: 5115.7083s\n",
            "#601 [Test Phase] Loss: 0.6821 Acc: 58.9533% Time: 5124.1534s\n",
            "#602 [Test Phase] Loss: 0.6939 Acc: 63.9565% Time: 5132.7070s\n",
            "#603 [Test Phase] Loss: 0.7282 Acc: 61.6853% Time: 5141.1278s\n",
            "#604 [Test Phase] Loss: 0.6998 Acc: 64.0224% Time: 5149.5035s\n",
            "#605 [Test Phase] Loss: 0.7419 Acc: 60.8953% Time: 5158.1732s\n",
            "#606 [Test Phase] Loss: 0.7475 Acc: 61.3891% Time: 5166.5754s\n",
            "#607 [Test Phase] Loss: 0.7031 Acc: 62.7716% Time: 5175.1481s\n",
            "#608 [Test Phase] Loss: 0.7632 Acc: 59.7762% Time: 5183.7873s\n",
            "#609 [Test Phase] Loss: 0.8589 Acc: 53.5220% Time: 5192.2288s\n",
            "#610 [Test Phase] Loss: 0.7408 Acc: 60.9941% Time: 5200.7388s\n",
            "#611 [Test Phase] Loss: 0.6605 Acc: 65.7011% Time: 5209.4317s\n",
            "#612 [Test Phase] Loss: 0.7031 Acc: 56.6491% Time: 5217.9545s\n",
            "#613 [Test Phase] Loss: 0.8036 Acc: 57.4062% Time: 5226.5037s\n",
            "#614 [Test Phase] Loss: 0.7403 Acc: 60.9282% Time: 5234.9195s\n",
            "#615 [Test Phase] Loss: 0.7633 Acc: 59.0520% Time: 5243.2857s\n",
            "#616 [Test Phase] Loss: 0.5866 Acc: 70.3094% Time: 5251.7083s\n",
            "#617 [Test Phase] Loss: 0.7902 Acc: 57.9987% Time: 5260.1147s\n",
            "#618 [Test Phase] Loss: 0.6965 Acc: 63.6274% Time: 5268.5866s\n",
            "#619 [Test Phase] Loss: 0.8220 Acc: 55.7275% Time: 5277.1170s\n",
            "#620 [Test Phase] Loss: 0.6387 Acc: 67.5115% Time: 5285.6364s\n",
            "#621 [Test Phase] Loss: 0.6260 Acc: 67.4128% Time: 5294.1054s\n",
            "#622 [Test Phase] Loss: 0.7669 Acc: 59.4141% Time: 5302.4725s\n",
            "#623 [Test Phase] Loss: 0.7965 Acc: 49.1113% Time: 5310.9527s\n",
            "#624 [Test Phase] Loss: 0.6050 Acc: 69.2232% Time: 5319.4108s\n",
            "#625 [Test Phase] Loss: 0.6039 Acc: 69.4536% Time: 5327.8390s\n",
            "#626 [Test Phase] Loss: 0.7380 Acc: 61.3562% Time: 5336.1552s\n",
            "#627 [Test Phase] Loss: 0.8386 Acc: 54.7729% Time: 5344.6550s\n",
            "#628 [Test Phase] Loss: 0.7649 Acc: 59.2495% Time: 5353.0555s\n",
            "#629 [Test Phase] Loss: 0.6497 Acc: 66.5240% Time: 5361.3272s\n",
            "#630 [Test Phase] Loss: 0.5921 Acc: 70.1448% Time: 5369.7473s\n",
            "#631 [Test Phase] Loss: 0.7274 Acc: 61.6853% Time: 5378.1356s\n",
            "#632 [Test Phase] Loss: 0.5912 Acc: 70.2436% Time: 5386.5152s\n",
            "#633 [Test Phase] Loss: 0.7576 Acc: 59.7103% Time: 5394.9448s\n",
            "#634 [Test Phase] Loss: 0.6178 Acc: 65.9315% Time: 5403.2638s\n",
            "#635 [Test Phase] Loss: 0.7331 Acc: 61.5207% Time: 5411.4763s\n",
            "#636 [Test Phase] Loss: 0.7274 Acc: 62.0803% Time: 5419.8881s\n",
            "#637 [Test Phase] Loss: 0.5683 Acc: 71.5932% Time: 5428.0973s\n",
            "#638 [Test Phase] Loss: 0.8544 Acc: 53.3904% Time: 5436.4233s\n",
            "#639 [Test Phase] Loss: 0.7020 Acc: 63.1336% Time: 5444.7164s\n",
            "#640 [Test Phase] Loss: 0.6443 Acc: 66.5569% Time: 5453.0539s\n",
            "#641 [Test Phase] Loss: 0.8095 Acc: 56.0895% Time: 5461.4255s\n",
            "#642 [Test Phase] Loss: 0.6575 Acc: 66.3265% Time: 5469.6719s\n",
            "#643 [Test Phase] Loss: 0.7941 Acc: 57.4391% Time: 5477.9044s\n",
            "#644 [Test Phase] Loss: 0.7307 Acc: 61.2574% Time: 5486.1867s\n",
            "#645 [Test Phase] Loss: 0.7445 Acc: 53.0612% Time: 5494.4645s\n",
            "#646 [Test Phase] Loss: 0.7127 Acc: 62.3436% Time: 5502.8625s\n",
            "#647 [Test Phase] Loss: 0.7811 Acc: 58.5912% Time: 5511.3253s\n",
            "#648 [Test Phase] Loss: 0.7051 Acc: 63.2324% Time: 5519.7005s\n",
            "#649 [Test Phase] Loss: 0.7500 Acc: 60.7637% Time: 5528.0064s\n",
            "#650 [Test Phase] Loss: 0.6349 Acc: 67.2153% Time: 5536.5023s\n",
            "#651 [Test Phase] Loss: 0.5905 Acc: 69.5523% Time: 5544.9539s\n",
            "#652 [Test Phase] Loss: 0.7611 Acc: 59.2495% Time: 5553.3081s\n",
            "#653 [Test Phase] Loss: 0.7761 Acc: 58.6899% Time: 5561.6519s\n",
            "#654 [Test Phase] Loss: 0.6956 Acc: 63.8249% Time: 5570.0115s\n",
            "#655 [Test Phase] Loss: 0.7007 Acc: 63.0020% Time: 5578.3179s\n",
            "#656 [Test Phase] Loss: 0.6158 Acc: 65.4378% Time: 5586.7499s\n",
            "#657 [Test Phase] Loss: 0.7888 Acc: 57.8341% Time: 5595.1121s\n",
            "#658 [Test Phase] Loss: 0.6009 Acc: 69.4536% Time: 5603.4485s\n",
            "#659 [Test Phase] Loss: 0.8802 Acc: 51.7116% Time: 5611.7690s\n",
            "#660 [Test Phase] Loss: 0.8845 Acc: 52.2054% Time: 5620.0328s\n",
            "#661 [Test Phase] Loss: 0.6598 Acc: 66.0961% Time: 5628.4575s\n",
            "#662 [Test Phase] Loss: 0.7277 Acc: 61.8170% Time: 5636.8783s\n",
            "#663 [Test Phase] Loss: 0.7058 Acc: 63.6603% Time: 5645.3398s\n",
            "#664 [Test Phase] Loss: 0.8154 Acc: 56.4845% Time: 5653.8256s\n",
            "#665 [Test Phase] Loss: 0.7587 Acc: 60.3357% Time: 5662.1934s\n",
            "#666 [Test Phase] Loss: 0.6127 Acc: 68.6636% Time: 5670.6073s\n",
            "#667 [Test Phase] Loss: 0.8883 Acc: 31.9289% Time: 5679.0190s\n",
            "#668 [Test Phase] Loss: 0.6757 Acc: 60.1712% Time: 5687.4497s\n",
            "#669 [Test Phase] Loss: 0.6002 Acc: 69.2890% Time: 5695.7757s\n",
            "#670 [Test Phase] Loss: 0.7364 Acc: 61.6195% Time: 5703.9853s\n",
            "#671 [Test Phase] Loss: 0.6664 Acc: 65.8657% Time: 5712.3867s\n",
            "#672 [Test Phase] Loss: 0.8282 Acc: 55.6945% Time: 5720.7600s\n",
            "#673 [Test Phase] Loss: 0.6628 Acc: 65.9974% Time: 5729.2034s\n",
            "#674 [Test Phase] Loss: 0.7636 Acc: 59.4470% Time: 5737.6179s\n",
            "#675 [Test Phase] Loss: 0.8408 Acc: 54.5754% Time: 5745.9663s\n",
            "#676 [Test Phase] Loss: 0.8319 Acc: 55.5300% Time: 5754.2840s\n",
            "#677 [Test Phase] Loss: 0.6310 Acc: 67.3469% Time: 5762.7704s\n",
            "#678 [Test Phase] Loss: 0.7441 Acc: 61.0599% Time: 5771.1898s\n",
            "#679 [Test Phase] Loss: 0.7482 Acc: 52.6662% Time: 5779.6556s\n",
            "#680 [Test Phase] Loss: 0.8193 Acc: 56.7149% Time: 5788.0625s\n",
            "#681 [Test Phase] Loss: 0.6556 Acc: 66.1949% Time: 5796.3922s\n",
            "#682 [Test Phase] Loss: 0.7501 Acc: 60.9612% Time: 5804.8416s\n",
            "#683 [Test Phase] Loss: 0.6597 Acc: 65.3061% Time: 5813.1420s\n",
            "#684 [Test Phase] Loss: 0.7446 Acc: 61.0599% Time: 5821.5702s\n",
            "#685 [Test Phase] Loss: 0.6191 Acc: 68.4990% Time: 5829.9963s\n",
            "#686 [Test Phase] Loss: 0.7593 Acc: 60.1053% Time: 5838.3372s\n",
            "#687 [Test Phase] Loss: 0.7251 Acc: 61.6195% Time: 5846.5112s\n",
            "#688 [Test Phase] Loss: 0.7658 Acc: 59.7762% Time: 5854.7934s\n",
            "#689 [Test Phase] Loss: 0.8268 Acc: 55.6616% Time: 5863.1849s\n",
            "#690 [Test Phase] Loss: 0.6789 Acc: 59.5457% Time: 5871.6988s\n",
            "#691 [Test Phase] Loss: 0.6951 Acc: 63.8578% Time: 5880.0239s\n",
            "#692 [Test Phase] Loss: 0.7465 Acc: 60.6978% Time: 5888.4293s\n",
            "#693 [Test Phase] Loss: 0.7564 Acc: 60.2699% Time: 5896.7672s\n",
            "#694 [Test Phase] Loss: 0.7724 Acc: 59.2495% Time: 5905.1680s\n",
            "#695 [Test Phase] Loss: 0.6540 Acc: 65.8328% Time: 5913.2856s\n",
            "#696 [Test Phase] Loss: 0.6885 Acc: 64.4832% Time: 5921.7399s\n",
            "#697 [Test Phase] Loss: 0.6239 Acc: 68.1698% Time: 5930.1953s\n",
            "#698 [Test Phase] Loss: 0.6842 Acc: 64.4174% Time: 5938.5836s\n",
            "#699 [Test Phase] Loss: 0.7444 Acc: 60.7307% Time: 5946.9808s\n",
            "#700 [Test Phase] Loss: 0.7619 Acc: 59.4141% Time: 5955.3855s\n",
            "#701 [Test Phase] Loss: 0.7474 Acc: 52.6662% Time: 5963.7761s\n",
            "#702 [Test Phase] Loss: 0.7312 Acc: 62.2120% Time: 5972.1053s\n",
            "#703 [Test Phase] Loss: 0.8467 Acc: 54.7729% Time: 5980.3553s\n",
            "#704 [Test Phase] Loss: 0.6069 Acc: 68.6636% Time: 5988.6860s\n",
            "#705 [Test Phase] Loss: 0.8790 Acc: 52.7650% Time: 5997.1450s\n",
            "#706 [Test Phase] Loss: 0.6026 Acc: 69.2561% Time: 6005.4240s\n",
            "#707 [Test Phase] Loss: 0.6105 Acc: 68.9269% Time: 6013.7680s\n",
            "#708 [Test Phase] Loss: 0.7143 Acc: 62.5082% Time: 6021.9635s\n",
            "#709 [Test Phase] Loss: 0.6581 Acc: 65.9315% Time: 6030.3225s\n",
            "#710 [Test Phase] Loss: 0.6817 Acc: 64.8124% Time: 6038.5968s\n",
            "#711 [Test Phase] Loss: 0.6980 Acc: 63.9895% Time: 6046.8946s\n",
            "#712 [Test Phase] Loss: 0.6285 Acc: 64.1540% Time: 6055.1400s\n",
            "#713 [Test Phase] Loss: 0.7431 Acc: 60.4345% Time: 6063.4165s\n",
            "#714 [Test Phase] Loss: 0.7262 Acc: 62.1791% Time: 6071.5820s\n",
            "#715 [Test Phase] Loss: 0.7751 Acc: 59.1178% Time: 6079.8766s\n",
            "#716 [Test Phase] Loss: 0.7790 Acc: 59.1837% Time: 6088.1081s\n",
            "#717 [Test Phase] Loss: 0.6720 Acc: 65.2403% Time: 6096.4382s\n",
            "#718 [Test Phase] Loss: 0.8955 Acc: 51.8104% Time: 6104.6524s\n",
            "#719 [Test Phase] Loss: 0.5387 Acc: 73.1731% Time: 6112.9633s\n",
            "#720 [Test Phase] Loss: 0.7970 Acc: 57.8341% Time: 6121.2570s\n",
            "#721 [Test Phase] Loss: 0.7555 Acc: 60.4345% Time: 6129.4408s\n",
            "#722 [Test Phase] Loss: 0.6915 Acc: 63.7920% Time: 6137.7042s\n",
            "#723 [Test Phase] Loss: 0.8238 Acc: 46.1817% Time: 6146.0143s\n",
            "#724 [Test Phase] Loss: 0.7621 Acc: 60.2370% Time: 6154.3245s\n",
            "#725 [Test Phase] Loss: 0.7261 Acc: 61.8828% Time: 6162.7214s\n",
            "#726 [Test Phase] Loss: 0.7749 Acc: 58.7887% Time: 6171.0388s\n",
            "#727 [Test Phase] Loss: 0.7511 Acc: 60.1053% Time: 6179.4974s\n",
            "#728 [Test Phase] Loss: 0.7274 Acc: 61.9816% Time: 6187.8286s\n",
            "#729 [Test Phase] Loss: 0.6673 Acc: 65.3061% Time: 6196.1567s\n",
            "#730 [Test Phase] Loss: 0.6338 Acc: 67.6103% Time: 6204.5603s\n",
            "#731 [Test Phase] Loss: 0.7172 Acc: 62.9361% Time: 6212.8880s\n",
            "#732 [Test Phase] Loss: 0.7477 Acc: 60.7966% Time: 6221.3234s\n",
            "#733 [Test Phase] Loss: 0.5887 Acc: 70.3423% Time: 6229.7717s\n",
            "#734 [Test Phase] Loss: 0.7112 Acc: 56.3529% Time: 6237.9672s\n",
            "#735 [Test Phase] Loss: 0.7922 Acc: 57.8012% Time: 6246.3603s\n",
            "#736 [Test Phase] Loss: 0.6244 Acc: 68.4003% Time: 6254.6001s\n",
            "#737 [Test Phase] Loss: 0.7875 Acc: 58.3608% Time: 6263.0068s\n",
            "#738 [Test Phase] Loss: 0.8990 Acc: 51.1521% Time: 6271.4912s\n",
            "#739 [Test Phase] Loss: 0.7389 Acc: 61.4549% Time: 6279.7878s\n",
            "#740 [Test Phase] Loss: 0.8314 Acc: 55.7933% Time: 6288.2544s\n",
            "#741 [Test Phase] Loss: 0.7235 Acc: 62.3107% Time: 6296.6064s\n",
            "#742 [Test Phase] Loss: 0.7685 Acc: 59.5128% Time: 6305.0304s\n",
            "#743 [Test Phase] Loss: 0.6883 Acc: 64.3515% Time: 6313.3421s\n",
            "#744 [Test Phase] Loss: 0.7964 Acc: 57.8670% Time: 6321.5179s\n",
            "#745 [Test Phase] Loss: 0.7834 Acc: 49.9012% Time: 6329.7259s\n",
            "#746 [Test Phase] Loss: 0.8782 Acc: 52.3700% Time: 6338.1781s\n",
            "#747 [Test Phase] Loss: 0.7341 Acc: 61.2245% Time: 6346.4451s\n",
            "#748 [Test Phase] Loss: 0.7836 Acc: 57.6037% Time: 6354.7007s\n",
            "#749 [Test Phase] Loss: 0.8176 Acc: 56.9124% Time: 6363.0050s\n",
            "#750 [Test Phase] Loss: 0.7983 Acc: 57.3074% Time: 6371.4921s\n",
            "#751 [Test Phase] Loss: 0.7463 Acc: 61.1587% Time: 6379.7865s\n",
            "#752 [Test Phase] Loss: 0.7366 Acc: 61.6524% Time: 6388.2148s\n",
            "#753 [Test Phase] Loss: 0.8065 Acc: 57.7353% Time: 6396.5702s\n",
            "#754 [Test Phase] Loss: 0.6777 Acc: 65.1745% Time: 6404.8234s\n",
            "#755 [Test Phase] Loss: 0.7447 Acc: 60.3687% Time: 6413.0609s\n",
            "#756 [Test Phase] Loss: 0.7169 Acc: 56.2212% Time: 6421.3818s\n",
            "#757 [Test Phase] Loss: 0.8956 Acc: 52.1066% Time: 6429.7445s\n",
            "#758 [Test Phase] Loss: 0.8372 Acc: 56.1883% Time: 6438.0810s\n",
            "#759 [Test Phase] Loss: 0.6521 Acc: 66.2936% Time: 6446.3881s\n",
            "#760 [Test Phase] Loss: 0.7759 Acc: 59.0520% Time: 6454.8062s\n",
            "#761 [Test Phase] Loss: 0.7030 Acc: 63.2653% Time: 6463.1086s\n",
            "#762 [Test Phase] Loss: 0.7944 Acc: 57.5049% Time: 6471.2991s\n",
            "#763 [Test Phase] Loss: 0.6941 Acc: 63.2324% Time: 6479.6124s\n",
            "#764 [Test Phase] Loss: 0.7229 Acc: 61.6195% Time: 6487.9896s\n",
            "#765 [Test Phase] Loss: 0.7136 Acc: 62.7716% Time: 6496.3174s\n",
            "#766 [Test Phase] Loss: 0.7244 Acc: 62.3107% Time: 6504.7344s\n",
            "#767 [Test Phase] Loss: 0.6938 Acc: 57.6366% Time: 6513.0905s\n",
            "#768 [Test Phase] Loss: 0.7037 Acc: 62.9361% Time: 6521.4543s\n",
            "#769 [Test Phase] Loss: 0.7734 Acc: 59.2824% Time: 6529.8572s\n",
            "#770 [Test Phase] Loss: 0.7642 Acc: 59.9737% Time: 6538.1923s\n",
            "#771 [Test Phase] Loss: 0.8367 Acc: 55.6287% Time: 6546.5200s\n",
            "#772 [Test Phase] Loss: 0.8434 Acc: 55.0033% Time: 6554.6214s\n",
            "#773 [Test Phase] Loss: 0.7415 Acc: 61.0928% Time: 6562.9586s\n",
            "#774 [Test Phase] Loss: 0.7264 Acc: 61.9487% Time: 6571.3386s\n",
            "#775 [Test Phase] Loss: 0.8499 Acc: 54.0816% Time: 6579.7281s\n",
            "#776 [Test Phase] Loss: 0.7426 Acc: 60.9612% Time: 6588.1501s\n",
            "#777 [Test Phase] Loss: 0.8137 Acc: 56.9454% Time: 6596.5137s\n",
            "#778 [Test Phase] Loss: 0.7781 Acc: 43.2521% Time: 6604.7502s\n",
            "#779 [Test Phase] Loss: 0.6553 Acc: 61.7841% Time: 6613.0804s\n",
            "#780 [Test Phase] Loss: 0.7628 Acc: 60.4345% Time: 6621.4305s\n",
            "#781 [Test Phase] Loss: 0.7025 Acc: 63.5616% Time: 6629.7531s\n",
            "#782 [Test Phase] Loss: 0.7908 Acc: 57.4062% Time: 6638.0863s\n",
            "#783 [Test Phase] Loss: 0.8325 Acc: 55.3983% Time: 6646.3446s\n",
            "#784 [Test Phase] Loss: 0.6817 Acc: 64.1211% Time: 6654.8009s\n",
            "#785 [Test Phase] Loss: 0.7077 Acc: 63.3641% Time: 6663.1221s\n",
            "#786 [Test Phase] Loss: 0.6842 Acc: 65.0099% Time: 6671.5810s\n",
            "#787 [Test Phase] Loss: 0.8487 Acc: 54.3779% Time: 6679.8364s\n",
            "#788 [Test Phase] Loss: 0.8387 Acc: 54.8387% Time: 6688.2920s\n",
            "#789 [Test Phase] Loss: 0.6707 Acc: 65.4707% Time: 6696.6618s\n",
            "#790 [Test Phase] Loss: 0.5712 Acc: 70.9019% Time: 6705.0542s\n",
            "#791 [Test Phase] Loss: 0.8355 Acc: 55.5958% Time: 6713.4554s\n",
            "#792 [Test Phase] Loss: 0.6393 Acc: 67.6432% Time: 6721.8682s\n",
            "#793 [Test Phase] Loss: 0.7033 Acc: 63.4628% Time: 6730.1747s\n",
            "#794 [Test Phase] Loss: 0.6221 Acc: 68.7294% Time: 6738.4195s\n",
            "#795 [Test Phase] Loss: 0.6755 Acc: 64.9111% Time: 6746.7482s\n",
            "#796 [Test Phase] Loss: 0.6553 Acc: 66.6228% Time: 6755.0161s\n",
            "#797 [Test Phase] Loss: 0.6172 Acc: 68.0711% Time: 6763.4527s\n",
            "#798 [Test Phase] Loss: 0.7384 Acc: 61.3562% Time: 6771.7835s\n",
            "#799 [Test Phase] Loss: 0.6028 Acc: 69.2561% Time: 6780.1814s\n",
            "#800 [Test Phase] Loss: 0.7428 Acc: 60.9282% Time: 6788.4797s\n",
            "#801 [Test Phase] Loss: 0.5825 Acc: 68.8940% Time: 6796.8618s\n",
            "#802 [Test Phase] Loss: 0.7193 Acc: 63.0020% Time: 6805.2049s\n",
            "#803 [Test Phase] Loss: 0.7568 Acc: 60.7966% Time: 6813.5157s\n",
            "#804 [Test Phase] Loss: 0.8414 Acc: 55.0033% Time: 6821.7657s\n",
            "#805 [Test Phase] Loss: 0.8035 Acc: 57.8670% Time: 6830.0691s\n",
            "#806 [Test Phase] Loss: 0.7163 Acc: 62.1461% Time: 6838.5172s\n",
            "#807 [Test Phase] Loss: 0.6791 Acc: 64.7465% Time: 6846.8589s\n",
            "#808 [Test Phase] Loss: 0.6728 Acc: 65.7669% Time: 6855.1930s\n",
            "#809 [Test Phase] Loss: 0.6875 Acc: 64.5161% Time: 6863.6364s\n",
            "#810 [Test Phase] Loss: 0.7735 Acc: 59.1837% Time: 6872.0560s\n",
            "#811 [Test Phase] Loss: 0.9125 Acc: 51.0533% Time: 6880.2313s\n",
            "#812 [Test Phase] Loss: 0.6925 Acc: 58.3608% Time: 6888.5841s\n",
            "#813 [Test Phase] Loss: 0.7471 Acc: 61.0599% Time: 6896.8193s\n",
            "#814 [Test Phase] Loss: 0.5839 Acc: 70.9019% Time: 6905.2387s\n",
            "#815 [Test Phase] Loss: 0.7157 Acc: 63.0020% Time: 6913.6091s\n",
            "#816 [Test Phase] Loss: 0.7064 Acc: 63.7261% Time: 6921.8527s\n",
            "#817 [Test Phase] Loss: 0.9680 Acc: 47.3009% Time: 6930.3127s\n",
            "#818 [Test Phase] Loss: 0.7518 Acc: 60.0395% Time: 6938.6510s\n",
            "#819 [Test Phase] Loss: 0.8629 Acc: 53.3575% Time: 6947.0680s\n",
            "#820 [Test Phase] Loss: 0.7235 Acc: 62.1791% Time: 6955.4420s\n",
            "#821 [Test Phase] Loss: 0.7388 Acc: 60.9282% Time: 6963.7871s\n",
            "#822 [Test Phase] Loss: 0.5566 Acc: 73.0086% Time: 6972.1811s\n",
            "#823 [Test Phase] Loss: 0.6932 Acc: 58.1633% Time: 6980.4163s\n",
            "#824 [Test Phase] Loss: 0.7570 Acc: 60.6320% Time: 6988.7821s\n",
            "#825 [Test Phase] Loss: 0.6842 Acc: 64.3186% Time: 6997.0631s\n",
            "#826 [Test Phase] Loss: 0.8011 Acc: 57.6366% Time: 7005.3729s\n",
            "#827 [Test Phase] Loss: 0.7913 Acc: 58.5583% Time: 7013.8077s\n",
            "#828 [Test Phase] Loss: 0.8054 Acc: 57.4720% Time: 7022.1367s\n",
            "#829 [Test Phase] Loss: 0.7335 Acc: 61.5866% Time: 7030.4448s\n",
            "#830 [Test Phase] Loss: 0.5820 Acc: 70.7044% Time: 7038.7481s\n",
            "#831 [Test Phase] Loss: 0.7533 Acc: 60.5991% Time: 7046.9371s\n",
            "#832 [Test Phase] Loss: 0.6111 Acc: 69.5523% Time: 7055.4304s\n",
            "#833 [Test Phase] Loss: 0.8424 Acc: 55.2666% Time: 7063.8300s\n",
            "#834 [Test Phase] Loss: 0.7923 Acc: 50.2962% Time: 7072.2625s\n",
            "#835 [Test Phase] Loss: 0.7706 Acc: 59.3812% Time: 7080.5545s\n",
            "#836 [Test Phase] Loss: 0.6671 Acc: 65.7669% Time: 7088.7595s\n",
            "#837 [Test Phase] Loss: 0.8996 Acc: 51.4812% Time: 7097.0794s\n",
            "#838 [Test Phase] Loss: 0.7392 Acc: 61.6524% Time: 7105.4226s\n",
            "#839 [Test Phase] Loss: 0.7720 Acc: 58.8216% Time: 7113.7587s\n",
            "#840 [Test Phase] Loss: 0.7868 Acc: 58.8216% Time: 7122.1484s\n",
            "#841 [Test Phase] Loss: 0.7014 Acc: 63.6932% Time: 7130.5850s\n",
            "#842 [Test Phase] Loss: 0.7217 Acc: 62.9032% Time: 7138.7541s\n",
            "#843 [Test Phase] Loss: 0.6329 Acc: 67.2811% Time: 7147.0742s\n",
            "#844 [Test Phase] Loss: 0.7076 Acc: 63.3641% Time: 7155.4176s\n",
            "#845 [Test Phase] Loss: 0.7489 Acc: 53.5550% Time: 7163.8567s\n",
            "#846 [Test Phase] Loss: 0.7783 Acc: 59.5128% Time: 7172.1435s\n",
            "#847 [Test Phase] Loss: 0.7196 Acc: 62.5411% Time: 7180.6590s\n",
            "#848 [Test Phase] Loss: 0.7424 Acc: 61.0599% Time: 7188.9833s\n",
            "#849 [Test Phase] Loss: 0.8247 Acc: 55.5958% Time: 7197.2820s\n",
            "#850 [Test Phase] Loss: 0.6328 Acc: 68.2357% Time: 7205.6923s\n",
            "#851 [Test Phase] Loss: 0.6713 Acc: 66.2607% Time: 7214.1092s\n",
            "#852 [Test Phase] Loss: 0.6826 Acc: 64.3845% Time: 7222.2583s\n",
            "#853 [Test Phase] Loss: 0.6552 Acc: 66.9849% Time: 7230.5015s\n",
            "#854 [Test Phase] Loss: 0.6401 Acc: 66.8532% Time: 7238.7818s\n",
            "#855 [Test Phase] Loss: 0.8479 Acc: 54.5754% Time: 7247.0876s\n",
            "#856 [Test Phase] Loss: 0.6303 Acc: 64.4174% Time: 7255.3269s\n",
            "#857 [Test Phase] Loss: 0.7695 Acc: 59.7432% Time: 7263.6287s\n",
            "#858 [Test Phase] Loss: 0.7662 Acc: 59.8091% Time: 7272.0027s\n",
            "#859 [Test Phase] Loss: 0.6628 Acc: 66.3924% Time: 7280.3408s\n",
            "#860 [Test Phase] Loss: 0.8527 Acc: 53.9500% Time: 7288.5614s\n",
            "#861 [Test Phase] Loss: 0.9165 Acc: 50.8229% Time: 7296.9918s\n",
            "#862 [Test Phase] Loss: 0.5439 Acc: 73.4694% Time: 7305.4509s\n",
            "#863 [Test Phase] Loss: 0.7407 Acc: 61.0599% Time: 7313.7485s\n",
            "#864 [Test Phase] Loss: 0.6817 Acc: 64.6807% Time: 7322.2151s\n",
            "#865 [Test Phase] Loss: 0.7899 Acc: 58.0974% Time: 7330.4745s\n",
            "#866 [Test Phase] Loss: 0.6906 Acc: 64.4832% Time: 7338.9883s\n",
            "#867 [Test Phase] Loss: 0.6065 Acc: 66.4253% Time: 7347.2355s\n",
            "#868 [Test Phase] Loss: 0.7788 Acc: 58.5253% Time: 7355.6502s\n",
            "#869 [Test Phase] Loss: 0.7165 Acc: 62.6070% Time: 7364.1341s\n",
            "#870 [Test Phase] Loss: 0.6195 Acc: 68.6965% Time: 7372.4773s\n",
            "#871 [Test Phase] Loss: 0.6683 Acc: 65.5365% Time: 7380.8286s\n",
            "#872 [Test Phase] Loss: 0.6935 Acc: 64.2199% Time: 7389.1115s\n",
            "#873 [Test Phase] Loss: 0.7237 Acc: 61.8499% Time: 7397.4501s\n",
            "#874 [Test Phase] Loss: 0.7357 Acc: 60.9941% Time: 7405.5959s\n",
            "#875 [Test Phase] Loss: 0.7579 Acc: 60.2699% Time: 7413.8784s\n",
            "#876 [Test Phase] Loss: 0.7155 Acc: 62.1461% Time: 7422.1833s\n",
            "#877 [Test Phase] Loss: 0.5648 Acc: 72.1856% Time: 7430.6082s\n",
            "#878 [Test Phase] Loss: 0.6089 Acc: 67.2153% Time: 7438.9105s\n",
            "#879 [Test Phase] Loss: 0.8562 Acc: 54.3120% Time: 7447.2860s\n",
            "#880 [Test Phase] Loss: 0.7278 Acc: 62.0145% Time: 7455.6479s\n",
            "#881 [Test Phase] Loss: 0.8295 Acc: 55.9250% Time: 7463.8411s\n",
            "#882 [Test Phase] Loss: 0.6451 Acc: 66.5569% Time: 7472.1814s\n",
            "#883 [Test Phase] Loss: 0.8638 Acc: 54.2462% Time: 7480.3826s\n",
            "#884 [Test Phase] Loss: 0.7359 Acc: 62.1132% Time: 7488.7594s\n",
            "#885 [Test Phase] Loss: 0.7824 Acc: 58.4924% Time: 7497.0596s\n",
            "#886 [Test Phase] Loss: 0.8160 Acc: 56.6820% Time: 7505.3828s\n",
            "#887 [Test Phase] Loss: 0.8703 Acc: 53.7525% Time: 7513.6818s\n",
            "#888 [Test Phase] Loss: 0.7791 Acc: 59.4799% Time: 7522.0948s\n",
            "#889 [Test Phase] Loss: 0.5322 Acc: 76.5306% Time: 7530.4226s\n",
            "#890 [Test Phase] Loss: 0.6586 Acc: 62.1461% Time: 7538.9017s\n",
            "#891 [Test Phase] Loss: 0.7886 Acc: 58.2291% Time: 7547.2086s\n",
            "#892 [Test Phase] Loss: 0.8604 Acc: 54.8058% Time: 7555.6481s\n",
            "#893 [Test Phase] Loss: 0.6416 Acc: 67.9394% Time: 7563.9780s\n",
            "#894 [Test Phase] Loss: 0.7732 Acc: 60.0724% Time: 7572.3660s\n",
            "#895 [Test Phase] Loss: 0.7647 Acc: 59.9408% Time: 7580.7627s\n",
            "#896 [Test Phase] Loss: 0.7688 Acc: 59.8091% Time: 7589.2820s\n",
            "#897 [Test Phase] Loss: 0.7891 Acc: 57.4391% Time: 7597.6798s\n",
            "#898 [Test Phase] Loss: 0.7449 Acc: 61.1916% Time: 7606.0152s\n",
            "#899 [Test Phase] Loss: 0.7165 Acc: 62.5082% Time: 7614.4174s\n",
            "#900 [Test Phase] Loss: 0.7186 Acc: 62.9032% Time: 7622.6646s\n",
            "#901 [Test Phase] Loss: 0.6905 Acc: 58.9862% Time: 7630.8800s\n",
            "#902 [Test Phase] Loss: 0.7662 Acc: 60.1053% Time: 7639.2477s\n",
            "#903 [Test Phase] Loss: 0.7057 Acc: 63.2982% Time: 7647.4316s\n",
            "#904 [Test Phase] Loss: 0.9242 Acc: 50.0329% Time: 7655.7694s\n",
            "#905 [Test Phase] Loss: 0.6870 Acc: 64.2857% Time: 7664.1481s\n",
            "#906 [Test Phase] Loss: 0.7160 Acc: 63.4299% Time: 7672.5353s\n",
            "#907 [Test Phase] Loss: 0.9272 Acc: 49.4404% Time: 7681.0507s\n",
            "#908 [Test Phase] Loss: 0.9236 Acc: 50.0000% Time: 7689.4459s\n",
            "#909 [Test Phase] Loss: 0.6929 Acc: 64.1870% Time: 7697.8226s\n",
            "#910 [Test Phase] Loss: 0.7394 Acc: 61.9157% Time: 7706.0879s\n",
            "#911 [Test Phase] Loss: 0.7870 Acc: 58.6570% Time: 7714.4069s\n",
            "#912 [Test Phase] Loss: 0.5779 Acc: 69.9144% Time: 7722.6838s\n",
            "#913 [Test Phase] Loss: 0.7109 Acc: 63.3311% Time: 7730.9483s\n",
            "#914 [Test Phase] Loss: 0.7206 Acc: 62.4753% Time: 7739.1488s\n",
            "#915 [Test Phase] Loss: 0.6414 Acc: 67.0507% Time: 7747.3712s\n",
            "#916 [Test Phase] Loss: 0.7833 Acc: 59.3483% Time: 7755.7370s\n",
            "#917 [Test Phase] Loss: 0.9331 Acc: 50.0000% Time: 7764.1487s\n",
            "#918 [Test Phase] Loss: 0.7238 Acc: 62.3766% Time: 7772.5428s\n",
            "#919 [Test Phase] Loss: 0.7836 Acc: 58.6241% Time: 7780.7872s\n",
            "#920 [Test Phase] Loss: 0.7162 Acc: 62.6728% Time: 7789.2100s\n",
            "#921 [Test Phase] Loss: 0.7133 Acc: 62.8703% Time: 7797.5955s\n",
            "#922 [Test Phase] Loss: 0.7752 Acc: 58.6570% Time: 7805.9161s\n",
            "#923 [Test Phase] Loss: 0.7114 Acc: 57.0770% Time: 7814.2613s\n",
            "#924 [Test Phase] Loss: 0.7340 Acc: 61.7512% Time: 7822.7058s\n",
            "#925 [Test Phase] Loss: 0.6852 Acc: 63.9895% Time: 7831.1606s\n",
            "#926 [Test Phase] Loss: 0.7787 Acc: 59.0191% Time: 7839.4359s\n",
            "#927 [Test Phase] Loss: 0.6416 Acc: 67.1494% Time: 7847.7594s\n",
            "#928 [Test Phase] Loss: 0.7586 Acc: 60.3687% Time: 7856.1728s\n",
            "#929 [Test Phase] Loss: 0.7016 Acc: 63.3641% Time: 7864.5147s\n",
            "#930 [Test Phase] Loss: 0.7585 Acc: 60.9612% Time: 7872.8689s\n",
            "#931 [Test Phase] Loss: 0.7410 Acc: 61.1587% Time: 7881.1002s\n",
            "#932 [Test Phase] Loss: 0.6997 Acc: 63.5286% Time: 7889.2594s\n",
            "#933 [Test Phase] Loss: 0.7543 Acc: 60.4016% Time: 7897.5484s\n",
            "#934 [Test Phase] Loss: 0.5436 Acc: 73.1402% Time: 7905.8865s\n",
            "#935 [Test Phase] Loss: 0.6322 Acc: 67.6761% Time: 7914.2644s\n",
            "#936 [Test Phase] Loss: 0.6583 Acc: 66.7544% Time: 7922.6894s\n",
            "#937 [Test Phase] Loss: 0.9093 Acc: 50.6583% Time: 7930.9437s\n",
            "#938 [Test Phase] Loss: 0.7596 Acc: 60.7307% Time: 7939.3698s\n",
            "#939 [Test Phase] Loss: 0.8078 Acc: 58.0316% Time: 7947.7361s\n",
            "#940 [Test Phase] Loss: 0.6653 Acc: 65.7999% Time: 7956.0912s\n",
            "#941 [Test Phase] Loss: 0.7338 Acc: 61.7841% Time: 7964.4434s\n",
            "#942 [Test Phase] Loss: 0.6689 Acc: 65.8328% Time: 7972.6969s\n",
            "#943 [Test Phase] Loss: 0.7386 Acc: 62.0803% Time: 7981.0130s\n",
            "#944 [Test Phase] Loss: 0.7172 Acc: 62.8045% Time: 7989.4655s\n",
            "#945 [Test Phase] Loss: 0.5784 Acc: 69.9144% Time: 7997.8731s\n",
            "#946 [Test Phase] Loss: 0.7284 Acc: 62.1791% Time: 8006.2691s\n",
            "#947 [Test Phase] Loss: 0.6304 Acc: 67.8078% Time: 8014.5508s\n",
            "#948 [Test Phase] Loss: 0.5604 Acc: 72.4161% Time: 8022.8038s\n",
            "#949 [Test Phase] Loss: 0.7301 Acc: 61.9157% Time: 8031.0846s\n",
            "#950 [Test Phase] Loss: 0.7384 Acc: 62.0803% Time: 8039.4611s\n",
            "#951 [Test Phase] Loss: 0.7918 Acc: 58.8216% Time: 8047.8700s\n",
            "#952 [Test Phase] Loss: 0.8142 Acc: 56.8795% Time: 8056.3011s\n",
            "#953 [Test Phase] Loss: 0.7508 Acc: 60.7637% Time: 8064.6309s\n",
            "#954 [Test Phase] Loss: 0.8019 Acc: 57.8670% Time: 8072.9533s\n",
            "#955 [Test Phase] Loss: 0.6707 Acc: 65.3061% Time: 8081.4360s\n",
            "#956 [Test Phase] Loss: 0.6773 Acc: 59.8420% Time: 8089.8518s\n",
            "#957 [Test Phase] Loss: 0.8054 Acc: 57.5708% Time: 8098.1779s\n",
            "#958 [Test Phase] Loss: 0.8477 Acc: 55.4641% Time: 8106.4042s\n",
            "#959 [Test Phase] Loss: 0.7173 Acc: 62.9691% Time: 8114.8847s\n",
            "#960 [Test Phase] Loss: 0.7427 Acc: 61.7841% Time: 8123.2399s\n",
            "#961 [Test Phase] Loss: 0.9073 Acc: 51.6458% Time: 8131.6189s\n",
            "#962 [Test Phase] Loss: 0.7370 Acc: 61.9157% Time: 8140.0739s\n",
            "#963 [Test Phase] Loss: 0.6806 Acc: 65.3061% Time: 8148.4433s\n",
            "#964 [Test Phase] Loss: 0.6931 Acc: 64.0224% Time: 8156.8031s\n",
            "#965 [Test Phase] Loss: 0.7949 Acc: 58.5912% Time: 8165.1242s\n",
            "#966 [Test Phase] Loss: 0.9083 Acc: 51.7775% Time: 8173.4698s\n",
            "#967 [Test Phase] Loss: 0.7009 Acc: 57.8341% Time: 8181.8235s\n",
            "#968 [Test Phase] Loss: 0.8024 Acc: 57.5049% Time: 8190.2700s\n",
            "#969 [Test Phase] Loss: 0.6409 Acc: 66.9190% Time: 8198.7687s\n",
            "#970 [Test Phase] Loss: 0.7361 Acc: 61.7182% Time: 8207.1937s\n",
            "#971 [Test Phase] Loss: 0.6298 Acc: 67.6432% Time: 8215.7384s\n",
            "#972 [Test Phase] Loss: 0.6845 Acc: 64.7465% Time: 8224.0564s\n",
            "#973 [Test Phase] Loss: 0.7608 Acc: 60.9282% Time: 8232.4166s\n",
            "#974 [Test Phase] Loss: 0.7236 Acc: 63.2324% Time: 8240.7720s\n",
            "#975 [Test Phase] Loss: 0.7649 Acc: 60.5991% Time: 8249.0752s\n",
            "#976 [Test Phase] Loss: 0.7422 Acc: 61.8170% Time: 8257.4287s\n",
            "#977 [Test Phase] Loss: 0.7857 Acc: 59.0520% Time: 8265.8370s\n",
            "#978 [Test Phase] Loss: 0.5765 Acc: 69.6840% Time: 8274.1958s\n",
            "#979 [Test Phase] Loss: 0.7215 Acc: 62.1791% Time: 8282.5205s\n",
            "#980 [Test Phase] Loss: 0.8341 Acc: 55.8591% Time: 8290.7946s\n",
            "#981 [Test Phase] Loss: 0.7573 Acc: 60.5003% Time: 8299.2218s\n",
            "#982 [Test Phase] Loss: 0.6471 Acc: 66.9519% Time: 8307.5808s\n",
            "#983 [Test Phase] Loss: 0.7506 Acc: 60.5662% Time: 8315.9706s\n",
            "#984 [Test Phase] Loss: 0.7904 Acc: 58.7228% Time: 8324.1713s\n",
            "#985 [Test Phase] Loss: 0.8888 Acc: 53.0612% Time: 8332.5519s\n",
            "#986 [Test Phase] Loss: 0.6197 Acc: 69.0257% Time: 8340.7130s\n",
            "#987 [Test Phase] Loss: 0.8017 Acc: 57.2087% Time: 8349.1446s\n",
            "#988 [Test Phase] Loss: 0.7668 Acc: 59.7432% Time: 8357.4045s\n",
            "#989 [Test Phase] Loss: 0.7516 Acc: 54.3779% Time: 8365.7940s\n",
            "#990 [Test Phase] Loss: 0.7636 Acc: 59.9078% Time: 8374.1331s\n",
            "#991 [Test Phase] Loss: 0.7281 Acc: 61.4549% Time: 8382.6132s\n",
            "#992 [Test Phase] Loss: 0.6953 Acc: 64.2199% Time: 8390.9944s\n",
            "#993 [Test Phase] Loss: 0.7348 Acc: 61.2245% Time: 8399.3557s\n",
            "#994 [Test Phase] Loss: 0.7532 Acc: 60.0066% Time: 8407.6096s\n",
            "#995 [Test Phase] Loss: 0.6577 Acc: 66.7874% Time: 8415.9723s\n",
            "#996 [Test Phase] Loss: 0.7015 Acc: 63.8578% Time: 8424.3228s\n",
            "#997 [Test Phase] Loss: 0.7778 Acc: 58.9203% Time: 8432.6093s\n",
            "#998 [Test Phase] Loss: 0.7125 Acc: 62.9691% Time: 8440.8762s\n",
            "#999 [Test Phase] Loss: 0.8893 Acc: 52.6004% Time: 8449.2188s\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "path = \"C:/team3/resnet/models/#22 resnet_models\"\n",
        "\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "    model.load_state_dict(torch.load(path + \"/\" + file))\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "            # print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "            # imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "    # writer1.add_scalars(\"Loss/sum\", {'epoch_loss_train': epoch_loss, 'epoch_loss_test': epoch_loss1}, epoch)\n",
        "    # writer1.add_scalars(\"Acc/sum\", {'epoch_acc_train': epoch_acc/100, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    # writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_loss_test': epoch_loss1, 'epoch_acc_train': epoch_acc/100, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#0 Loss: 0.8026 Acc: 50.9524% Time: 39.0565s\n",
        "#0 Loss: 0.8026 Acc: 50.9524% Time: 37.5618s\n",
        "#0 Loss: 0.8026 Acc: 50.9524% Time: 32.1134s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "q78FprL_lPKP"
      },
      "outputs": [],
      "source": [
        "#모델 저장\n",
        "\n",
        "torch.save(model.state_dict(),'resnet_dict.pth')\n",
        "# torch.save(model,'model.pth')\n",
        "torch.save(model,'resnet.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = models.resnet34(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "model.load_state_dict(torch.load('#2 resnet_dict.pth'))\n",
        "model.eval()\n",
        "model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4ZRz72LDZpdP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[Test Phase] Loss: 0.5954 Acc: 72.7781% Time: 9.2968s\n"
          ]
        }
      ],
      "source": [
        "# model.eval()\n",
        "start_time = time.time()\n",
        "writer = SummaryWriter()\n",
        "\n",
        "with torch.no_grad():\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "        print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "        # imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "    epoch_loss = running_loss / len(test_datasets)\n",
        "    epoch_acc = running_corrects / len(test_datasets) * 100.\n",
        "    print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc, time.time() - start_time))\n",
        "    \n",
        "    # writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    # writer.add_scalar(\"acc/train\", epoch_acc, epoch)\n",
        "    # writer.add_scalar(\"sum/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"sum/train\", epoch_acc/100, epoch)\n",
        "\n",
        "writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "[Test Phase] Loss: 1.4326 Acc: 10.5003% Time: 209.7178s\n",
        "[Test Phase] Loss: 1.4326 Acc: 10.5003% Time: 215.8944s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHorVg4-Z68P"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "image = Image.open('test_img.jpg')\n",
        "image = transforms_test(image).unsqueeze(0).to(device)\n",
        "print(image.shape)\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    print(outputs)\n",
        "    print(class_names[preds[0]])\n",
        "    imshow(image.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 저장한 모델 가져와서 쓰기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9VvK7xiaax0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"resnet_dict.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKCkA0fXmEHm"
      },
      "outputs": [],
      "source": [
        "files.download(\"resnet.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0vAUYhKpmIsR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = models.resnet34(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "model.load_state_dict(torch.load('#2 resnet_dict.pth'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwzRuBb1nzxe"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "model = model.to(device)\n",
        "image = Image.open('test.jpg')\n",
        "image = transforms_test(image).unsqueeze(0).to(device)\n",
        "print(image.shape)\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    print(outputs)\n",
        "    print(class_names[preds[0]])\n",
        "    imshow(image.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LzKPl5rLoNce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터셋 크기: 2004\n",
            "테스트 데이터셋 크기: 3038\n",
            "클래스: ['0', '1']\n",
            "#0 Loss: 0.6915 Acc: 52.6946% Time: 48.1336s\n",
            "#1 Loss: 0.6880 Acc: 54.7904% Time: 97.0300s\n",
            "#2 Loss: 0.6845 Acc: 55.6886% Time: 145.6909s\n",
            "#3 Loss: 0.6767 Acc: 58.0339% Time: 193.9887s\n",
            "#4 Loss: 0.6671 Acc: 58.5329% Time: 242.2940s\n",
            "#5 Loss: 0.6587 Acc: 60.5289% Time: 290.4696s\n",
            "#6 Loss: 0.6583 Acc: 61.5768% Time: 339.0127s\n",
            "#7 Loss: 0.6476 Acc: 62.5749% Time: 387.4094s\n",
            "#8 Loss: 0.6478 Acc: 62.7744% Time: 435.7222s\n",
            "#9 Loss: 0.6409 Acc: 63.2735% Time: 484.0915s\n",
            "#10 Loss: 0.6344 Acc: 64.7206% Time: 532.4853s\n",
            "#11 Loss: 0.6207 Acc: 65.7186% Time: 580.8902s\n",
            "#12 Loss: 0.6198 Acc: 65.8184% Time: 629.2581s\n",
            "#13 Loss: 0.5996 Acc: 68.6128% Time: 677.4193s\n",
            "#14 Loss: 0.5886 Acc: 69.0120% Time: 725.5995s\n",
            "#15 Loss: 0.5767 Acc: 69.5110% Time: 773.9869s\n",
            "#16 Loss: 0.5604 Acc: 70.4591% Time: 822.3404s\n",
            "#17 Loss: 0.5368 Acc: 73.8523% Time: 870.7910s\n",
            "#18 Loss: 0.5094 Acc: 75.5489% Time: 919.1789s\n",
            "#19 Loss: 0.4817 Acc: 76.1477% Time: 967.3650s\n",
            "#20 Loss: 0.4526 Acc: 79.5908% Time: 1015.7216s\n",
            "#21 Loss: 0.4177 Acc: 80.8882% Time: 1064.1038s\n",
            "#22 Loss: 0.3700 Acc: 84.6806% Time: 1112.4452s\n",
            "#23 Loss: 0.3191 Acc: 86.7764% Time: 1160.7983s\n",
            "#24 Loss: 0.2761 Acc: 88.5729% Time: 1209.1433s\n",
            "#25 Loss: 0.2299 Acc: 91.5170% Time: 1257.4112s\n",
            "#26 Loss: 0.1961 Acc: 92.1158% Time: 1305.6732s\n",
            "#27 Loss: 0.1811 Acc: 93.3633% Time: 1354.0273s\n",
            "#28 Loss: 0.1196 Acc: 96.4072% Time: 1402.3957s\n",
            "#29 Loss: 0.1110 Acc: 96.3573% Time: 1450.6260s\n",
            "#30 Loss: 0.0766 Acc: 97.9541% Time: 1498.9907s\n",
            "#31 Loss: 0.0627 Acc: 97.6048% Time: 1547.3718s\n",
            "#32 Loss: 0.0479 Acc: 98.5529% Time: 1595.7641s\n",
            "#33 Loss: 0.0457 Acc: 98.7525% Time: 1644.1402s\n",
            "#34 Loss: 0.0434 Acc: 98.5030% Time: 1692.4779s\n",
            "#35 Loss: 0.0292 Acc: 99.3014% Time: 1740.8774s\n",
            "#36 Loss: 0.0173 Acc: 99.9002% Time: 1789.2670s\n",
            "#37 Loss: 0.0283 Acc: 99.2016% Time: 1837.4803s\n",
            "#38 Loss: 0.0162 Acc: 99.6507% Time: 1885.8920s\n",
            "#39 Loss: 0.0258 Acc: 99.3014% Time: 1934.3119s\n",
            "#40 Loss: 0.0190 Acc: 99.5509% Time: 1982.6269s\n",
            "#41 Loss: 0.0096 Acc: 99.8503% Time: 2030.9978s\n",
            "#42 Loss: 0.0093 Acc: 99.8503% Time: 2079.4010s\n",
            "#43 Loss: 0.0064 Acc: 99.9501% Time: 2127.8058s\n",
            "#44 Loss: 0.0050 Acc: 99.9501% Time: 2176.1886s\n",
            "#45 Loss: 0.0055 Acc: 99.9002% Time: 2224.6484s\n",
            "#46 Loss: 0.0027 Acc: 100.0000% Time: 2272.9357s\n",
            "#47 Loss: 0.0028 Acc: 100.0000% Time: 2321.3480s\n",
            "#48 Loss: 0.0044 Acc: 99.9002% Time: 2369.7606s\n",
            "#49 Loss: 0.0024 Acc: 100.0000% Time: 2418.1384s\n",
            "#50 Loss: 0.0054 Acc: 99.8503% Time: 2466.4563s\n",
            "#51 Loss: 0.0088 Acc: 99.7505% Time: 2514.7936s\n",
            "#52 Loss: 0.0023 Acc: 100.0000% Time: 2563.0453s\n",
            "#53 Loss: 0.0032 Acc: 100.0000% Time: 2611.3911s\n",
            "#54 Loss: 0.0020 Acc: 100.0000% Time: 2659.7527s\n",
            "#55 Loss: 0.0019 Acc: 100.0000% Time: 2708.1488s\n",
            "#56 Loss: 0.0030 Acc: 99.9002% Time: 2756.5907s\n",
            "#57 Loss: 0.0035 Acc: 99.9002% Time: 2804.9760s\n",
            "#58 Loss: 0.0022 Acc: 100.0000% Time: 2853.3353s\n",
            "#59 Loss: 0.0021 Acc: 99.9501% Time: 2901.7336s\n",
            "#60 Loss: 0.0021 Acc: 100.0000% Time: 2950.1304s\n",
            "#61 Loss: 0.0014 Acc: 100.0000% Time: 2998.5590s\n",
            "#62 Loss: 0.0010 Acc: 100.0000% Time: 3046.9093s\n",
            "#63 Loss: 0.0009 Acc: 100.0000% Time: 3095.2492s\n",
            "#64 Loss: 0.0005 Acc: 100.0000% Time: 3143.5792s\n",
            "#65 Loss: 0.0007 Acc: 100.0000% Time: 3191.8776s\n",
            "#66 Loss: 0.0007 Acc: 100.0000% Time: 3240.2227s\n",
            "#67 Loss: 0.0008 Acc: 100.0000% Time: 3288.5729s\n",
            "#68 Loss: 0.0011 Acc: 100.0000% Time: 3336.9950s\n",
            "#69 Loss: 0.0007 Acc: 100.0000% Time: 3385.4163s\n",
            "#70 Loss: 0.0171 Acc: 99.5010% Time: 3433.6644s\n",
            "#71 Loss: 0.0058 Acc: 99.8503% Time: 3481.9234s\n",
            "#72 Loss: 0.0055 Acc: 99.8503% Time: 3530.3401s\n",
            "#73 Loss: 0.0021 Acc: 100.0000% Time: 3578.7078s\n",
            "#74 Loss: 0.0011 Acc: 100.0000% Time: 3627.1069s\n",
            "#75 Loss: 0.0010 Acc: 100.0000% Time: 3675.5251s\n",
            "#76 Loss: 0.0032 Acc: 99.9002% Time: 3723.9043s\n",
            "#77 Loss: 0.0061 Acc: 99.8503% Time: 3772.2700s\n",
            "#78 Loss: 0.0033 Acc: 99.9501% Time: 3820.7208s\n",
            "#79 Loss: 0.0023 Acc: 99.9501% Time: 3869.0275s\n",
            "#80 Loss: 0.0036 Acc: 99.9002% Time: 3917.4056s\n",
            "#81 Loss: 0.0016 Acc: 100.0000% Time: 3965.8047s\n",
            "#82 Loss: 0.0061 Acc: 99.7505% Time: 4014.1972s\n",
            "#83 Loss: 0.0014 Acc: 100.0000% Time: 4062.5476s\n",
            "#84 Loss: 0.0006 Acc: 100.0000% Time: 4110.8933s\n",
            "#85 Loss: 0.0031 Acc: 99.9501% Time: 4159.1531s\n",
            "#86 Loss: 0.0011 Acc: 100.0000% Time: 4207.5679s\n",
            "#87 Loss: 0.0048 Acc: 99.8503% Time: 4255.9131s\n",
            "#88 Loss: 0.0008 Acc: 100.0000% Time: 4304.3282s\n",
            "#89 Loss: 0.0013 Acc: 100.0000% Time: 4352.7239s\n",
            "#90 Loss: 0.0007 Acc: 100.0000% Time: 4401.0384s\n",
            "#91 Loss: 0.0004 Acc: 100.0000% Time: 4449.4313s\n",
            "#92 Loss: 0.0004 Acc: 100.0000% Time: 4497.7706s\n",
            "#93 Loss: 0.0004 Acc: 100.0000% Time: 4545.9492s\n",
            "#94 Loss: 0.0006 Acc: 100.0000% Time: 4594.2127s\n",
            "#95 Loss: 0.0005 Acc: 100.0000% Time: 4642.5982s\n",
            "#96 Loss: 0.0003 Acc: 100.0000% Time: 4691.0518s\n",
            "#97 Loss: 0.0002 Acc: 100.0000% Time: 4739.3427s\n",
            "#98 Loss: 0.0003 Acc: 100.0000% Time: 4787.7291s\n",
            "#99 Loss: 0.0004 Acc: 100.0000% Time: 4836.1088s\n",
            "#100 Loss: 0.0007 Acc: 100.0000% Time: 4884.5057s\n",
            "#101 Loss: 0.0008 Acc: 100.0000% Time: 4932.9141s\n",
            "#102 Loss: 0.0010 Acc: 100.0000% Time: 4981.3218s\n",
            "#103 Loss: 0.0004 Acc: 100.0000% Time: 5029.6254s\n",
            "#104 Loss: 0.0004 Acc: 100.0000% Time: 5077.9900s\n",
            "#105 Loss: 0.0003 Acc: 100.0000% Time: 5126.2490s\n",
            "#106 Loss: 0.0003 Acc: 100.0000% Time: 5174.6198s\n",
            "#107 Loss: 0.0002 Acc: 100.0000% Time: 5223.0218s\n",
            "#108 Loss: 0.0003 Acc: 100.0000% Time: 5271.3877s\n",
            "#109 Loss: 0.0003 Acc: 100.0000% Time: 5319.7257s\n",
            "#110 Loss: 0.0003 Acc: 100.0000% Time: 5368.1781s\n",
            "#111 Loss: 0.0002 Acc: 100.0000% Time: 5416.5253s\n",
            "#112 Loss: 0.0002 Acc: 100.0000% Time: 5464.6887s\n",
            "#113 Loss: 0.0002 Acc: 100.0000% Time: 5513.1326s\n",
            "#114 Loss: 0.0002 Acc: 100.0000% Time: 5561.4574s\n",
            "#115 Loss: 0.0007 Acc: 100.0000% Time: 5609.6753s\n",
            "#116 Loss: 0.0006 Acc: 100.0000% Time: 5658.0473s\n",
            "#117 Loss: 0.0002 Acc: 100.0000% Time: 5706.4543s\n",
            "#118 Loss: 0.0002 Acc: 100.0000% Time: 5754.8249s\n",
            "#119 Loss: 0.0002 Acc: 100.0000% Time: 5803.2252s\n",
            "#120 Loss: 0.0002 Acc: 100.0000% Time: 5851.5972s\n",
            "#121 Loss: 0.0002 Acc: 100.0000% Time: 5899.9883s\n",
            "#122 Loss: 0.0001 Acc: 100.0000% Time: 5948.3374s\n",
            "#123 Loss: 0.0005 Acc: 100.0000% Time: 5996.7083s\n",
            "#124 Loss: 0.0002 Acc: 100.0000% Time: 6045.1519s\n",
            "#125 Loss: 0.0001 Acc: 100.0000% Time: 6093.5552s\n",
            "#126 Loss: 0.0004 Acc: 100.0000% Time: 6141.9729s\n",
            "#127 Loss: 0.0002 Acc: 100.0000% Time: 6190.2619s\n",
            "#128 Loss: 0.0002 Acc: 100.0000% Time: 6238.4330s\n",
            "#129 Loss: 0.0003 Acc: 100.0000% Time: 6286.8653s\n",
            "#130 Loss: 0.0002 Acc: 100.0000% Time: 6335.2011s\n",
            "#131 Loss: 0.0002 Acc: 100.0000% Time: 6383.5639s\n",
            "#132 Loss: 0.0003 Acc: 100.0000% Time: 6431.7955s\n",
            "#133 Loss: 0.0003 Acc: 100.0000% Time: 6480.1108s\n",
            "#134 Loss: 0.0001 Acc: 100.0000% Time: 6528.3938s\n",
            "#135 Loss: 0.0001 Acc: 100.0000% Time: 6576.7921s\n",
            "#136 Loss: 0.0001 Acc: 100.0000% Time: 6625.1767s\n",
            "#137 Loss: 0.0001 Acc: 100.0000% Time: 6673.5246s\n",
            "#138 Loss: 0.0001 Acc: 100.0000% Time: 6721.8899s\n",
            "#139 Loss: 0.0002 Acc: 100.0000% Time: 6770.2925s\n",
            "#140 Loss: 0.0001 Acc: 100.0000% Time: 6818.7288s\n",
            "#141 Loss: 0.0001 Acc: 100.0000% Time: 6867.0889s\n",
            "#142 Loss: 0.0002 Acc: 100.0000% Time: 6915.4818s\n",
            "#143 Loss: 0.0001 Acc: 100.0000% Time: 6963.8251s\n",
            "#144 Loss: 0.0001 Acc: 100.0000% Time: 7012.2099s\n",
            "#145 Loss: 0.0002 Acc: 100.0000% Time: 7060.6071s\n",
            "#146 Loss: 0.0001 Acc: 100.0000% Time: 7108.7975s\n",
            "#147 Loss: 0.0001 Acc: 100.0000% Time: 7157.1641s\n",
            "#148 Loss: 0.0001 Acc: 100.0000% Time: 7205.5272s\n",
            "#149 Loss: 0.0001 Acc: 100.0000% Time: 7253.8708s\n",
            "#150 Loss: 0.0002 Acc: 100.0000% Time: 7302.2796s\n",
            "#151 Loss: 0.0002 Acc: 100.0000% Time: 7350.5572s\n",
            "#152 Loss: 0.0001 Acc: 100.0000% Time: 7398.8976s\n",
            "#153 Loss: 0.0001 Acc: 100.0000% Time: 7447.2782s\n",
            "#154 Loss: 0.0001 Acc: 100.0000% Time: 7495.7246s\n",
            "#155 Loss: 0.0003 Acc: 100.0000% Time: 7544.0514s\n",
            "#156 Loss: 0.0001 Acc: 100.0000% Time: 7592.2597s\n",
            "#157 Loss: 0.0001 Acc: 100.0000% Time: 7640.6229s\n",
            "#158 Loss: 0.0001 Acc: 100.0000% Time: 7689.0272s\n",
            "#159 Loss: 0.0001 Acc: 100.0000% Time: 7737.4045s\n",
            "#160 Loss: 0.0001 Acc: 100.0000% Time: 7785.8128s\n",
            "#161 Loss: 0.0001 Acc: 100.0000% Time: 7834.0947s\n",
            "#162 Loss: 0.0001 Acc: 100.0000% Time: 7882.3602s\n",
            "#163 Loss: 0.0001 Acc: 100.0000% Time: 7930.7066s\n",
            "#164 Loss: 0.0001 Acc: 100.0000% Time: 7979.0775s\n",
            "#165 Loss: 0.0001 Acc: 100.0000% Time: 8027.4705s\n",
            "#166 Loss: 0.0001 Acc: 100.0000% Time: 8075.8412s\n",
            "#167 Loss: 0.0001 Acc: 100.0000% Time: 8124.0846s\n",
            "#168 Loss: 0.0020 Acc: 99.9002% Time: 8172.3780s\n",
            "#169 Loss: 0.0003 Acc: 100.0000% Time: 8220.6881s\n",
            "#170 Loss: 0.0002 Acc: 100.0000% Time: 8269.0238s\n",
            "#171 Loss: 0.0003 Acc: 100.0000% Time: 8317.4371s\n",
            "#172 Loss: 0.0002 Acc: 100.0000% Time: 8365.8062s\n",
            "#173 Loss: 0.0001 Acc: 100.0000% Time: 8414.1185s\n",
            "#174 Loss: 0.0001 Acc: 100.0000% Time: 8462.3642s\n",
            "#175 Loss: 0.0003 Acc: 100.0000% Time: 8510.7374s\n",
            "#176 Loss: 0.0001 Acc: 100.0000% Time: 8559.0994s\n",
            "#177 Loss: 0.0001 Acc: 100.0000% Time: 8607.4963s\n",
            "#178 Loss: 0.0002 Acc: 100.0000% Time: 8655.9294s\n",
            "#179 Loss: 0.0001 Acc: 100.0000% Time: 8704.3283s\n",
            "#180 Loss: 0.0001 Acc: 100.0000% Time: 8752.5956s\n",
            "#181 Loss: 0.0001 Acc: 100.0000% Time: 8800.8423s\n",
            "#182 Loss: 0.0001 Acc: 100.0000% Time: 8849.2156s\n",
            "#183 Loss: 0.0001 Acc: 100.0000% Time: 8897.6289s\n",
            "#184 Loss: 0.0001 Acc: 100.0000% Time: 8946.0083s\n",
            "#185 Loss: 0.0001 Acc: 100.0000% Time: 8994.1898s\n",
            "#186 Loss: 0.0001 Acc: 100.0000% Time: 9042.5778s\n",
            "#187 Loss: 0.0001 Acc: 100.0000% Time: 9090.9158s\n",
            "#188 Loss: 0.0001 Acc: 100.0000% Time: 9139.3310s\n",
            "#189 Loss: 0.0001 Acc: 100.0000% Time: 9187.7729s\n",
            "#190 Loss: 0.0001 Acc: 100.0000% Time: 9236.0797s\n",
            "#191 Loss: 0.0001 Acc: 100.0000% Time: 9284.4829s\n",
            "#192 Loss: 0.0001 Acc: 100.0000% Time: 9332.7751s\n",
            "#193 Loss: 0.0001 Acc: 100.0000% Time: 9381.0331s\n",
            "#194 Loss: 0.0001 Acc: 100.0000% Time: 9429.3811s\n",
            "#195 Loss: 0.0001 Acc: 100.0000% Time: 9477.6474s\n",
            "#196 Loss: 0.0001 Acc: 100.0000% Time: 9526.0468s\n",
            "#197 Loss: 0.0001 Acc: 100.0000% Time: 9574.4215s\n",
            "#198 Loss: 0.0002 Acc: 100.0000% Time: 9622.7458s\n",
            "#199 Loss: 0.0001 Acc: 100.0000% Time: 9671.0764s\n",
            "#0 [Test Phase] Loss: 0.6965 Acc: 49.6379% Time: 29.3867s\n",
            "#1 [Test Phase] Loss: 0.7005 Acc: 48.4858% Time: 58.7813s\n",
            "#2 [Test Phase] Loss: 0.6202 Acc: 62.4424% Time: 88.0257s\n",
            "#3 [Test Phase] Loss: 2.3460 Acc: 68.4332% Time: 117.3833s\n",
            "#4 [Test Phase] Loss: 2.2769 Acc: 70.3752% Time: 146.7923s\n",
            "#5 [Test Phase] Loss: 2.0719 Acc: 72.0540% Time: 176.1783s\n",
            "#6 [Test Phase] Loss: 2.1447 Acc: 71.2640% Time: 205.5049s\n",
            "#7 [Test Phase] Loss: 2.2021 Acc: 70.5727% Time: 234.7295s\n",
            "#8 [Test Phase] Loss: 2.0323 Acc: 71.8894% Time: 264.1278s\n",
            "#9 [Test Phase] Loss: 2.1490 Acc: 70.7373% Time: 293.5609s\n",
            "#10 [Test Phase] Loss: 2.0951 Acc: 71.7577% Time: 322.9316s\n",
            "#11 [Test Phase] Loss: 2.0455 Acc: 72.1856% Time: 352.3128s\n",
            "#12 [Test Phase] Loss: 2.1965 Acc: 70.7702% Time: 381.6111s\n",
            "#13 [Test Phase] Loss: 0.6048 Acc: 64.6149% Time: 410.7831s\n",
            "#14 [Test Phase] Loss: 2.2552 Acc: 70.5727% Time: 440.0156s\n",
            "#15 [Test Phase] Loss: 2.3097 Acc: 70.3423% Time: 469.2480s\n",
            "#16 [Test Phase] Loss: 2.3240 Acc: 70.4411% Time: 498.5674s\n",
            "#17 [Test Phase] Loss: 2.2531 Acc: 71.1982% Time: 527.8912s\n",
            "#18 [Test Phase] Loss: 2.3605 Acc: 70.0790% Time: 557.2666s\n",
            "#19 [Test Phase] Loss: 2.9388 Acc: 65.8328% Time: 586.5647s\n",
            "#20 [Test Phase] Loss: 2.4480 Acc: 69.9473% Time: 615.7852s\n",
            "#21 [Test Phase] Loss: 2.2264 Acc: 71.9223% Time: 645.1157s\n",
            "#22 [Test Phase] Loss: 2.2584 Acc: 71.7248% Time: 674.4191s\n",
            "#23 [Test Phase] Loss: 2.3563 Acc: 71.0007% Time: 703.7894s\n",
            "#24 [Test Phase] Loss: 0.6581 Acc: 58.5583% Time: 733.1740s\n",
            "#25 [Test Phase] Loss: 2.2867 Acc: 71.9881% Time: 762.5602s\n",
            "#26 [Test Phase] Loss: 2.3979 Acc: 70.8361% Time: 791.9539s\n",
            "#27 [Test Phase] Loss: 2.5282 Acc: 69.5853% Time: 821.3557s\n",
            "#28 [Test Phase] Loss: 2.3779 Acc: 70.9019% Time: 850.7432s\n",
            "#29 [Test Phase] Loss: 2.2471 Acc: 72.1198% Time: 880.1630s\n",
            "#30 [Test Phase] Loss: 2.5336 Acc: 70.0132% Time: 909.5546s\n",
            "#31 [Test Phase] Loss: 2.6524 Acc: 68.1369% Time: 938.8791s\n",
            "#32 [Test Phase] Loss: 2.1397 Acc: 72.6794% Time: 968.2190s\n",
            "#33 [Test Phase] Loss: 2.2255 Acc: 72.2844% Time: 997.7050s\n",
            "#34 [Test Phase] Loss: 2.3290 Acc: 70.9677% Time: 1027.1152s\n",
            "#35 [Test Phase] Loss: 0.8179 Acc: 44.4371% Time: 1056.5008s\n",
            "#36 [Test Phase] Loss: 2.4106 Acc: 70.2765% Time: 1085.9056s\n",
            "#37 [Test Phase] Loss: 2.3966 Acc: 70.6386% Time: 1115.2849s\n",
            "#38 [Test Phase] Loss: 2.5685 Acc: 68.8940% Time: 1144.6648s\n",
            "#39 [Test Phase] Loss: 2.8279 Acc: 66.8203% Time: 1174.0377s\n",
            "#40 [Test Phase] Loss: 2.6394 Acc: 68.6965% Time: 1203.3914s\n",
            "#41 [Test Phase] Loss: 2.4894 Acc: 69.7828% Time: 1232.7322s\n",
            "#42 [Test Phase] Loss: 2.4880 Acc: 69.9473% Time: 1261.9497s\n",
            "#43 [Test Phase] Loss: 2.4962 Acc: 69.9473% Time: 1291.3192s\n",
            "#44 [Test Phase] Loss: 2.5188 Acc: 69.8815% Time: 1320.5257s\n",
            "#45 [Test Phase] Loss: 2.8335 Acc: 67.0507% Time: 1349.9725s\n",
            "#46 [Test Phase] Loss: 0.5766 Acc: 68.5648% Time: 1379.3708s\n",
            "#47 [Test Phase] Loss: 2.6975 Acc: 68.5319% Time: 1408.7415s\n",
            "#48 [Test Phase] Loss: 2.5675 Acc: 69.3548% Time: 1438.1109s\n",
            "#49 [Test Phase] Loss: 2.7432 Acc: 67.8078% Time: 1467.5331s\n",
            "#50 [Test Phase] Loss: 2.4913 Acc: 70.2436% Time: 1496.9566s\n",
            "#51 [Test Phase] Loss: 2.4160 Acc: 70.8690% Time: 1526.2602s\n",
            "#52 [Test Phase] Loss: 2.7060 Acc: 67.9724% Time: 1555.4668s\n",
            "#53 [Test Phase] Loss: 2.4222 Acc: 70.4411% Time: 1584.8005s\n",
            "#54 [Test Phase] Loss: 2.5456 Acc: 69.3877% Time: 1614.2207s\n",
            "#55 [Test Phase] Loss: 2.5605 Acc: 69.5523% Time: 1643.5184s\n",
            "#56 [Test Phase] Loss: 2.4673 Acc: 70.3094% Time: 1672.8129s\n",
            "#57 [Test Phase] Loss: 0.6142 Acc: 64.8453% Time: 1702.0221s\n",
            "#58 [Test Phase] Loss: 2.1995 Acc: 72.3502% Time: 1731.2085s\n",
            "#59 [Test Phase] Loss: 2.3298 Acc: 71.5273% Time: 1760.3926s\n",
            "#60 [Test Phase] Loss: 2.5124 Acc: 69.9144% Time: 1789.6736s\n",
            "#61 [Test Phase] Loss: 2.4769 Acc: 70.2436% Time: 1819.0277s\n",
            "#62 [Test Phase] Loss: 2.5661 Acc: 69.4536% Time: 1848.4434s\n",
            "#63 [Test Phase] Loss: 2.2631 Acc: 71.9552% Time: 1877.7573s\n",
            "#64 [Test Phase] Loss: 2.3726 Acc: 71.1652% Time: 1907.1953s\n",
            "#65 [Test Phase] Loss: 2.5275 Acc: 70.0132% Time: 1936.3959s\n",
            "#66 [Test Phase] Loss: 2.6659 Acc: 68.6307% Time: 1965.7963s\n",
            "#67 [Test Phase] Loss: 2.7228 Acc: 68.2357% Time: 1995.1981s\n",
            "#68 [Test Phase] Loss: 0.6055 Acc: 65.1745% Time: 2024.5600s\n",
            "#69 [Test Phase] Loss: 2.5306 Acc: 70.0461% Time: 2053.9760s\n",
            "#70 [Test Phase] Loss: 2.6610 Acc: 68.9598% Time: 2083.3321s\n",
            "#71 [Test Phase] Loss: 2.6284 Acc: 69.3219% Time: 2112.7479s\n",
            "#72 [Test Phase] Loss: 2.6204 Acc: 69.5853% Time: 2142.1130s\n",
            "#73 [Test Phase] Loss: 2.5375 Acc: 70.3752% Time: 2171.4273s\n",
            "#74 [Test Phase] Loss: 2.5896 Acc: 69.8486% Time: 2200.7198s\n",
            "#75 [Test Phase] Loss: 2.6064 Acc: 69.7828% Time: 2230.1140s\n",
            "#76 [Test Phase] Loss: 2.4652 Acc: 71.1982% Time: 2259.4908s\n",
            "#77 [Test Phase] Loss: 2.4991 Acc: 68.6636% Time: 2288.7951s\n",
            "#78 [Test Phase] Loss: 2.4759 Acc: 69.7498% Time: 2318.0278s\n",
            "#79 [Test Phase] Loss: 0.6010 Acc: 66.9190% Time: 2347.4448s\n",
            "#80 [Test Phase] Loss: 2.4331 Acc: 70.3094% Time: 2376.8511s\n",
            "#81 [Test Phase] Loss: 3.3045 Acc: 63.0349% Time: 2406.2391s\n",
            "#82 [Test Phase] Loss: 2.6064 Acc: 68.8611% Time: 2435.6234s\n",
            "#83 [Test Phase] Loss: 2.4270 Acc: 70.7044% Time: 2464.9947s\n",
            "#84 [Test Phase] Loss: 2.6365 Acc: 68.6965% Time: 2494.3514s\n",
            "#85 [Test Phase] Loss: 2.1329 Acc: 72.8769% Time: 2523.7833s\n",
            "#86 [Test Phase] Loss: 2.5179 Acc: 69.9473% Time: 2553.1947s\n",
            "#87 [Test Phase] Loss: 2.4882 Acc: 70.7702% Time: 2582.5924s\n",
            "#88 [Test Phase] Loss: 2.9495 Acc: 67.3799% Time: 2611.9587s\n",
            "#89 [Test Phase] Loss: 2.7100 Acc: 69.2232% Time: 2641.2614s\n",
            "#90 [Test Phase] Loss: 0.9020 Acc: 49.6379% Time: 2670.6017s\n",
            "#91 [Test Phase] Loss: 2.7221 Acc: 68.8611% Time: 2700.1494s\n",
            "#92 [Test Phase] Loss: 2.7060 Acc: 69.0257% Time: 2729.5332s\n",
            "#93 [Test Phase] Loss: 2.7072 Acc: 69.0586% Time: 2758.9047s\n",
            "#94 [Test Phase] Loss: 2.6265 Acc: 69.9144% Time: 2788.3066s\n",
            "#95 [Test Phase] Loss: 2.8407 Acc: 68.1040% Time: 2817.7386s\n",
            "#96 [Test Phase] Loss: 2.8543 Acc: 67.8078% Time: 2847.1187s\n",
            "#97 [Test Phase] Loss: 2.6491 Acc: 70.0132% Time: 2876.5090s\n",
            "#98 [Test Phase] Loss: 2.7439 Acc: 68.9598% Time: 2905.8811s\n",
            "#99 [Test Phase] Loss: 2.5977 Acc: 70.1119% Time: 2935.2969s\n",
            "#100 [Test Phase] Loss: 2.5865 Acc: 70.4082% Time: 2964.6853s\n",
            "#101 [Test Phase] Loss: 0.3623 Acc: 86.0105% Time: 2994.0944s\n",
            "#102 [Test Phase] Loss: 2.7691 Acc: 68.7953% Time: 3023.4367s\n",
            "#103 [Test Phase] Loss: 2.5168 Acc: 71.0336% Time: 3052.6661s\n",
            "#104 [Test Phase] Loss: 2.7232 Acc: 69.2890% Time: 3082.0053s\n",
            "#105 [Test Phase] Loss: 2.6417 Acc: 70.0461% Time: 3111.4215s\n",
            "#106 [Test Phase] Loss: 2.6878 Acc: 69.8157% Time: 3140.7578s\n",
            "#107 [Test Phase] Loss: 2.5902 Acc: 70.4082% Time: 3169.9970s\n",
            "#108 [Test Phase] Loss: 2.6075 Acc: 70.4082% Time: 3199.3477s\n",
            "#109 [Test Phase] Loss: 2.6266 Acc: 70.0461% Time: 3228.6392s\n",
            "#110 [Test Phase] Loss: 2.2883 Acc: 73.2719% Time: 3257.9632s\n",
            "#111 [Test Phase] Loss: 2.5264 Acc: 71.1982% Time: 3287.3365s\n",
            "#112 [Test Phase] Loss: 0.6802 Acc: 56.7808% Time: 3316.6557s\n",
            "#113 [Test Phase] Loss: 0.6614 Acc: 64.4174% Time: 3345.8580s\n",
            "#114 [Test Phase] Loss: 1.0109 Acc: 49.4404% Time: 3375.0767s\n",
            "#115 [Test Phase] Loss: 0.9069 Acc: 55.9250% Time: 3404.2616s\n",
            "#116 [Test Phase] Loss: 0.4219 Acc: 81.7972% Time: 3433.4259s\n",
            "#117 [Test Phase] Loss: 0.8240 Acc: 65.3061% Time: 3462.6353s\n",
            "#118 [Test Phase] Loss: 1.1553 Acc: 54.0816% Time: 3491.8318s\n",
            "#119 [Test Phase] Loss: 0.5017 Acc: 80.8097% Time: 3521.0509s\n",
            "#120 [Test Phase] Loss: 0.6618 Acc: 75.6419% Time: 3550.3495s\n",
            "#121 [Test Phase] Loss: 0.6953 Acc: 75.0823% Time: 3579.6988s\n",
            "#122 [Test Phase] Loss: 1.2027 Acc: 63.8907% Time: 3609.0347s\n",
            "#123 [Test Phase] Loss: 0.6797 Acc: 55.7933% Time: 3638.3190s\n",
            "#124 [Test Phase] Loss: 1.2331 Acc: 66.6557% Time: 3667.6795s\n",
            "#125 [Test Phase] Loss: 1.3007 Acc: 65.6024% Time: 3696.8960s\n",
            "#126 [Test Phase] Loss: 1.6395 Acc: 63.8578% Time: 3726.1270s\n",
            "#127 [Test Phase] Loss: 1.2712 Acc: 69.5853% Time: 3755.3385s\n",
            "#128 [Test Phase] Loss: 1.2335 Acc: 69.4207% Time: 3784.6560s\n",
            "#129 [Test Phase] Loss: 1.4359 Acc: 67.8078% Time: 3813.9207s\n",
            "#130 [Test Phase] Loss: 1.6970 Acc: 66.4582% Time: 3843.2442s\n",
            "#131 [Test Phase] Loss: 1.3880 Acc: 70.9019% Time: 3872.4701s\n",
            "#132 [Test Phase] Loss: 1.8464 Acc: 65.0757% Time: 3901.6644s\n",
            "#133 [Test Phase] Loss: 1.1222 Acc: 72.9427% Time: 3930.8494s\n",
            "#134 [Test Phase] Loss: 0.7771 Acc: 39.5655% Time: 3960.2795s\n",
            "#135 [Test Phase] Loss: 1.4069 Acc: 70.5727% Time: 3989.7037s\n",
            "#136 [Test Phase] Loss: 1.6647 Acc: 67.8407% Time: 4018.9791s\n",
            "#137 [Test Phase] Loss: 1.3553 Acc: 73.5681% Time: 4048.3189s\n",
            "#138 [Test Phase] Loss: 1.7675 Acc: 67.5773% Time: 4077.6998s\n",
            "#139 [Test Phase] Loss: 1.6915 Acc: 69.8486% Time: 4107.0460s\n",
            "#140 [Test Phase] Loss: 1.8173 Acc: 68.9928% Time: 4136.3825s\n",
            "#141 [Test Phase] Loss: 1.8770 Acc: 69.3877% Time: 4165.6982s\n",
            "#142 [Test Phase] Loss: 1.8924 Acc: 68.7623% Time: 4195.0615s\n",
            "#143 [Test Phase] Loss: 1.4075 Acc: 74.3910% Time: 4224.3020s\n",
            "#144 [Test Phase] Loss: 2.0571 Acc: 68.1369% Time: 4253.6509s\n",
            "#145 [Test Phase] Loss: 0.7185 Acc: 49.0125% Time: 4282.8399s\n",
            "#146 [Test Phase] Loss: 2.8115 Acc: 59.6445% Time: 4312.1706s\n",
            "#147 [Test Phase] Loss: 2.2007 Acc: 65.4378% Time: 4341.3560s\n",
            "#148 [Test Phase] Loss: 1.9850 Acc: 69.1573% Time: 4370.6097s\n",
            "#149 [Test Phase] Loss: 1.9053 Acc: 69.8486% Time: 4400.0184s\n",
            "#150 [Test Phase] Loss: 2.2239 Acc: 67.0507% Time: 4429.3236s\n",
            "#151 [Test Phase] Loss: 1.9913 Acc: 70.2436% Time: 4458.5897s\n",
            "#152 [Test Phase] Loss: 1.9696 Acc: 69.6511% Time: 4487.9454s\n",
            "#153 [Test Phase] Loss: 2.0157 Acc: 69.7498% Time: 4517.2522s\n",
            "#154 [Test Phase] Loss: 2.0556 Acc: 68.6965% Time: 4546.5517s\n",
            "#155 [Test Phase] Loss: 3.1133 Acc: 58.4595% Time: 4575.9421s\n",
            "#156 [Test Phase] Loss: 0.6637 Acc: 56.2212% Time: 4605.2747s\n",
            "#157 [Test Phase] Loss: 1.7882 Acc: 71.9881% Time: 4634.5466s\n",
            "#158 [Test Phase] Loss: 2.1113 Acc: 68.9928% Time: 4663.8920s\n",
            "#159 [Test Phase] Loss: 2.0749 Acc: 70.3094% Time: 4693.1046s\n",
            "#160 [Test Phase] Loss: 2.2388 Acc: 68.6307% Time: 4722.5261s\n",
            "#161 [Test Phase] Loss: 2.3160 Acc: 68.1698% Time: 4751.7791s\n",
            "#162 [Test Phase] Loss: 2.3864 Acc: 67.8407% Time: 4781.0055s\n",
            "#163 [Test Phase] Loss: 2.5374 Acc: 66.0632% Time: 4810.2737s\n",
            "#164 [Test Phase] Loss: 2.0689 Acc: 71.3298% Time: 4839.6443s\n",
            "#165 [Test Phase] Loss: 2.1021 Acc: 71.1652% Time: 4868.8671s\n",
            "#166 [Test Phase] Loss: 2.1459 Acc: 70.4411% Time: 4898.1923s\n",
            "#167 [Test Phase] Loss: 0.6214 Acc: 62.4095% Time: 4927.4878s\n",
            "#168 [Test Phase] Loss: 2.7664 Acc: 61.5207% Time: 4956.8572s\n",
            "#169 [Test Phase] Loss: 2.1388 Acc: 67.5444% Time: 4986.2411s\n",
            "#170 [Test Phase] Loss: 2.0317 Acc: 68.6965% Time: 5015.5417s\n",
            "#171 [Test Phase] Loss: 1.7184 Acc: 72.0211% Time: 5044.7312s\n",
            "#172 [Test Phase] Loss: 1.8073 Acc: 71.3957% Time: 5074.0634s\n",
            "#173 [Test Phase] Loss: 2.0667 Acc: 68.9928% Time: 5103.4068s\n",
            "#174 [Test Phase] Loss: 2.2419 Acc: 67.3140% Time: 5132.7860s\n",
            "#175 [Test Phase] Loss: 1.7961 Acc: 72.4161% Time: 5162.1712s\n",
            "#176 [Test Phase] Loss: 2.2304 Acc: 65.8986% Time: 5191.5570s\n",
            "#177 [Test Phase] Loss: 3.6316 Acc: 55.1020% Time: 5220.9535s\n",
            "#178 [Test Phase] Loss: 0.5894 Acc: 66.8532% Time: 5250.3132s\n",
            "#179 [Test Phase] Loss: 2.0957 Acc: 68.0053% Time: 5279.7049s\n",
            "#180 [Test Phase] Loss: 3.0606 Acc: 59.3153% Time: 5309.1276s\n",
            "#181 [Test Phase] Loss: 1.5348 Acc: 74.7860% Time: 5338.5137s\n",
            "#182 [Test Phase] Loss: 2.4092 Acc: 65.9315% Time: 5367.7981s\n",
            "#183 [Test Phase] Loss: 2.1017 Acc: 68.9928% Time: 5397.0948s\n",
            "#184 [Test Phase] Loss: 1.6950 Acc: 74.3252% Time: 5426.5007s\n",
            "#185 [Test Phase] Loss: 2.3333 Acc: 67.2482% Time: 5455.7805s\n",
            "#186 [Test Phase] Loss: 1.9775 Acc: 69.0586% Time: 5485.0617s\n",
            "#187 [Test Phase] Loss: 1.6618 Acc: 72.8769% Time: 5514.4223s\n",
            "#188 [Test Phase] Loss: 1.7520 Acc: 72.5477% Time: 5543.6952s\n",
            "#189 [Test Phase] Loss: 0.6953 Acc: 53.2258% Time: 5572.9891s\n",
            "#190 [Test Phase] Loss: 1.9957 Acc: 70.1119% Time: 5602.2659s\n",
            "#191 [Test Phase] Loss: 2.0819 Acc: 69.3548% Time: 5631.6252s\n",
            "#192 [Test Phase] Loss: 2.3074 Acc: 67.6432% Time: 5660.9789s\n",
            "#193 [Test Phase] Loss: 1.8891 Acc: 72.2186% Time: 5690.2356s\n",
            "#194 [Test Phase] Loss: 1.9764 Acc: 71.8565% Time: 5719.5647s\n",
            "#195 [Test Phase] Loss: 2.6088 Acc: 65.1415% Time: 5748.8715s\n",
            "#196 [Test Phase] Loss: 2.1950 Acc: 69.6511% Time: 5778.1896s\n",
            "#197 [Test Phase] Loss: 2.0825 Acc: 70.9348% Time: 5807.4735s\n",
            "#198 [Test Phase] Loss: 2.2197 Acc: 69.7498% Time: 5836.6932s\n",
            "#199 [Test Phase] Loss: 2.2091 Acc: 69.9144% Time: 5866.0321s\n"
          ]
        }
      ],
      "source": [
        "#8\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.models.vgg import vgg16\n",
        "\n",
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = vgg16(pretrained=True)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "fc = nn.Sequential(\n",
        "    nn.Linear(25088,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,4096),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096,2)\n",
        ")\n",
        "\n",
        "model.classifier = fc\n",
        "model.to(device)\n",
        "\n",
        "import tqdm\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# from torch.optim.adam import Adam\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    Resize(224),\n",
        "    # RandomCrop((224,224),padding=4),\n",
        "    # RandomHorizontalFlip(p=0.5),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.4914,0.4822,0.4465) , std = (0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=16, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)\n",
        "\n",
        "lr = 0.0001\n",
        "optim = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 200\n",
        "writer = SummaryWriter()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    ################################# 여기 ##################################\n",
        "    torch.save(model.state_dict(), f'C:/team3/vgg/models/#8 vgg_models/vgg_dict{epoch}.pth')\n",
        "\n",
        "writer.close()\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "################################# 여기 ##################################\n",
        "path = \"C:/team3/vgg/models/#8 vgg_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "start_time = time.time()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    dict_model = torch.load(path + \"/\" + file, map_location=device)\n",
        "    model.load_state_dict(dict_model)\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('#{} [Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "team3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "38eac6efdfb6e1d89e5adada41cd1cba1407b7df80f8c1b640481bdc8f4da74b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
