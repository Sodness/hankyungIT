{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'apt'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "# 한글 폰트 설치하기 (꼭! 설치가 완료되면 [런타임 다시 시작]을 누르고 다시 실행하기)\n",
        "!apt install fonts-nanum -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] [-e] <local project path> ...\n",
            "  c:\\Users\\user\\.conda\\envs\\han\\python.exe -m pip install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -y\n"
          ]
        }
      ],
      "source": [
        "%pip install fonts-nanum -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3krZm7boUYaC"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib.font_manager' has no attribute '_rebuild'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\user\\Desktop\\7. 김한호 강사님\\2022.12.13\\resnet34_pretrain_final.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/7.%20%EA%B9%80%ED%95%9C%ED%98%B8%20%EA%B0%95%EC%82%AC%EB%8B%98/2022.12.13/resnet34_pretrain_final.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m font \u001b[39m=\u001b[39m fm\u001b[39m.\u001b[39mFontProperties(fname\u001b[39m=\u001b[39mfontpath, size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/7.%20%EA%B9%80%ED%95%9C%ED%98%B8%20%EA%B0%95%EC%82%AC%EB%8B%98/2022.12.13/resnet34_pretrain_final.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mrc(\u001b[39m'\u001b[39m\u001b[39mfont\u001b[39m\u001b[39m'\u001b[39m, family\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNanumBarunGothic\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/7.%20%EA%B9%80%ED%95%9C%ED%98%B8%20%EA%B0%95%EC%82%AC%EB%8B%98/2022.12.13/resnet34_pretrain_final.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m matplotlib\u001b[39m.\u001b[39;49mfont_manager\u001b[39m.\u001b[39;49m_rebuild()\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.font_manager' has no attribute '_rebuild'"
          ]
        }
      ],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 한글 폰트 설정하기\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=10)\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "matplotlib.font_manager._rebuild()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4Z_KLEjUVZWY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "directory_list = [\n",
        "    './custom_dataset/train/',\n",
        "    './custom_dataset/test/',\n",
        "]\n",
        "\n",
        "# 초기 디렉토리 만들기\n",
        "for directory in directory_list:\n",
        "    if not os.path.isdir(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "# 수집한 이미지를 학습 데이터와 평가 데이터로 구분하는 함수\n",
        "def dataset_split(query, train_cnt):\n",
        "    # 학습 및 평가 데이터셋 디렉토리 만들기\n",
        "    for directory in directory_list:\n",
        "        if not os.path.isdir(directory + '/' + query):\n",
        "            os.makedirs(directory + '/' + query)\n",
        "    # 학습 및 평가 데이터셋 준비하기\n",
        "    cnt = 0\n",
        "    for file_name in os.listdir(query):\n",
        "        if cnt < train_cnt:\n",
        "            # print(f'[Train Dataset] {file_name}')\n",
        "            shutil.move(query + '/' + file_name, './custom_dataset/train/' + query + '/' + file_name)\n",
        "        else:\n",
        "            # print(f'[Test Dataset] {file_name}')\n",
        "            shutil.move(query + '/' + file_name, './custom_dataset/test/' + query + '/' + file_name)\n",
        "        cnt += 1\n",
        "    shutil.rmtree(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kOR6UwvhXsC5"
      },
      "outputs": [],
      "source": [
        "query = '0'\n",
        "dataset_split(query, 210)\n",
        "query = '1'\n",
        "dataset_split(query, 210)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQ48DEenYHev"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device 객체"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(2022)\n",
        "torch.cuda.manual_seed(2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pXHpCHvcYJ2P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터셋 크기: 2004\n",
            "테스트 데이터셋 크기: 3038\n",
            "클래스: ['0', '1']\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    # transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "# shuffle : set to True to have the data reshuffled at every epoch (이거 True 설정해도 결과값이 같게 나오네 완전 랜덤으로 섞는건 아닌듯)\n",
        "# num_workers : how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\n",
        "# 보통의 일반적인 환경에서 오픈소스로 풀려있는 모델을 학습시킬때는 코어 개수의 절반정도 수치면 무난하게 시스템 리소스를 사용하며 학습이 가능했습니다\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=4, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=4, shuffle=True, num_workers=4)\n",
        "\n",
        "print('학습 데이터셋 크기:', len(train_datasets))\n",
        "print('테스트 데이터셋 크기:', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('클래스:', class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SYwt5TpIYTGX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 1, 0, 0])\n",
            "(3, 228, 906)\n",
            "===input==> (228, 906, 3)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def imshow(input, title):\n",
        "    # torch.Tensor를 numpy 객체로 변환\n",
        "    print(input.numpy().shape)\n",
        "\n",
        "    input = input.numpy().transpose((1, 2, 0))\n",
        "    # 이미지 정규화 해제하기\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    input = std * input + mean\n",
        "    input = np.clip(input, 0, 1)\n",
        "    # 이미지 출력\n",
        "\n",
        "    print('===input==>',input.shape)\n",
        "    plt.imshow(input)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 학습 데이터를 배치 단위로 불러오기\n",
        "iterator = iter(train_dataloader)\n",
        "\n",
        "# 현재 배치를 이용해 격자 형태의 이미지를 만들어 시각화\n",
        "inputs, classes = next(iterator)\n",
        "print(classes)\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "G1btTeRKYa0-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\prj\\team3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# 이미 학습된 resnet34 신경망을 불러온다\n",
        "model = models.resnet34(pretrained=True)\n",
        "print(model)\n",
        "#for name,module in model.named_parameters():\n",
        "#    module.requires_grad = False\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_features = model.fc.in_features\n",
        "# 전이 학습(transfer learning): 모델의 출력 뉴런 수를 3개로 교체하여 마지막 레이어 다시 학습\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "model = model.to(device)\n",
        "for name,module in model.named_parameters():\n",
        "    print( module.requires_grad )\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "#stochastic gradint descent : 확률적 경사하강법\n",
        "#미니배치를 사용하여 다소 부정확할수는 있지만 계산 속도가 빠르다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "K1phMzVPYu9u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Loss: 5.4785 Acc: 53.5429% Time: 7.7399s\n",
            "#1 Loss: 3.8845 Acc: 54.5409% Time: 15.4265s\n",
            "#2 Loss: 5.6512 Acc: 54.9900% Time: 23.1467s\n",
            "#3 Loss: 5.3920 Acc: 54.2914% Time: 30.7902s\n",
            "#4 Loss: 4.8860 Acc: 54.9401% Time: 38.4101s\n",
            "#5 Loss: 4.4315 Acc: 56.0878% Time: 46.0553s\n",
            "#6 Loss: 5.0751 Acc: 55.3892% Time: 53.6808s\n",
            "#7 Loss: 5.1890 Acc: 54.6407% Time: 61.2118s\n",
            "#8 Loss: 4.8743 Acc: 55.1397% Time: 68.8264s\n",
            "#9 Loss: 4.2732 Acc: 56.4371% Time: 76.4265s\n",
            "#10 Loss: 4.4926 Acc: 57.9341% Time: 84.0219s\n",
            "#11 Loss: 4.8677 Acc: 55.7884% Time: 91.5816s\n",
            "#12 Loss: 4.4856 Acc: 55.7385% Time: 99.1684s\n",
            "#13 Loss: 4.5875 Acc: 56.3872% Time: 106.7769s\n",
            "#14 Loss: 5.6784 Acc: 56.9860% Time: 114.3750s\n",
            "#15 Loss: 4.4142 Acc: 56.4870% Time: 122.0486s\n",
            "#16 Loss: 5.0942 Acc: 56.8862% Time: 129.6353s\n",
            "#17 Loss: 4.9072 Acc: 57.7844% Time: 137.2828s\n",
            "#18 Loss: 5.5248 Acc: 57.9840% Time: 144.8415s\n",
            "#19 Loss: 4.8779 Acc: 55.8882% Time: 152.3683s\n",
            "#20 Loss: 4.5187 Acc: 57.7844% Time: 159.9798s\n",
            "#21 Loss: 4.8342 Acc: 57.6846% Time: 167.5248s\n",
            "#22 Loss: 6.1598 Acc: 54.7904% Time: 175.1612s\n",
            "#23 Loss: 5.4318 Acc: 54.9401% Time: 182.8230s\n",
            "#24 Loss: 5.0121 Acc: 57.4850% Time: 190.3764s\n",
            "#25 Loss: 5.8087 Acc: 56.0379% Time: 197.9767s\n",
            "#26 Loss: 5.0576 Acc: 57.3353% Time: 205.5688s\n",
            "#27 Loss: 4.3881 Acc: 56.9361% Time: 213.2388s\n",
            "#28 Loss: 5.2754 Acc: 59.0319% Time: 220.9186s\n",
            "#29 Loss: 4.9998 Acc: 59.1317% Time: 228.5078s\n",
            "#30 Loss: 5.1160 Acc: 55.3393% Time: 236.2135s\n",
            "#31 Loss: 5.6397 Acc: 58.1836% Time: 243.7045s\n",
            "#32 Loss: 5.2362 Acc: 54.1417% Time: 251.1239s\n",
            "#33 Loss: 4.5567 Acc: 58.2335% Time: 258.7566s\n",
            "#34 Loss: 5.2927 Acc: 56.7864% Time: 266.3330s\n",
            "#35 Loss: 4.9533 Acc: 56.3872% Time: 273.9472s\n",
            "#36 Loss: 4.6022 Acc: 57.7844% Time: 281.5119s\n",
            "#37 Loss: 4.4445 Acc: 57.3353% Time: 289.1462s\n",
            "#38 Loss: 3.9468 Acc: 56.4371% Time: 296.7188s\n",
            "#39 Loss: 4.7036 Acc: 56.1377% Time: 304.3390s\n",
            "#40 Loss: 5.7285 Acc: 56.4371% Time: 311.9294s\n",
            "#41 Loss: 4.6229 Acc: 55.7385% Time: 319.5402s\n",
            "#42 Loss: 4.7415 Acc: 56.3872% Time: 327.0824s\n",
            "#43 Loss: 4.0581 Acc: 57.4351% Time: 334.6866s\n",
            "#44 Loss: 4.9298 Acc: 56.5369% Time: 342.3546s\n",
            "#45 Loss: 4.8925 Acc: 56.6866% Time: 349.8718s\n",
            "#46 Loss: 4.6657 Acc: 58.1836% Time: 357.4226s\n",
            "#47 Loss: 4.8882 Acc: 56.8363% Time: 365.0656s\n",
            "#48 Loss: 6.0951 Acc: 56.9361% Time: 372.7119s\n",
            "#49 Loss: 4.8574 Acc: 56.8862% Time: 380.2831s\n",
            "#50 Loss: 5.0090 Acc: 57.8842% Time: 387.8950s\n",
            "#51 Loss: 5.0161 Acc: 56.0379% Time: 395.4913s\n",
            "#52 Loss: 4.7683 Acc: 56.0379% Time: 403.0832s\n",
            "#53 Loss: 4.7639 Acc: 56.6367% Time: 410.7163s\n",
            "#54 Loss: 4.1057 Acc: 57.5349% Time: 418.3238s\n",
            "#55 Loss: 5.3278 Acc: 55.6886% Time: 425.8984s\n",
            "#56 Loss: 5.1812 Acc: 56.0379% Time: 433.5271s\n",
            "#57 Loss: 4.2565 Acc: 59.2814% Time: 441.0787s\n",
            "#58 Loss: 5.0364 Acc: 57.5349% Time: 448.6737s\n",
            "#59 Loss: 3.6695 Acc: 59.4810% Time: 456.2832s\n",
            "#60 Loss: 4.7931 Acc: 55.7884% Time: 463.9021s\n",
            "#61 Loss: 5.3281 Acc: 55.6886% Time: 471.5046s\n",
            "#62 Loss: 4.6324 Acc: 57.8343% Time: 479.0681s\n",
            "#63 Loss: 4.2949 Acc: 58.8822% Time: 486.6695s\n",
            "#64 Loss: 4.8329 Acc: 56.7864% Time: 494.1914s\n",
            "#65 Loss: 4.6428 Acc: 57.3353% Time: 501.8243s\n",
            "#66 Loss: 5.1068 Acc: 57.4351% Time: 509.4233s\n",
            "#67 Loss: 5.3237 Acc: 57.5349% Time: 517.0893s\n",
            "#68 Loss: 5.3563 Acc: 57.2854% Time: 524.7307s\n",
            "#69 Loss: 4.4849 Acc: 59.5808% Time: 532.4069s\n",
            "#70 Loss: 4.3672 Acc: 58.3832% Time: 540.0537s\n",
            "#71 Loss: 4.9319 Acc: 58.1337% Time: 547.7119s\n",
            "#72 Loss: 4.5168 Acc: 57.1357% Time: 555.2649s\n",
            "#73 Loss: 4.7263 Acc: 57.1357% Time: 562.9237s\n",
            "#74 Loss: 4.2548 Acc: 58.2834% Time: 570.4362s\n",
            "#75 Loss: 4.5930 Acc: 57.7844% Time: 578.0453s\n",
            "#76 Loss: 4.6145 Acc: 56.5369% Time: 585.5742s\n",
            "#77 Loss: 4.7886 Acc: 56.4870% Time: 593.2020s\n",
            "#78 Loss: 5.3382 Acc: 57.6347% Time: 600.7650s\n",
            "#79 Loss: 4.6866 Acc: 58.5828% Time: 608.4301s\n",
            "#80 Loss: 4.3237 Acc: 59.1317% Time: 616.0848s\n",
            "#81 Loss: 4.4386 Acc: 58.2335% Time: 623.6377s\n",
            "#82 Loss: 5.5074 Acc: 55.3393% Time: 631.2076s\n",
            "#83 Loss: 4.6409 Acc: 55.7884% Time: 638.7892s\n",
            "#84 Loss: 5.6804 Acc: 55.5888% Time: 646.4487s\n",
            "#85 Loss: 4.7096 Acc: 55.6886% Time: 653.9859s\n",
            "#86 Loss: 3.6254 Acc: 60.8283% Time: 661.5715s\n",
            "#87 Loss: 4.7758 Acc: 57.7844% Time: 668.9911s\n",
            "#88 Loss: 6.0947 Acc: 56.7864% Time: 676.5875s\n",
            "#89 Loss: 5.4175 Acc: 58.6826% Time: 684.2519s\n",
            "#90 Loss: 5.5454 Acc: 57.5848% Time: 691.7765s\n",
            "#91 Loss: 4.7212 Acc: 57.1357% Time: 699.3852s\n",
            "#92 Loss: 4.2938 Acc: 57.9341% Time: 707.0116s\n",
            "#93 Loss: 5.1805 Acc: 56.6367% Time: 714.6014s\n",
            "#94 Loss: 4.5948 Acc: 56.2874% Time: 722.1523s\n",
            "#95 Loss: 4.2354 Acc: 58.1337% Time: 729.7505s\n",
            "#96 Loss: 4.6419 Acc: 56.3872% Time: 737.4142s\n",
            "#97 Loss: 5.9589 Acc: 53.6926% Time: 744.9489s\n",
            "#98 Loss: 4.6461 Acc: 57.4351% Time: 752.5374s\n",
            "#99 Loss: 5.4027 Acc: 56.3373% Time: 760.1757s\n",
            "#100 Loss: 4.7437 Acc: 57.4850% Time: 767.7958s\n",
            "#101 Loss: 3.8039 Acc: 59.4311% Time: 775.4140s\n",
            "#102 Loss: 4.7482 Acc: 56.5868% Time: 782.9305s\n",
            "#103 Loss: 4.3714 Acc: 58.3832% Time: 790.5511s\n",
            "#104 Loss: 3.9322 Acc: 58.7824% Time: 798.1320s\n",
            "#105 Loss: 4.4487 Acc: 55.2395% Time: 805.7567s\n",
            "#106 Loss: 5.9411 Acc: 57.1856% Time: 813.3675s\n",
            "#107 Loss: 5.0051 Acc: 56.6866% Time: 820.9952s\n",
            "#108 Loss: 5.1439 Acc: 55.4391% Time: 828.5481s\n",
            "#109 Loss: 4.1999 Acc: 58.2834% Time: 836.1099s\n",
            "#110 Loss: 4.5922 Acc: 57.8343% Time: 843.8160s\n",
            "#111 Loss: 5.0129 Acc: 55.1896% Time: 851.3575s\n",
            "#112 Loss: 5.8415 Acc: 54.2415% Time: 858.9421s\n",
            "#113 Loss: 4.4412 Acc: 59.1816% Time: 866.5264s\n",
            "#114 Loss: 4.9713 Acc: 59.4311% Time: 874.1402s\n",
            "#115 Loss: 3.9589 Acc: 60.2794% Time: 881.7621s\n",
            "#116 Loss: 4.9588 Acc: 55.8383% Time: 889.3714s\n",
            "#117 Loss: 4.6548 Acc: 58.3333% Time: 896.9762s\n",
            "#118 Loss: 5.1827 Acc: 56.3373% Time: 904.5481s\n",
            "#119 Loss: 4.7237 Acc: 57.2854% Time: 912.1293s\n",
            "#120 Loss: 4.5614 Acc: 58.5828% Time: 919.7254s\n",
            "#121 Loss: 4.7037 Acc: 56.9860% Time: 927.3399s\n",
            "#122 Loss: 4.5587 Acc: 56.8363% Time: 934.8798s\n",
            "#123 Loss: 4.1643 Acc: 58.1836% Time: 942.5234s\n",
            "#124 Loss: 5.3055 Acc: 56.5369% Time: 950.0879s\n",
            "#125 Loss: 4.3210 Acc: 58.8822% Time: 957.6834s\n",
            "#126 Loss: 4.2802 Acc: 56.7365% Time: 965.2945s\n",
            "#127 Loss: 5.2727 Acc: 56.8363% Time: 972.8526s\n",
            "#128 Loss: 5.0359 Acc: 57.0359% Time: 980.5246s\n",
            "#129 Loss: 4.3358 Acc: 58.1836% Time: 988.0729s\n",
            "#130 Loss: 4.4131 Acc: 56.5369% Time: 995.6252s\n",
            "#131 Loss: 4.2619 Acc: 55.6886% Time: 1003.3214s\n",
            "#132 Loss: 4.3159 Acc: 58.0838% Time: 1010.8879s\n",
            "#133 Loss: 4.5257 Acc: 57.8842% Time: 1018.4847s\n",
            "#134 Loss: 5.1328 Acc: 57.1357% Time: 1026.0547s\n",
            "#135 Loss: 5.1516 Acc: 56.5868% Time: 1033.6784s\n",
            "#136 Loss: 6.1243 Acc: 53.9920% Time: 1041.2584s\n",
            "#137 Loss: 4.6921 Acc: 57.0359% Time: 1048.9068s\n",
            "#138 Loss: 4.5215 Acc: 56.8363% Time: 1056.5410s\n",
            "#139 Loss: 5.1510 Acc: 54.1916% Time: 1064.2087s\n",
            "#140 Loss: 5.5306 Acc: 55.4890% Time: 1071.8637s\n",
            "#141 Loss: 5.2829 Acc: 56.4870% Time: 1079.5056s\n",
            "#142 Loss: 4.6506 Acc: 55.7884% Time: 1087.0931s\n",
            "#143 Loss: 4.4776 Acc: 57.6846% Time: 1094.6809s\n",
            "#144 Loss: 4.7839 Acc: 57.6846% Time: 1102.3064s\n",
            "#145 Loss: 5.0283 Acc: 58.3333% Time: 1109.8558s\n",
            "#146 Loss: 5.8502 Acc: 56.6866% Time: 1117.4541s\n",
            "#147 Loss: 4.4462 Acc: 56.4371% Time: 1125.0416s\n",
            "#148 Loss: 4.4033 Acc: 57.8343% Time: 1132.6965s\n",
            "#149 Loss: 5.0050 Acc: 55.2894% Time: 1140.3006s\n",
            "#150 Loss: 4.3077 Acc: 57.4850% Time: 1147.8688s\n",
            "#151 Loss: 4.6627 Acc: 56.1377% Time: 1155.4046s\n",
            "#152 Loss: 5.5706 Acc: 57.1357% Time: 1163.0218s\n",
            "#153 Loss: 5.3293 Acc: 56.3872% Time: 1170.6672s\n",
            "#154 Loss: 4.3107 Acc: 56.5868% Time: 1178.2733s\n",
            "#155 Loss: 4.8872 Acc: 56.4371% Time: 1185.8480s\n",
            "#156 Loss: 4.5120 Acc: 57.2854% Time: 1193.4599s\n",
            "#157 Loss: 4.3728 Acc: 57.0359% Time: 1201.0789s\n",
            "#158 Loss: 4.8265 Acc: 59.1816% Time: 1208.6111s\n",
            "#159 Loss: 4.8560 Acc: 58.4830% Time: 1216.3075s\n",
            "#160 Loss: 4.1870 Acc: 57.4850% Time: 1223.9147s\n",
            "#161 Loss: 4.2505 Acc: 56.7365% Time: 1231.4358s\n",
            "#162 Loss: 3.9207 Acc: 59.9800% Time: 1239.0310s\n",
            "#163 Loss: 4.8655 Acc: 56.7365% Time: 1246.5899s\n",
            "#164 Loss: 5.5636 Acc: 55.2894% Time: 1254.2209s\n",
            "#165 Loss: 5.1193 Acc: 57.9840% Time: 1261.8727s\n",
            "#166 Loss: 5.7238 Acc: 57.0858% Time: 1269.4042s\n",
            "#167 Loss: 3.8255 Acc: 58.7824% Time: 1276.9534s\n",
            "#168 Loss: 4.6594 Acc: 58.1337% Time: 1284.5682s\n",
            "#169 Loss: 4.4291 Acc: 57.3852% Time: 1292.2015s\n",
            "#170 Loss: 5.1067 Acc: 55.5389% Time: 1299.8517s\n",
            "#171 Loss: 4.6669 Acc: 57.5848% Time: 1307.3673s\n",
            "#172 Loss: 5.2208 Acc: 57.1856% Time: 1315.0624s\n",
            "#173 Loss: 4.4668 Acc: 60.1297% Time: 1322.6035s\n",
            "#174 Loss: 4.8846 Acc: 56.4371% Time: 1330.1909s\n",
            "#175 Loss: 5.0122 Acc: 57.7345% Time: 1337.5602s\n",
            "#176 Loss: 5.2703 Acc: 57.1856% Time: 1345.2191s\n",
            "#177 Loss: 5.7721 Acc: 56.9361% Time: 1352.7939s\n",
            "#178 Loss: 4.7654 Acc: 56.9860% Time: 1360.4031s\n",
            "#179 Loss: 4.2589 Acc: 57.5349% Time: 1367.9438s\n",
            "#180 Loss: 4.2904 Acc: 56.9860% Time: 1375.5801s\n",
            "#181 Loss: 5.0140 Acc: 54.7904% Time: 1383.2457s\n",
            "#182 Loss: 4.6934 Acc: 57.1357% Time: 1390.9812s\n",
            "#183 Loss: 5.5339 Acc: 57.4850% Time: 1398.5762s\n",
            "#184 Loss: 4.4769 Acc: 57.5349% Time: 1406.2595s\n",
            "#185 Loss: 5.0989 Acc: 56.1876% Time: 1413.7796s\n",
            "#186 Loss: 5.8430 Acc: 54.3912% Time: 1421.4196s\n",
            "#187 Loss: 4.7811 Acc: 56.2874% Time: 1429.2025s\n",
            "#188 Loss: 5.1513 Acc: 56.2375% Time: 1436.7559s\n",
            "#189 Loss: 4.2252 Acc: 58.1836% Time: 1444.3722s\n",
            "#190 Loss: 4.8197 Acc: 57.0858% Time: 1451.8873s\n",
            "#191 Loss: 5.4038 Acc: 56.4371% Time: 1459.3845s\n",
            "#192 Loss: 3.9346 Acc: 56.6367% Time: 1466.9107s\n",
            "#193 Loss: 4.3366 Acc: 57.7844% Time: 1474.5171s\n",
            "#194 Loss: 4.1155 Acc: 59.1317% Time: 1482.0679s\n",
            "#195 Loss: 4.5771 Acc: 57.7844% Time: 1489.5349s\n",
            "#196 Loss: 4.9856 Acc: 56.5868% Time: 1497.0938s\n",
            "#197 Loss: 4.6631 Acc: 58.5329% Time: 1504.6967s\n",
            "#198 Loss: 4.8240 Acc: 55.2894% Time: 1512.3650s\n",
            "#199 Loss: 4.0387 Acc: 59.3812% Time: 1519.8953s\n",
            "#200 Loss: 4.6273 Acc: 57.1856% Time: 1527.4977s\n",
            "#201 Loss: 4.5896 Acc: 56.2874% Time: 1535.0879s\n",
            "#202 Loss: 6.3849 Acc: 55.8383% Time: 1542.6868s\n",
            "#203 Loss: 4.7309 Acc: 56.8862% Time: 1550.3599s\n",
            "#204 Loss: 4.8600 Acc: 58.1836% Time: 1557.8427s\n",
            "#205 Loss: 4.9836 Acc: 57.7345% Time: 1565.2869s\n",
            "#206 Loss: 4.4020 Acc: 58.0838% Time: 1572.8492s\n",
            "#207 Loss: 5.0587 Acc: 56.0878% Time: 1580.4786s\n",
            "#208 Loss: 5.3771 Acc: 54.6906% Time: 1588.0714s\n",
            "#209 Loss: 5.2272 Acc: 56.2375% Time: 1595.6572s\n",
            "#210 Loss: 4.8462 Acc: 57.1856% Time: 1603.2645s\n",
            "#211 Loss: 6.0580 Acc: 55.2894% Time: 1610.8862s\n",
            "#212 Loss: 5.7487 Acc: 58.1337% Time: 1618.4545s\n",
            "#213 Loss: 5.8068 Acc: 56.8862% Time: 1625.8861s\n",
            "#214 Loss: 4.3821 Acc: 58.7325% Time: 1633.4286s\n",
            "#215 Loss: 5.1064 Acc: 56.4870% Time: 1641.0423s\n",
            "#216 Loss: 4.3647 Acc: 58.4830% Time: 1648.7242s\n",
            "#217 Loss: 4.5080 Acc: 56.5369% Time: 1656.2513s\n",
            "#218 Loss: 5.3143 Acc: 55.7884% Time: 1663.8706s\n",
            "#219 Loss: 5.0373 Acc: 56.1876% Time: 1671.4335s\n",
            "#220 Loss: 4.7660 Acc: 55.7385% Time: 1679.0603s\n",
            "#221 Loss: 6.0135 Acc: 55.9381% Time: 1686.6963s\n",
            "#222 Loss: 4.6765 Acc: 56.9860% Time: 1694.2670s\n",
            "#223 Loss: 5.2520 Acc: 57.8343% Time: 1701.8397s\n",
            "#224 Loss: 5.2591 Acc: 56.1876% Time: 1709.3791s\n",
            "#225 Loss: 4.6367 Acc: 59.1317% Time: 1716.8936s\n",
            "#226 Loss: 5.1026 Acc: 57.9840% Time: 1724.4427s\n",
            "#227 Loss: 4.9948 Acc: 54.9401% Time: 1732.0573s\n",
            "#228 Loss: 4.6613 Acc: 57.5349% Time: 1739.4057s\n",
            "#229 Loss: 4.6833 Acc: 55.1397% Time: 1747.0470s\n",
            "#230 Loss: 4.6601 Acc: 56.3373% Time: 1754.6170s\n",
            "#231 Loss: 5.4971 Acc: 56.6866% Time: 1762.1870s\n",
            "#232 Loss: 4.7205 Acc: 58.6826% Time: 1769.8437s\n",
            "#233 Loss: 4.4677 Acc: 56.1876% Time: 1777.4240s\n",
            "#234 Loss: 4.5194 Acc: 58.3333% Time: 1785.0280s\n",
            "#235 Loss: 4.5398 Acc: 56.8862% Time: 1792.5579s\n",
            "#236 Loss: 4.6377 Acc: 57.6347% Time: 1800.1997s\n",
            "#237 Loss: 4.6691 Acc: 58.6826% Time: 1807.6086s\n",
            "#238 Loss: 5.0663 Acc: 58.1337% Time: 1815.1856s\n",
            "#239 Loss: 4.2194 Acc: 61.1277% Time: 1822.7491s\n",
            "#240 Loss: 5.4128 Acc: 57.2355% Time: 1830.2041s\n",
            "#241 Loss: 4.8791 Acc: 58.0838% Time: 1837.7304s\n",
            "#242 Loss: 4.5642 Acc: 57.9341% Time: 1845.2236s\n",
            "#243 Loss: 4.7739 Acc: 57.4850% Time: 1852.8220s\n",
            "#244 Loss: 5.4893 Acc: 56.5369% Time: 1860.4387s\n",
            "#245 Loss: 4.4918 Acc: 56.2375% Time: 1868.0132s\n",
            "#246 Loss: 3.9712 Acc: 58.5828% Time: 1875.6002s\n",
            "#247 Loss: 4.8992 Acc: 57.1357% Time: 1883.2209s\n",
            "#248 Loss: 4.8555 Acc: 58.2335% Time: 1890.7841s\n",
            "#249 Loss: 5.8872 Acc: 55.0898% Time: 1898.3926s\n",
            "#250 Loss: 5.2235 Acc: 56.0878% Time: 1906.0085s\n",
            "#251 Loss: 5.9904 Acc: 55.7385% Time: 1913.5136s\n",
            "#252 Loss: 5.2081 Acc: 55.2894% Time: 1921.1491s\n",
            "#253 Loss: 4.6546 Acc: 56.2874% Time: 1928.7715s\n",
            "#254 Loss: 5.5117 Acc: 54.6407% Time: 1936.3439s\n",
            "#255 Loss: 4.7680 Acc: 57.1856% Time: 1943.7425s\n",
            "#256 Loss: 5.0699 Acc: 55.5389% Time: 1951.3710s\n",
            "#257 Loss: 5.2789 Acc: 55.7385% Time: 1959.0097s\n",
            "#258 Loss: 6.0551 Acc: 56.0379% Time: 1966.5507s\n",
            "#259 Loss: 5.0375 Acc: 57.0359% Time: 1974.1470s\n",
            "#260 Loss: 3.9300 Acc: 59.9301% Time: 1981.6735s\n",
            "#261 Loss: 4.7437 Acc: 58.4331% Time: 1989.3298s\n",
            "#262 Loss: 5.8092 Acc: 56.0379% Time: 1996.9082s\n",
            "#263 Loss: 4.7071 Acc: 58.1836% Time: 2004.5288s\n",
            "#264 Loss: 4.8842 Acc: 56.3872% Time: 2012.1531s\n",
            "#265 Loss: 4.1952 Acc: 58.4830% Time: 2019.5111s\n",
            "#266 Loss: 4.8097 Acc: 56.6866% Time: 2026.9445s\n",
            "#267 Loss: 4.8190 Acc: 57.8842% Time: 2034.5465s\n",
            "#268 Loss: 4.3638 Acc: 59.4810% Time: 2042.2267s\n",
            "#269 Loss: 5.1972 Acc: 56.5369% Time: 2049.6795s\n",
            "#270 Loss: 5.8702 Acc: 56.4371% Time: 2057.1933s\n",
            "#271 Loss: 4.8414 Acc: 58.0838% Time: 2064.7455s\n",
            "#272 Loss: 5.1687 Acc: 55.5389% Time: 2072.3105s\n",
            "#273 Loss: 5.7393 Acc: 55.5888% Time: 2079.9132s\n",
            "#274 Loss: 4.9479 Acc: 57.1856% Time: 2087.5185s\n",
            "#275 Loss: 4.7119 Acc: 60.1297% Time: 2095.0544s\n",
            "#276 Loss: 5.0425 Acc: 57.3353% Time: 2102.5703s\n",
            "#277 Loss: 4.2884 Acc: 56.9860% Time: 2110.0964s\n",
            "#278 Loss: 5.4742 Acc: 57.6846% Time: 2117.6818s\n",
            "#279 Loss: 4.6508 Acc: 56.9860% Time: 2125.1410s\n",
            "#280 Loss: 4.5257 Acc: 56.0878% Time: 2132.7475s\n",
            "#281 Loss: 4.8016 Acc: 57.8343% Time: 2140.2653s\n",
            "#282 Loss: 5.7553 Acc: 58.3333% Time: 2147.6935s\n",
            "#283 Loss: 4.4852 Acc: 56.6866% Time: 2155.3758s\n",
            "#284 Loss: 5.5125 Acc: 56.9860% Time: 2162.9100s\n",
            "#285 Loss: 4.6422 Acc: 57.0858% Time: 2170.4687s\n",
            "#286 Loss: 5.4372 Acc: 55.9381% Time: 2178.0338s\n",
            "#287 Loss: 4.7734 Acc: 57.0359% Time: 2185.5031s\n",
            "#288 Loss: 5.8126 Acc: 56.2874% Time: 2193.0757s\n",
            "#289 Loss: 4.5460 Acc: 57.3852% Time: 2200.6423s\n",
            "#290 Loss: 4.2465 Acc: 57.7844% Time: 2208.0972s\n",
            "#291 Loss: 4.3038 Acc: 57.0858% Time: 2215.6728s\n",
            "#292 Loss: 4.6115 Acc: 56.1876% Time: 2223.2727s\n",
            "#293 Loss: 4.1771 Acc: 58.4331% Time: 2230.8763s\n",
            "#294 Loss: 4.5596 Acc: 56.6367% Time: 2238.2746s\n",
            "#295 Loss: 5.1281 Acc: 56.5369% Time: 2245.8746s\n",
            "#296 Loss: 4.6843 Acc: 57.0359% Time: 2253.4268s\n",
            "#297 Loss: 5.5180 Acc: 55.8882% Time: 2261.0434s\n",
            "#298 Loss: 4.4277 Acc: 56.1876% Time: 2268.6877s\n",
            "#299 Loss: 4.3758 Acc: 55.4391% Time: 2276.2641s\n",
            "#300 Loss: 5.4881 Acc: 57.6846% Time: 2283.8410s\n",
            "#301 Loss: 4.8800 Acc: 58.4830% Time: 2291.4254s\n",
            "#302 Loss: 5.4435 Acc: 57.4351% Time: 2298.9927s\n",
            "#303 Loss: 4.3383 Acc: 57.5349% Time: 2306.6601s\n",
            "#304 Loss: 5.6377 Acc: 55.7385% Time: 2314.2254s\n",
            "#305 Loss: 5.3045 Acc: 56.5369% Time: 2321.7943s\n",
            "#306 Loss: 4.9970 Acc: 57.6347% Time: 2329.4076s\n",
            "#307 Loss: 4.7104 Acc: 55.5888% Time: 2337.0296s\n",
            "#308 Loss: 5.6525 Acc: 56.6367% Time: 2344.5859s\n",
            "#309 Loss: 4.6134 Acc: 58.0838% Time: 2352.1050s\n",
            "#310 Loss: 4.4412 Acc: 57.6347% Time: 2359.5875s\n",
            "#311 Loss: 4.5001 Acc: 57.7345% Time: 2367.2403s\n",
            "#312 Loss: 5.0210 Acc: 57.9341% Time: 2374.8404s\n",
            "#313 Loss: 5.2054 Acc: 56.6367% Time: 2382.4039s\n",
            "#314 Loss: 4.8153 Acc: 55.9880% Time: 2390.0882s\n",
            "#315 Loss: 4.7403 Acc: 56.0379% Time: 2397.6303s\n",
            "#316 Loss: 4.3070 Acc: 55.9381% Time: 2405.2006s\n",
            "#317 Loss: 4.0399 Acc: 57.9341% Time: 2412.8532s\n",
            "#318 Loss: 4.4810 Acc: 55.6886% Time: 2420.3751s\n",
            "#319 Loss: 4.7601 Acc: 56.4371% Time: 2427.8546s\n",
            "#320 Loss: 4.1612 Acc: 58.5828% Time: 2435.3727s\n",
            "#321 Loss: 5.1982 Acc: 56.9361% Time: 2442.9913s\n",
            "#322 Loss: 4.0653 Acc: 60.0299% Time: 2450.5943s\n",
            "#323 Loss: 4.5961 Acc: 58.4830% Time: 2458.1899s\n",
            "#324 Loss: 4.4516 Acc: 57.4351% Time: 2465.7912s\n",
            "#325 Loss: 5.9216 Acc: 57.3353% Time: 2473.3396s\n",
            "#326 Loss: 4.5333 Acc: 57.1856% Time: 2480.8112s\n",
            "#327 Loss: 5.6781 Acc: 56.7365% Time: 2488.4316s\n",
            "#328 Loss: 4.7072 Acc: 58.2834% Time: 2496.0156s\n",
            "#329 Loss: 4.9638 Acc: 57.0858% Time: 2503.5602s\n",
            "#330 Loss: 4.1261 Acc: 57.8343% Time: 2511.1665s\n",
            "#331 Loss: 4.3078 Acc: 55.4890% Time: 2518.7913s\n",
            "#332 Loss: 5.3276 Acc: 56.6866% Time: 2526.3852s\n",
            "#333 Loss: 4.8781 Acc: 55.7884% Time: 2534.0264s\n",
            "#334 Loss: 5.9328 Acc: 54.4411% Time: 2541.5731s\n",
            "#335 Loss: 4.6960 Acc: 58.5329% Time: 2549.1312s\n",
            "#336 Loss: 4.7838 Acc: 56.6866% Time: 2556.7322s\n",
            "#337 Loss: 5.3492 Acc: 55.5888% Time: 2564.3654s\n",
            "#338 Loss: 3.9554 Acc: 58.3832% Time: 2571.7436s\n",
            "#339 Loss: 5.5542 Acc: 57.5349% Time: 2579.3478s\n",
            "#340 Loss: 4.5761 Acc: 57.1856% Time: 2586.9039s\n",
            "#341 Loss: 4.2521 Acc: 58.9321% Time: 2594.5431s\n",
            "#342 Loss: 5.5933 Acc: 58.1836% Time: 2602.1734s\n",
            "#343 Loss: 4.4566 Acc: 56.3373% Time: 2609.5765s\n",
            "#344 Loss: 5.4373 Acc: 56.4870% Time: 2617.1348s\n",
            "#345 Loss: 4.5362 Acc: 56.6866% Time: 2624.7169s\n",
            "#346 Loss: 3.6430 Acc: 59.5808% Time: 2632.1793s\n",
            "#347 Loss: 4.8965 Acc: 56.4371% Time: 2639.7806s\n",
            "#348 Loss: 4.7126 Acc: 57.8343% Time: 2647.3537s\n",
            "#349 Loss: 4.6500 Acc: 56.2375% Time: 2654.9104s\n",
            "#350 Loss: 4.1463 Acc: 58.0838% Time: 2662.5273s\n",
            "#351 Loss: 4.4812 Acc: 56.6866% Time: 2669.9784s\n",
            "#352 Loss: 5.1859 Acc: 55.4391% Time: 2677.6006s\n",
            "#353 Loss: 5.1984 Acc: 57.1856% Time: 2685.1370s\n",
            "#354 Loss: 4.6751 Acc: 59.0319% Time: 2692.7654s\n",
            "#355 Loss: 5.4461 Acc: 56.9361% Time: 2700.2386s\n",
            "#356 Loss: 4.4493 Acc: 57.6846% Time: 2707.7663s\n",
            "#357 Loss: 4.9310 Acc: 57.5848% Time: 2715.2972s\n",
            "#358 Loss: 5.2430 Acc: 56.6367% Time: 2722.7674s\n",
            "#359 Loss: 5.9898 Acc: 54.3912% Time: 2730.2971s\n",
            "#360 Loss: 4.9753 Acc: 55.9880% Time: 2737.9026s\n",
            "#361 Loss: 5.3410 Acc: 55.5888% Time: 2745.3526s\n",
            "#362 Loss: 4.9886 Acc: 58.9321% Time: 2752.9461s\n",
            "#363 Loss: 4.0372 Acc: 59.8802% Time: 2760.5165s\n",
            "#364 Loss: 4.6806 Acc: 56.7864% Time: 2768.1294s\n",
            "#365 Loss: 4.7688 Acc: 57.7844% Time: 2775.7117s\n",
            "#366 Loss: 6.6978 Acc: 53.8423% Time: 2783.2764s\n",
            "#367 Loss: 5.3459 Acc: 55.4890% Time: 2790.8391s\n",
            "#368 Loss: 4.7757 Acc: 56.6367% Time: 2798.4507s\n",
            "#369 Loss: 6.0045 Acc: 55.8882% Time: 2805.8398s\n",
            "#370 Loss: 4.1132 Acc: 59.1317% Time: 2813.4998s\n",
            "#371 Loss: 4.6261 Acc: 57.3852% Time: 2821.0896s\n",
            "#372 Loss: 3.9181 Acc: 57.2355% Time: 2828.6743s\n",
            "#373 Loss: 4.6998 Acc: 55.8882% Time: 2836.3041s\n",
            "#374 Loss: 4.0231 Acc: 59.4311% Time: 2843.8294s\n",
            "#375 Loss: 4.6797 Acc: 55.3892% Time: 2851.2982s\n",
            "#376 Loss: 5.9413 Acc: 55.8882% Time: 2858.9025s\n",
            "#377 Loss: 4.7658 Acc: 57.7844% Time: 2866.4910s\n",
            "#378 Loss: 4.3588 Acc: 57.0359% Time: 2874.0596s\n",
            "#379 Loss: 4.6681 Acc: 58.3333% Time: 2881.6834s\n",
            "#380 Loss: 4.6593 Acc: 57.0359% Time: 2889.1996s\n",
            "#381 Loss: 4.7309 Acc: 58.2335% Time: 2896.8742s\n",
            "#382 Loss: 5.0833 Acc: 54.6906% Time: 2904.4763s\n",
            "#383 Loss: 5.6563 Acc: 58.0838% Time: 2912.0041s\n",
            "#384 Loss: 5.6034 Acc: 56.4870% Time: 2919.7096s\n",
            "#385 Loss: 6.1772 Acc: 58.8822% Time: 2927.2486s\n",
            "#386 Loss: 4.3690 Acc: 57.6347% Time: 2934.7419s\n",
            "#387 Loss: 3.9914 Acc: 55.8383% Time: 2942.2329s\n",
            "#388 Loss: 5.7899 Acc: 57.1357% Time: 2949.8328s\n",
            "#389 Loss: 5.3837 Acc: 56.7864% Time: 2957.4350s\n",
            "#390 Loss: 3.9064 Acc: 58.8323% Time: 2965.0161s\n",
            "#391 Loss: 5.7098 Acc: 55.5389% Time: 2972.5877s\n",
            "#392 Loss: 4.2612 Acc: 57.7345% Time: 2980.0224s\n",
            "#393 Loss: 6.5236 Acc: 54.9900% Time: 2987.6012s\n",
            "#394 Loss: 6.0500 Acc: 55.6886% Time: 2995.0496s\n",
            "#395 Loss: 4.9719 Acc: 57.1856% Time: 3002.5693s\n",
            "#396 Loss: 5.0120 Acc: 55.8383% Time: 3010.0464s\n",
            "#397 Loss: 4.7505 Acc: 56.9860% Time: 3017.5667s\n",
            "#398 Loss: 5.2089 Acc: 57.2355% Time: 3025.0516s\n",
            "#399 Loss: 5.2084 Acc: 56.9860% Time: 3032.5814s\n",
            "#400 Loss: 4.8346 Acc: 58.3832% Time: 3040.1909s\n",
            "#401 Loss: 4.6832 Acc: 57.2355% Time: 3047.7887s\n",
            "#402 Loss: 4.1517 Acc: 58.6327% Time: 3055.2502s\n",
            "#403 Loss: 5.2003 Acc: 55.4890% Time: 3062.8448s\n",
            "#404 Loss: 6.4191 Acc: 56.9860% Time: 3070.4053s\n",
            "#405 Loss: 5.4318 Acc: 58.4830% Time: 3077.8480s\n",
            "#406 Loss: 4.3054 Acc: 56.6866% Time: 3085.3959s\n",
            "#407 Loss: 4.2545 Acc: 58.5828% Time: 3092.8593s\n",
            "#408 Loss: 4.8591 Acc: 56.0878% Time: 3100.4283s\n",
            "#409 Loss: 4.9308 Acc: 55.5888% Time: 3108.0259s\n",
            "#410 Loss: 5.0099 Acc: 58.6826% Time: 3115.5712s\n",
            "#411 Loss: 5.0040 Acc: 58.2335% Time: 3123.1905s\n",
            "#412 Loss: 4.8100 Acc: 56.7864% Time: 3130.7850s\n",
            "#413 Loss: 4.7643 Acc: 55.9381% Time: 3138.3868s\n",
            "#414 Loss: 5.2511 Acc: 57.0858% Time: 3145.9323s\n",
            "#415 Loss: 5.2751 Acc: 58.1337% Time: 3153.4151s\n",
            "#416 Loss: 5.5453 Acc: 57.5349% Time: 3160.9418s\n",
            "#417 Loss: 5.2439 Acc: 57.8842% Time: 3168.4065s\n",
            "#418 Loss: 4.8107 Acc: 58.7824% Time: 3175.9111s\n",
            "#419 Loss: 5.3488 Acc: 55.6886% Time: 3183.5834s\n",
            "#420 Loss: 4.6689 Acc: 58.9820% Time: 3191.1126s\n",
            "#421 Loss: 4.5955 Acc: 56.5369% Time: 3198.7338s\n",
            "#422 Loss: 4.8304 Acc: 56.0379% Time: 3206.4098s\n",
            "#423 Loss: 5.6819 Acc: 57.4351% Time: 3213.9154s\n",
            "#424 Loss: 5.4421 Acc: 58.2335% Time: 3221.5649s\n",
            "#425 Loss: 5.5981 Acc: 56.5868% Time: 3229.1509s\n",
            "#426 Loss: 5.6474 Acc: 56.2874% Time: 3236.7044s\n",
            "#427 Loss: 5.1846 Acc: 57.0858% Time: 3244.1535s\n",
            "#428 Loss: 5.0342 Acc: 56.8363% Time: 3251.7493s\n",
            "#429 Loss: 5.6844 Acc: 58.3333% Time: 3259.3399s\n",
            "#430 Loss: 4.8227 Acc: 56.6367% Time: 3266.9449s\n",
            "#431 Loss: 5.8835 Acc: 57.6347% Time: 3274.3222s\n",
            "#432 Loss: 4.4018 Acc: 58.1836% Time: 3281.9275s\n",
            "#433 Loss: 4.8750 Acc: 56.6367% Time: 3289.5318s\n",
            "#434 Loss: 4.8720 Acc: 58.0339% Time: 3297.0791s\n",
            "#435 Loss: 5.1338 Acc: 56.4371% Time: 3304.7187s\n",
            "#436 Loss: 4.5795 Acc: 58.4830% Time: 3312.3161s\n",
            "#437 Loss: 4.6716 Acc: 59.0319% Time: 3319.7720s\n",
            "#438 Loss: 5.6485 Acc: 54.4910% Time: 3327.3182s\n",
            "#439 Loss: 4.8846 Acc: 56.9860% Time: 3334.9300s\n",
            "#440 Loss: 5.9981 Acc: 56.4870% Time: 3342.5531s\n",
            "#441 Loss: 4.3666 Acc: 58.7824% Time: 3350.0847s\n",
            "#442 Loss: 5.0048 Acc: 58.2335% Time: 3357.7066s\n",
            "#443 Loss: 5.4795 Acc: 56.1377% Time: 3365.3150s\n",
            "#444 Loss: 4.8955 Acc: 55.8383% Time: 3372.9055s\n",
            "#445 Loss: 4.5008 Acc: 57.9341% Time: 3380.4446s\n",
            "#446 Loss: 5.5351 Acc: 56.0878% Time: 3388.1280s\n",
            "#447 Loss: 5.3589 Acc: 54.7405% Time: 3395.6992s\n",
            "#448 Loss: 4.4379 Acc: 58.9820% Time: 3403.2564s\n",
            "#449 Loss: 4.3951 Acc: 58.9321% Time: 3410.9499s\n",
            "#450 Loss: 4.3859 Acc: 58.2834% Time: 3418.4964s\n",
            "#451 Loss: 4.3756 Acc: 56.9361% Time: 3426.0504s\n",
            "#452 Loss: 5.6764 Acc: 55.3393% Time: 3433.4822s\n",
            "#453 Loss: 4.5197 Acc: 57.0359% Time: 3441.0665s\n",
            "#454 Loss: 4.9931 Acc: 58.6826% Time: 3448.5054s\n",
            "#455 Loss: 4.4796 Acc: 57.7844% Time: 3456.0963s\n",
            "#456 Loss: 4.9720 Acc: 56.4371% Time: 3463.6240s\n",
            "#457 Loss: 5.2894 Acc: 59.0319% Time: 3471.1224s\n",
            "#458 Loss: 4.8556 Acc: 58.1337% Time: 3478.6710s\n",
            "#459 Loss: 4.6191 Acc: 57.9341% Time: 3486.2961s\n",
            "#460 Loss: 5.5746 Acc: 55.0399% Time: 3493.8158s\n",
            "#461 Loss: 4.1825 Acc: 57.5349% Time: 3501.4620s\n",
            "#462 Loss: 5.0077 Acc: 57.2854% Time: 3509.0877s\n",
            "#463 Loss: 3.8358 Acc: 57.6846% Time: 3516.6174s\n",
            "#464 Loss: 4.9111 Acc: 58.1836% Time: 3524.2351s\n",
            "#465 Loss: 3.8973 Acc: 59.5808% Time: 3531.6883s\n",
            "#466 Loss: 4.1387 Acc: 57.7345% Time: 3539.2523s\n",
            "#467 Loss: 4.9035 Acc: 57.0359% Time: 3546.8241s\n",
            "#468 Loss: 5.1632 Acc: 56.5868% Time: 3554.4189s\n",
            "#469 Loss: 4.7146 Acc: 56.1377% Time: 3561.8337s\n",
            "#470 Loss: 4.0842 Acc: 58.4331% Time: 3569.2513s\n",
            "#471 Loss: 5.0487 Acc: 56.8363% Time: 3576.8426s\n",
            "#472 Loss: 5.5156 Acc: 56.0379% Time: 3584.3900s\n",
            "#473 Loss: 5.3130 Acc: 55.9381% Time: 3591.8592s\n",
            "#474 Loss: 4.6110 Acc: 58.4331% Time: 3599.3896s\n",
            "#475 Loss: 5.8122 Acc: 55.3892% Time: 3606.8397s\n",
            "#476 Loss: 3.9291 Acc: 57.8842% Time: 3614.4521s\n",
            "#477 Loss: 4.9590 Acc: 58.1337% Time: 3622.0237s\n",
            "#478 Loss: 4.5648 Acc: 56.3872% Time: 3629.6129s\n",
            "#479 Loss: 5.1155 Acc: 56.6367% Time: 3637.2290s\n",
            "#480 Loss: 5.0668 Acc: 55.2894% Time: 3644.8342s\n",
            "#481 Loss: 4.6909 Acc: 57.6846% Time: 3652.4944s\n",
            "#482 Loss: 4.3018 Acc: 56.3373% Time: 3659.9886s\n",
            "#483 Loss: 4.5646 Acc: 57.7844% Time: 3667.6031s\n",
            "#484 Loss: 4.9155 Acc: 57.3852% Time: 3675.1890s\n",
            "#485 Loss: 5.4290 Acc: 57.5349% Time: 3682.7788s\n",
            "#486 Loss: 5.0074 Acc: 57.8343% Time: 3690.4128s\n",
            "#487 Loss: 5.3306 Acc: 55.4890% Time: 3697.9922s\n",
            "#488 Loss: 4.4506 Acc: 56.5369% Time: 3705.5565s\n",
            "#489 Loss: 4.6852 Acc: 56.4371% Time: 3713.0012s\n",
            "#490 Loss: 3.9157 Acc: 56.4870% Time: 3720.5915s\n",
            "#491 Loss: 5.4851 Acc: 57.8842% Time: 3728.1663s\n",
            "#492 Loss: 3.8483 Acc: 59.3812% Time: 3735.7889s\n",
            "#493 Loss: 3.9859 Acc: 57.6846% Time: 3743.4284s\n",
            "#494 Loss: 5.1774 Acc: 54.9401% Time: 3751.0026s\n",
            "#495 Loss: 5.7275 Acc: 56.4371% Time: 3758.5965s\n",
            "#496 Loss: 5.0930 Acc: 55.7385% Time: 3766.1061s\n",
            "#497 Loss: 4.4822 Acc: 56.2874% Time: 3773.6344s\n",
            "#498 Loss: 4.9372 Acc: 55.9880% Time: 3781.1561s\n",
            "#499 Loss: 4.9560 Acc: 58.6327% Time: 3788.7612s\n",
            "#500 Loss: 4.8679 Acc: 55.9381% Time: 3796.3412s\n",
            "#501 Loss: 4.7201 Acc: 56.9860% Time: 3803.9169s\n",
            "#502 Loss: 4.6031 Acc: 57.4351% Time: 3811.4198s\n",
            "#503 Loss: 4.0302 Acc: 56.8363% Time: 3818.9365s\n",
            "#504 Loss: 6.0083 Acc: 55.1896% Time: 3826.3775s\n",
            "#505 Loss: 4.2571 Acc: 56.2874% Time: 3833.9042s\n",
            "#506 Loss: 4.4333 Acc: 57.6347% Time: 3841.5461s\n",
            "#507 Loss: 4.8364 Acc: 58.0838% Time: 3849.1450s\n",
            "#508 Loss: 4.3605 Acc: 57.2355% Time: 3856.7391s\n",
            "#509 Loss: 4.3452 Acc: 57.1856% Time: 3864.3668s\n",
            "#510 Loss: 4.7915 Acc: 57.8343% Time: 3871.9298s\n",
            "#511 Loss: 5.2308 Acc: 55.7884% Time: 3879.5714s\n",
            "#512 Loss: 5.0251 Acc: 54.8902% Time: 3887.1480s\n",
            "#513 Loss: 4.9100 Acc: 56.3872% Time: 3894.7324s\n",
            "#514 Loss: 5.0922 Acc: 55.4890% Time: 3902.1343s\n",
            "#515 Loss: 4.6034 Acc: 56.2874% Time: 3909.6845s\n",
            "#516 Loss: 4.7420 Acc: 57.5349% Time: 3917.1225s\n",
            "#517 Loss: 4.3775 Acc: 57.5349% Time: 3924.7312s\n",
            "#518 Loss: 5.0629 Acc: 56.1377% Time: 3932.2933s\n",
            "#519 Loss: 5.5230 Acc: 58.0339% Time: 3939.9095s\n",
            "#520 Loss: 5.0868 Acc: 55.2894% Time: 3947.3362s\n",
            "#521 Loss: 5.0556 Acc: 57.8343% Time: 3954.9519s\n",
            "#522 Loss: 5.4090 Acc: 56.7864% Time: 3962.5562s\n",
            "#523 Loss: 5.0560 Acc: 57.9341% Time: 3970.0727s\n",
            "#524 Loss: 4.7380 Acc: 58.0339% Time: 3977.5637s\n",
            "#525 Loss: 5.7290 Acc: 57.0359% Time: 3985.0535s\n",
            "#526 Loss: 5.1647 Acc: 56.6866% Time: 3992.5173s\n",
            "#527 Loss: 4.7497 Acc: 58.7325% Time: 4000.0840s\n",
            "#528 Loss: 4.6540 Acc: 57.3852% Time: 4007.7026s\n",
            "#529 Loss: 4.4669 Acc: 56.8862% Time: 4015.2407s\n",
            "#530 Loss: 4.5642 Acc: 56.9361% Time: 4022.7174s\n",
            "#531 Loss: 5.8039 Acc: 58.0838% Time: 4030.2872s\n",
            "#532 Loss: 5.5298 Acc: 57.2355% Time: 4037.8561s\n",
            "#533 Loss: 5.0933 Acc: 56.7864% Time: 4045.2990s\n",
            "#534 Loss: 4.3109 Acc: 58.4830% Time: 4052.8586s\n",
            "#535 Loss: 4.6123 Acc: 58.0838% Time: 4060.2842s\n",
            "#536 Loss: 4.0164 Acc: 58.3333% Time: 4067.8597s\n",
            "#537 Loss: 4.1968 Acc: 57.3852% Time: 4075.4687s\n",
            "#538 Loss: 4.5574 Acc: 57.4850% Time: 4083.0610s\n",
            "#539 Loss: 4.7260 Acc: 55.6886% Time: 4090.4882s\n",
            "#540 Loss: 4.4161 Acc: 57.2355% Time: 4098.0686s\n",
            "#541 Loss: 5.3672 Acc: 55.9880% Time: 4105.6237s\n",
            "#542 Loss: 5.7200 Acc: 55.8383% Time: 4113.2205s\n",
            "#543 Loss: 4.7916 Acc: 56.6866% Time: 4120.6590s\n",
            "#544 Loss: 4.4102 Acc: 56.8363% Time: 4128.2560s\n",
            "#545 Loss: 3.9877 Acc: 58.0339% Time: 4135.6822s\n",
            "#546 Loss: 4.0627 Acc: 59.2315% Time: 4143.2640s\n",
            "#547 Loss: 4.7307 Acc: 58.0838% Time: 4150.9461s\n",
            "#548 Loss: 4.8900 Acc: 56.9361% Time: 4158.4515s\n",
            "#549 Loss: 5.0471 Acc: 57.8343% Time: 4166.0436s\n",
            "#550 Loss: 4.4844 Acc: 58.5329% Time: 4173.6849s\n",
            "#551 Loss: 4.8937 Acc: 58.7325% Time: 4181.2106s\n",
            "#552 Loss: 5.0210 Acc: 56.5369% Time: 4188.6696s\n",
            "#553 Loss: 4.6066 Acc: 58.6826% Time: 4196.2537s\n",
            "#554 Loss: 4.4955 Acc: 57.4850% Time: 4203.8463s\n",
            "#555 Loss: 5.3140 Acc: 56.8862% Time: 4211.4080s\n",
            "#556 Loss: 4.2613 Acc: 59.3313% Time: 4219.0765s\n",
            "#557 Loss: 4.2107 Acc: 56.9361% Time: 4226.6378s\n",
            "#558 Loss: 4.7045 Acc: 56.5369% Time: 4234.2620s\n",
            "#559 Loss: 5.3337 Acc: 58.2335% Time: 4241.7662s\n",
            "#560 Loss: 4.6227 Acc: 59.5808% Time: 4249.2589s\n",
            "#561 Loss: 5.8608 Acc: 55.4391% Time: 4256.8033s\n",
            "#562 Loss: 6.1779 Acc: 55.2395% Time: 4264.4273s\n",
            "#563 Loss: 5.8897 Acc: 56.1876% Time: 4272.0130s\n",
            "#564 Loss: 4.9087 Acc: 55.9880% Time: 4279.4776s\n",
            "#565 Loss: 4.5670 Acc: 55.5888% Time: 4286.9861s\n",
            "#566 Loss: 4.9861 Acc: 56.5868% Time: 4294.5646s\n",
            "#567 Loss: 4.8165 Acc: 57.5848% Time: 4302.2113s\n",
            "#568 Loss: 5.7240 Acc: 56.9361% Time: 4309.7868s\n",
            "#569 Loss: 4.5510 Acc: 57.6347% Time: 4317.1975s\n",
            "#570 Loss: 5.3400 Acc: 55.5389% Time: 4324.7892s\n",
            "#571 Loss: 4.5067 Acc: 57.7844% Time: 4332.2006s\n",
            "#572 Loss: 4.9254 Acc: 57.9840% Time: 4339.5893s\n",
            "#573 Loss: 5.3351 Acc: 55.7385% Time: 4347.2469s\n",
            "#574 Loss: 5.7147 Acc: 54.6906% Time: 4354.7619s\n",
            "#575 Loss: 5.2320 Acc: 56.7365% Time: 4362.1481s\n",
            "#576 Loss: 5.8316 Acc: 53.2435% Time: 4369.7608s\n",
            "#577 Loss: 5.4464 Acc: 57.5349% Time: 4377.3891s\n",
            "#578 Loss: 5.2119 Acc: 56.8363% Time: 4384.9562s\n",
            "#579 Loss: 4.5183 Acc: 59.3313% Time: 4392.3716s\n",
            "#580 Loss: 5.0411 Acc: 57.4850% Time: 4399.8048s\n",
            "#581 Loss: 4.5056 Acc: 57.6347% Time: 4407.4034s\n",
            "#582 Loss: 4.8325 Acc: 58.1337% Time: 4414.9607s\n",
            "#583 Loss: 4.8911 Acc: 57.4351% Time: 4422.5881s\n",
            "#584 Loss: 4.7721 Acc: 56.0878% Time: 4430.0902s\n",
            "#585 Loss: 5.2372 Acc: 55.7884% Time: 4437.6316s\n",
            "#586 Loss: 5.9486 Acc: 56.5369% Time: 4445.2025s\n",
            "#587 Loss: 5.0134 Acc: 55.6387% Time: 4452.7578s\n",
            "#588 Loss: 4.5344 Acc: 58.4830% Time: 4460.3639s\n",
            "#589 Loss: 4.7472 Acc: 57.7844% Time: 4467.9742s\n",
            "#590 Loss: 3.9152 Acc: 56.5868% Time: 4475.5649s\n",
            "#591 Loss: 5.1223 Acc: 57.6347% Time: 4483.1768s\n",
            "#592 Loss: 5.3237 Acc: 56.9361% Time: 4490.7793s\n",
            "#593 Loss: 5.7733 Acc: 55.7884% Time: 4498.3466s\n",
            "#594 Loss: 4.7619 Acc: 58.8323% Time: 4505.9527s\n",
            "#595 Loss: 4.7136 Acc: 56.4870% Time: 4513.5174s\n",
            "#596 Loss: 5.3528 Acc: 57.2355% Time: 4521.1174s\n",
            "#597 Loss: 4.5775 Acc: 58.6826% Time: 4528.7035s\n",
            "#598 Loss: 5.8390 Acc: 57.2355% Time: 4536.1448s\n",
            "#599 Loss: 5.0930 Acc: 57.4850% Time: 4543.7594s\n",
            "#600 Loss: 4.4032 Acc: 58.4830% Time: 4551.2640s\n",
            "#601 Loss: 5.1041 Acc: 56.7365% Time: 4558.7194s\n",
            "#602 Loss: 5.3650 Acc: 56.6866% Time: 4566.3179s\n",
            "#603 Loss: 4.4016 Acc: 56.6866% Time: 4573.9694s\n",
            "#604 Loss: 3.8911 Acc: 58.1337% Time: 4581.5015s\n",
            "#605 Loss: 4.0331 Acc: 59.8802% Time: 4589.0659s\n",
            "#606 Loss: 5.1928 Acc: 54.9900% Time: 4596.5174s\n",
            "#607 Loss: 4.1961 Acc: 58.2335% Time: 4604.0909s\n",
            "#608 Loss: 5.3737 Acc: 57.8343% Time: 4611.6754s\n",
            "#609 Loss: 4.1464 Acc: 57.9341% Time: 4619.1567s\n",
            "#610 Loss: 5.2647 Acc: 55.3393% Time: 4626.7390s\n",
            "#611 Loss: 4.5449 Acc: 57.0858% Time: 4634.3283s\n",
            "#612 Loss: 4.5770 Acc: 56.1377% Time: 4641.8921s\n",
            "#613 Loss: 5.7801 Acc: 54.5409% Time: 4649.4881s\n",
            "#614 Loss: 4.9709 Acc: 58.2335% Time: 4657.1370s\n",
            "#615 Loss: 5.4191 Acc: 55.9381% Time: 4664.6617s\n",
            "#616 Loss: 5.0076 Acc: 56.3872% Time: 4672.2797s\n",
            "#617 Loss: 5.3416 Acc: 53.3433% Time: 4679.8537s\n",
            "#618 Loss: 5.4895 Acc: 57.5349% Time: 4687.3283s\n",
            "#619 Loss: 4.8837 Acc: 58.3333% Time: 4694.8855s\n",
            "#620 Loss: 4.6453 Acc: 56.7864% Time: 4702.4450s\n",
            "#621 Loss: 4.2000 Acc: 58.7325% Time: 4709.8546s\n",
            "#622 Loss: 5.5556 Acc: 56.0379% Time: 4717.5506s\n",
            "#623 Loss: 5.0610 Acc: 56.5868% Time: 4725.0582s\n",
            "#624 Loss: 4.7142 Acc: 58.2834% Time: 4732.7096s\n",
            "#625 Loss: 5.5230 Acc: 56.1876% Time: 4740.2611s\n",
            "#626 Loss: 4.9245 Acc: 57.7844% Time: 4747.8824s\n",
            "#627 Loss: 4.7426 Acc: 56.2375% Time: 4755.5398s\n",
            "#628 Loss: 4.5234 Acc: 58.3832% Time: 4763.0168s\n",
            "#629 Loss: 4.7815 Acc: 58.1337% Time: 4770.4778s\n",
            "#630 Loss: 5.3406 Acc: 55.4391% Time: 4778.0743s\n",
            "#631 Loss: 4.4731 Acc: 56.6866% Time: 4785.6746s\n",
            "#632 Loss: 5.3740 Acc: 56.4371% Time: 4793.2203s\n",
            "#633 Loss: 4.4969 Acc: 58.3333% Time: 4800.8484s\n",
            "#634 Loss: 5.2367 Acc: 56.9361% Time: 4808.3969s\n",
            "#635 Loss: 4.8712 Acc: 57.1856% Time: 4815.8704s\n",
            "#636 Loss: 6.2058 Acc: 55.5888% Time: 4823.4673s\n",
            "#637 Loss: 4.8052 Acc: 57.5848% Time: 4830.8722s\n",
            "#638 Loss: 4.9464 Acc: 58.2834% Time: 4838.4038s\n",
            "#639 Loss: 4.6611 Acc: 57.8343% Time: 4846.0182s\n",
            "#640 Loss: 5.2233 Acc: 57.2854% Time: 4853.5798s\n",
            "#641 Loss: 4.6911 Acc: 57.0858% Time: 4861.0807s\n",
            "#642 Loss: 5.1720 Acc: 55.6387% Time: 4868.6145s\n",
            "#643 Loss: 4.8648 Acc: 57.5848% Time: 4876.2122s\n",
            "#644 Loss: 4.8260 Acc: 56.2375% Time: 4883.6387s\n",
            "#645 Loss: 4.6299 Acc: 57.8842% Time: 4891.2565s\n",
            "#646 Loss: 5.1880 Acc: 55.7884% Time: 4898.7409s\n",
            "#647 Loss: 4.9380 Acc: 58.0339% Time: 4906.2358s\n",
            "#648 Loss: 4.1623 Acc: 57.5848% Time: 4913.7656s\n",
            "#649 Loss: 4.7225 Acc: 57.2355% Time: 4921.1989s\n",
            "#650 Loss: 4.5609 Acc: 56.7365% Time: 4928.7947s\n",
            "#651 Loss: 4.9281 Acc: 57.9840% Time: 4936.3823s\n",
            "#652 Loss: 4.4943 Acc: 57.1357% Time: 4943.8096s\n",
            "#653 Loss: 4.9832 Acc: 56.4371% Time: 4951.3358s\n",
            "#654 Loss: 4.5766 Acc: 56.9860% Time: 4958.9909s\n",
            "#655 Loss: 5.7200 Acc: 56.0379% Time: 4966.5938s\n",
            "#656 Loss: 4.6205 Acc: 57.8343% Time: 4974.1907s\n",
            "#657 Loss: 4.5726 Acc: 57.2854% Time: 4981.7877s\n",
            "#658 Loss: 5.9758 Acc: 56.9860% Time: 4989.3581s\n",
            "#659 Loss: 4.3782 Acc: 59.9800% Time: 4996.9282s\n",
            "#660 Loss: 4.5972 Acc: 58.4331% Time: 5004.4025s\n",
            "#661 Loss: 4.7845 Acc: 58.6327% Time: 5011.8985s\n",
            "#662 Loss: 4.0565 Acc: 58.4331% Time: 5019.4001s\n",
            "#663 Loss: 4.3684 Acc: 58.9321% Time: 5026.8933s\n",
            "#664 Loss: 5.9619 Acc: 56.7365% Time: 5034.4136s\n",
            "#665 Loss: 4.6762 Acc: 56.5369% Time: 5041.9469s\n",
            "#666 Loss: 4.6741 Acc: 57.5848% Time: 5049.5509s\n",
            "#667 Loss: 4.8518 Acc: 56.8363% Time: 5057.1367s\n",
            "#668 Loss: 4.1548 Acc: 58.9820% Time: 5064.7505s\n",
            "#669 Loss: 4.8838 Acc: 57.7345% Time: 5072.4439s\n",
            "#670 Loss: 5.2131 Acc: 55.1397% Time: 5080.0064s\n",
            "#671 Loss: 4.7515 Acc: 57.1856% Time: 5087.5957s\n",
            "#672 Loss: 4.7511 Acc: 55.9381% Time: 5095.1405s\n",
            "#673 Loss: 5.1335 Acc: 55.6886% Time: 5102.5543s\n",
            "#674 Loss: 4.7553 Acc: 55.8882% Time: 5110.1277s\n",
            "#675 Loss: 6.0537 Acc: 56.6866% Time: 5117.7320s\n",
            "#676 Loss: 4.0141 Acc: 58.8822% Time: 5125.2907s\n",
            "#677 Loss: 4.7560 Acc: 57.5848% Time: 5132.9120s\n",
            "#678 Loss: 5.1750 Acc: 56.8363% Time: 5140.4966s\n",
            "#679 Loss: 4.7951 Acc: 57.2355% Time: 5148.0886s\n",
            "#680 Loss: 5.7675 Acc: 56.8862% Time: 5155.7552s\n",
            "#681 Loss: 5.0886 Acc: 55.8383% Time: 5163.3207s\n",
            "#682 Loss: 4.7198 Acc: 57.5848% Time: 5170.8640s\n",
            "#683 Loss: 5.7227 Acc: 56.4870% Time: 5178.5091s\n",
            "#684 Loss: 5.5475 Acc: 56.8862% Time: 5186.1137s\n",
            "#685 Loss: 5.7028 Acc: 56.3373% Time: 5193.7372s\n",
            "#686 Loss: 4.3791 Acc: 57.3353% Time: 5201.3187s\n",
            "#687 Loss: 5.2312 Acc: 58.5329% Time: 5208.9279s\n",
            "#688 Loss: 5.2694 Acc: 55.8383% Time: 5216.4492s\n",
            "#689 Loss: 5.7160 Acc: 55.5888% Time: 5223.9593s\n",
            "#690 Loss: 3.7417 Acc: 59.5808% Time: 5231.4757s\n",
            "#691 Loss: 4.6167 Acc: 56.5868% Time: 5239.0861s\n",
            "#692 Loss: 4.5583 Acc: 56.4371% Time: 5246.7225s\n",
            "#693 Loss: 5.0170 Acc: 57.8842% Time: 5254.2946s\n",
            "#694 Loss: 4.6090 Acc: 57.5848% Time: 5261.9253s\n",
            "#695 Loss: 4.5872 Acc: 57.0858% Time: 5269.4614s\n",
            "#696 Loss: 4.4804 Acc: 59.0319% Time: 5277.0809s\n",
            "#697 Loss: 4.5057 Acc: 58.1337% Time: 5284.6979s\n",
            "#698 Loss: 4.3826 Acc: 57.5848% Time: 5292.2838s\n",
            "#699 Loss: 5.7260 Acc: 55.5888% Time: 5299.9487s\n",
            "#700 Loss: 5.1737 Acc: 55.6387% Time: 5307.4690s\n",
            "#701 Loss: 4.7098 Acc: 57.8343% Time: 5315.1012s\n",
            "#702 Loss: 4.6479 Acc: 56.8862% Time: 5322.6493s\n",
            "#703 Loss: 4.1948 Acc: 57.1357% Time: 5330.1165s\n",
            "#704 Loss: 4.9341 Acc: 57.4351% Time: 5337.6279s\n",
            "#705 Loss: 5.8457 Acc: 54.0918% Time: 5345.0916s\n",
            "#706 Loss: 6.0240 Acc: 54.9401% Time: 5352.6426s\n",
            "#707 Loss: 4.2283 Acc: 59.0818% Time: 5360.2471s\n",
            "#708 Loss: 4.6732 Acc: 57.1856% Time: 5367.8971s\n",
            "#709 Loss: 4.2256 Acc: 56.4371% Time: 5375.4129s\n",
            "#710 Loss: 4.8452 Acc: 55.6886% Time: 5382.8894s\n",
            "#711 Loss: 4.7152 Acc: 58.9321% Time: 5390.4427s\n",
            "#712 Loss: 5.2009 Acc: 56.7864% Time: 5398.0956s\n",
            "#713 Loss: 4.8160 Acc: 57.7844% Time: 5405.6906s\n",
            "#714 Loss: 5.3764 Acc: 56.2375% Time: 5413.2736s\n",
            "#715 Loss: 5.7519 Acc: 57.2854% Time: 5420.9013s\n",
            "#716 Loss: 5.4977 Acc: 57.2355% Time: 5428.4787s\n",
            "#717 Loss: 4.9953 Acc: 57.9840% Time: 5436.0624s\n",
            "#718 Loss: 4.2006 Acc: 57.5848% Time: 5443.5888s\n",
            "#719 Loss: 4.0385 Acc: 59.2315% Time: 5451.0805s\n",
            "#720 Loss: 5.1456 Acc: 56.4870% Time: 5458.5980s\n",
            "#721 Loss: 5.0713 Acc: 56.3373% Time: 5466.2359s\n",
            "#722 Loss: 4.2771 Acc: 57.3852% Time: 5473.7947s\n",
            "#723 Loss: 5.8454 Acc: 55.5888% Time: 5481.4522s\n",
            "#724 Loss: 5.3977 Acc: 56.6866% Time: 5488.9699s\n",
            "#725 Loss: 4.2965 Acc: 57.2854% Time: 5496.4308s\n",
            "#726 Loss: 4.7250 Acc: 58.4331% Time: 5503.7895s\n",
            "#727 Loss: 4.8311 Acc: 58.7325% Time: 5511.4119s\n",
            "#728 Loss: 4.7879 Acc: 57.6846% Time: 5518.9990s\n",
            "#729 Loss: 4.7855 Acc: 57.3353% Time: 5526.6022s\n",
            "#730 Loss: 5.4238 Acc: 58.0339% Time: 5534.3118s\n",
            "#731 Loss: 6.3792 Acc: 57.1856% Time: 5541.8361s\n",
            "#732 Loss: 6.8486 Acc: 58.7824% Time: 5549.3935s\n",
            "#733 Loss: 5.5987 Acc: 57.2854% Time: 5556.9718s\n",
            "#734 Loss: 4.6329 Acc: 58.7824% Time: 5564.3982s\n",
            "#735 Loss: 5.6576 Acc: 53.9421% Time: 5572.0229s\n",
            "#736 Loss: 5.0827 Acc: 57.1357% Time: 5579.6007s\n",
            "#737 Loss: 4.3350 Acc: 57.7844% Time: 5587.1482s\n",
            "#738 Loss: 5.1187 Acc: 56.5369% Time: 5594.7583s\n",
            "#739 Loss: 5.1789 Acc: 56.3872% Time: 5602.1875s\n",
            "#740 Loss: 4.9637 Acc: 55.8882% Time: 5609.7976s\n",
            "#741 Loss: 4.5013 Acc: 57.1856% Time: 5617.4394s\n",
            "#742 Loss: 4.4255 Acc: 57.1357% Time: 5624.9502s\n",
            "#743 Loss: 4.7464 Acc: 56.7864% Time: 5632.5788s\n",
            "#744 Loss: 5.0051 Acc: 56.0878% Time: 5640.1741s\n",
            "#745 Loss: 5.5240 Acc: 55.3393% Time: 5647.7611s\n",
            "#746 Loss: 5.1112 Acc: 57.8343% Time: 5655.4048s\n",
            "#747 Loss: 5.0914 Acc: 57.1856% Time: 5662.9638s\n",
            "#748 Loss: 4.9811 Acc: 56.9361% Time: 5670.5579s\n",
            "#749 Loss: 4.6154 Acc: 57.7844% Time: 5678.1258s\n",
            "#750 Loss: 4.5575 Acc: 55.2395% Time: 5685.8491s\n",
            "#751 Loss: 5.3416 Acc: 56.0878% Time: 5693.6288s\n",
            "#752 Loss: 4.4490 Acc: 58.0838% Time: 5701.1450s\n",
            "#753 Loss: 4.4476 Acc: 56.7365% Time: 5708.8497s\n",
            "#754 Loss: 4.7112 Acc: 55.8383% Time: 5716.7767s\n",
            "#755 Loss: 4.5682 Acc: 58.1337% Time: 5724.7537s\n",
            "#756 Loss: 4.7923 Acc: 56.1377% Time: 5732.6212s\n",
            "#757 Loss: 4.5491 Acc: 54.8902% Time: 5740.5474s\n",
            "#758 Loss: 4.0013 Acc: 58.1337% Time: 5748.1883s\n",
            "#759 Loss: 4.8083 Acc: 55.2894% Time: 5755.7798s\n",
            "#760 Loss: 4.0423 Acc: 57.2854% Time: 5763.6350s\n",
            "#761 Loss: 4.4050 Acc: 58.2834% Time: 5771.2004s\n",
            "#762 Loss: 5.3561 Acc: 55.8882% Time: 5778.9156s\n",
            "#763 Loss: 5.3041 Acc: 57.6846% Time: 5786.6297s\n",
            "#764 Loss: 5.0228 Acc: 57.5848% Time: 5794.0972s\n",
            "#765 Loss: 4.6496 Acc: 56.6367% Time: 5801.7516s\n",
            "#766 Loss: 5.3900 Acc: 55.6886% Time: 5809.2702s\n",
            "#767 Loss: 5.0642 Acc: 59.2814% Time: 5816.8851s\n",
            "#768 Loss: 5.1544 Acc: 57.8842% Time: 5824.4857s\n",
            "#769 Loss: 4.7629 Acc: 59.6307% Time: 5832.2726s\n",
            "#770 Loss: 5.0830 Acc: 56.4371% Time: 5839.7716s\n",
            "#771 Loss: 4.6788 Acc: 58.5329% Time: 5847.4039s\n",
            "#772 Loss: 4.9553 Acc: 57.2854% Time: 5855.2643s\n",
            "#773 Loss: 4.7432 Acc: 56.5868% Time: 5863.0035s\n",
            "#774 Loss: 4.4408 Acc: 57.0359% Time: 5870.7932s\n",
            "#775 Loss: 4.7273 Acc: 56.3872% Time: 5880.9112s\n",
            "#776 Loss: 5.4291 Acc: 56.1876% Time: 5890.8408s\n",
            "#777 Loss: 4.7630 Acc: 55.8882% Time: 5900.7081s\n",
            "#778 Loss: 3.8360 Acc: 58.6826% Time: 5910.6252s\n",
            "#779 Loss: 5.1438 Acc: 57.5848% Time: 5920.7248s\n",
            "#780 Loss: 4.3322 Acc: 59.9301% Time: 5930.5058s\n",
            "#781 Loss: 4.5572 Acc: 58.1337% Time: 5938.9860s\n",
            "#782 Loss: 4.7800 Acc: 55.6886% Time: 5946.8514s\n",
            "#783 Loss: 5.0161 Acc: 56.2375% Time: 5954.5120s\n",
            "#784 Loss: 4.8145 Acc: 57.5848% Time: 5962.2645s\n",
            "#785 Loss: 5.0978 Acc: 57.1856% Time: 5972.1410s\n",
            "#786 Loss: 5.5736 Acc: 56.3373% Time: 5980.0710s\n",
            "#787 Loss: 5.1155 Acc: 55.4890% Time: 5987.6598s\n",
            "#788 Loss: 4.3653 Acc: 58.4830% Time: 5995.3932s\n",
            "#789 Loss: 4.4986 Acc: 56.9860% Time: 6005.4925s\n",
            "#790 Loss: 4.9302 Acc: 56.0379% Time: 6013.1652s\n",
            "#791 Loss: 5.0420 Acc: 55.8882% Time: 6021.0396s\n",
            "#792 Loss: 4.7126 Acc: 56.8862% Time: 6028.7955s\n",
            "#793 Loss: 4.4986 Acc: 57.7345% Time: 6036.5617s\n",
            "#794 Loss: 5.3546 Acc: 55.3393% Time: 6044.2452s\n",
            "#795 Loss: 5.4530 Acc: 58.0339% Time: 6051.8560s\n",
            "#796 Loss: 4.3008 Acc: 55.4890% Time: 6059.4929s\n",
            "#797 Loss: 4.4718 Acc: 58.0339% Time: 6067.1722s\n",
            "#798 Loss: 4.6235 Acc: 57.7844% Time: 6075.0107s\n",
            "#799 Loss: 5.1393 Acc: 56.3872% Time: 6082.6548s\n",
            "#800 Loss: 4.6318 Acc: 57.1856% Time: 6090.3221s\n",
            "#801 Loss: 4.6648 Acc: 57.3353% Time: 6098.1615s\n",
            "#802 Loss: 4.8795 Acc: 57.0359% Time: 6108.1492s\n",
            "#803 Loss: 5.0308 Acc: 57.2355% Time: 6117.9166s\n",
            "#804 Loss: 5.4572 Acc: 56.0878% Time: 6127.6203s\n",
            "#805 Loss: 4.6052 Acc: 58.2834% Time: 6137.4378s\n",
            "#806 Loss: 4.7942 Acc: 56.9361% Time: 6147.4065s\n",
            "#807 Loss: 5.1417 Acc: 57.0858% Time: 6157.6764s\n",
            "#808 Loss: 5.2031 Acc: 55.0898% Time: 6167.5205s\n",
            "#809 Loss: 4.5760 Acc: 56.9361% Time: 6177.3188s\n",
            "#810 Loss: 5.7203 Acc: 57.3852% Time: 6185.7185s\n",
            "#811 Loss: 4.8013 Acc: 56.0379% Time: 6193.4709s\n",
            "#812 Loss: 4.8052 Acc: 58.6826% Time: 6201.0669s\n",
            "#813 Loss: 4.8879 Acc: 55.9880% Time: 6208.6099s\n",
            "#814 Loss: 4.6590 Acc: 58.2335% Time: 6216.3822s\n",
            "#815 Loss: 4.3754 Acc: 57.3852% Time: 6226.5765s\n",
            "#816 Loss: 4.1827 Acc: 58.0339% Time: 6236.7690s\n",
            "#817 Loss: 4.6708 Acc: 55.1397% Time: 6246.6592s\n",
            "#818 Loss: 4.5101 Acc: 56.7864% Time: 6254.2567s\n",
            "#819 Loss: 4.2700 Acc: 55.6387% Time: 6261.9348s\n",
            "#820 Loss: 4.5337 Acc: 56.7365% Time: 6269.4203s\n",
            "#821 Loss: 4.5293 Acc: 55.4391% Time: 6276.8946s\n",
            "#822 Loss: 4.4487 Acc: 58.1337% Time: 6284.3331s\n",
            "#823 Loss: 4.2815 Acc: 55.9880% Time: 6291.7979s\n",
            "#824 Loss: 4.8981 Acc: 56.6866% Time: 6299.2829s\n",
            "#825 Loss: 4.0678 Acc: 57.7345% Time: 6306.7849s\n",
            "#826 Loss: 5.0477 Acc: 56.8363% Time: 6314.3182s\n",
            "#827 Loss: 5.2428 Acc: 56.0878% Time: 6321.7355s\n",
            "#828 Loss: 4.6148 Acc: 55.9880% Time: 6329.3381s\n",
            "#829 Loss: 4.3960 Acc: 59.8303% Time: 6336.9022s\n",
            "#830 Loss: 4.6416 Acc: 56.9361% Time: 6344.5740s\n",
            "#831 Loss: 4.9757 Acc: 56.8862% Time: 6352.1280s\n",
            "#832 Loss: 4.5321 Acc: 56.1876% Time: 6359.7237s\n",
            "#833 Loss: 4.8996 Acc: 55.5888% Time: 6367.2870s\n",
            "#834 Loss: 4.8682 Acc: 57.1856% Time: 6374.9568s\n",
            "#835 Loss: 5.9191 Acc: 58.0838% Time: 6382.3301s\n",
            "#836 Loss: 4.6893 Acc: 57.9840% Time: 6389.9647s\n",
            "#837 Loss: 5.0864 Acc: 55.8383% Time: 6397.4200s\n",
            "#838 Loss: 5.3984 Acc: 56.2375% Time: 6404.9784s\n",
            "#839 Loss: 5.2208 Acc: 57.2854% Time: 6412.4788s\n",
            "#840 Loss: 5.5538 Acc: 56.2375% Time: 6419.9476s\n",
            "#841 Loss: 4.7824 Acc: 54.9401% Time: 6427.5020s\n",
            "#842 Loss: 4.1923 Acc: 58.2834% Time: 6435.0802s\n",
            "#843 Loss: 4.9984 Acc: 57.3353% Time: 6442.5402s\n",
            "#844 Loss: 5.3757 Acc: 59.1816% Time: 6450.1464s\n",
            "#845 Loss: 4.7194 Acc: 56.8862% Time: 6457.6814s\n",
            "#846 Loss: 5.0315 Acc: 57.2854% Time: 6465.1135s\n",
            "#847 Loss: 4.6188 Acc: 56.6866% Time: 6472.6617s\n",
            "#848 Loss: 4.9462 Acc: 57.2355% Time: 6480.2568s\n",
            "#849 Loss: 4.9346 Acc: 57.6347% Time: 6487.6975s\n",
            "#850 Loss: 5.4375 Acc: 57.9840% Time: 6495.3452s\n",
            "#851 Loss: 4.6956 Acc: 56.7864% Time: 6502.9117s\n",
            "#852 Loss: 4.1146 Acc: 58.1337% Time: 6510.2835s\n",
            "#853 Loss: 5.1505 Acc: 55.4391% Time: 6517.7638s\n",
            "#854 Loss: 4.9961 Acc: 59.8303% Time: 6525.3133s\n",
            "#855 Loss: 4.1888 Acc: 56.4371% Time: 6532.9717s\n",
            "#856 Loss: 5.0085 Acc: 57.4351% Time: 6540.4981s\n",
            "#857 Loss: 4.8919 Acc: 55.5389% Time: 6548.0699s\n",
            "#858 Loss: 3.9777 Acc: 59.2814% Time: 6555.5121s\n",
            "#859 Loss: 5.2300 Acc: 55.6387% Time: 6563.0492s\n",
            "#860 Loss: 5.6232 Acc: 56.5369% Time: 6570.6846s\n",
            "#861 Loss: 6.3661 Acc: 55.4391% Time: 6578.2817s\n",
            "#862 Loss: 5.3853 Acc: 57.4351% Time: 6585.8477s\n",
            "#863 Loss: 4.8305 Acc: 55.1397% Time: 6593.4868s\n",
            "#864 Loss: 5.8834 Acc: 56.9361% Time: 6601.0339s\n",
            "#865 Loss: 5.2414 Acc: 56.7864% Time: 6608.5975s\n",
            "#866 Loss: 4.8712 Acc: 57.0359% Time: 6616.2425s\n",
            "#867 Loss: 4.8712 Acc: 56.8862% Time: 6623.8484s\n",
            "#868 Loss: 5.1119 Acc: 57.7345% Time: 6631.2824s\n",
            "#869 Loss: 5.0954 Acc: 56.7365% Time: 6638.8397s\n",
            "#870 Loss: 4.7175 Acc: 57.7844% Time: 6646.2398s\n",
            "#871 Loss: 4.6183 Acc: 56.5868% Time: 6653.8145s\n",
            "#872 Loss: 5.2970 Acc: 58.0339% Time: 6661.2495s\n",
            "#873 Loss: 5.3090 Acc: 57.6846% Time: 6668.8583s\n",
            "#874 Loss: 4.4592 Acc: 56.8862% Time: 6676.4385s\n",
            "#875 Loss: 4.5075 Acc: 56.2375% Time: 6684.0436s\n",
            "#876 Loss: 5.3240 Acc: 54.6407% Time: 6691.6207s\n",
            "#877 Loss: 4.8790 Acc: 59.2315% Time: 6699.1876s\n",
            "#878 Loss: 5.0937 Acc: 54.8902% Time: 6706.6810s\n",
            "#879 Loss: 5.2138 Acc: 56.7365% Time: 6714.1741s\n",
            "#880 Loss: 5.3624 Acc: 56.6866% Time: 6721.6676s\n",
            "#881 Loss: 5.1975 Acc: 58.5329% Time: 6729.2227s\n",
            "#882 Loss: 4.7418 Acc: 58.2335% Time: 6736.8405s\n",
            "#883 Loss: 5.2973 Acc: 58.1337% Time: 6744.4063s\n",
            "#884 Loss: 4.1609 Acc: 57.6347% Time: 6751.8565s\n",
            "#885 Loss: 4.3076 Acc: 57.6846% Time: 6759.3855s\n",
            "#886 Loss: 6.0498 Acc: 55.2395% Time: 6766.8465s\n",
            "#887 Loss: 5.0422 Acc: 57.7345% Time: 6774.3862s\n",
            "#888 Loss: 4.6736 Acc: 57.9840% Time: 6781.9636s\n",
            "#889 Loss: 4.7032 Acc: 55.9880% Time: 6789.5515s\n",
            "#890 Loss: 4.9199 Acc: 56.6866% Time: 6797.1862s\n",
            "#891 Loss: 4.7089 Acc: 55.9381% Time: 6804.7019s\n",
            "#892 Loss: 4.1433 Acc: 56.6866% Time: 6812.3539s\n",
            "#893 Loss: 5.3210 Acc: 57.7844% Time: 6820.0002s\n",
            "#894 Loss: 4.8812 Acc: 56.0878% Time: 6827.5437s\n",
            "#895 Loss: 6.3879 Acc: 54.9401% Time: 6835.0354s\n",
            "#896 Loss: 5.8308 Acc: 57.2355% Time: 6842.5838s\n",
            "#897 Loss: 4.5843 Acc: 57.9341% Time: 6850.1765s\n",
            "#898 Loss: 4.8236 Acc: 57.0359% Time: 6857.6159s\n",
            "#899 Loss: 4.1992 Acc: 57.5848% Time: 6865.1947s\n",
            "#900 Loss: 4.2554 Acc: 58.1836% Time: 6872.5785s\n",
            "#901 Loss: 4.2953 Acc: 58.7325% Time: 6880.2190s\n",
            "#902 Loss: 4.1129 Acc: 57.8842% Time: 6887.7534s\n",
            "#903 Loss: 4.0485 Acc: 57.9840% Time: 6895.3407s\n",
            "#904 Loss: 4.4255 Acc: 56.3872% Time: 6902.7497s\n",
            "#905 Loss: 5.4211 Acc: 57.7844% Time: 6910.1630s\n",
            "#906 Loss: 5.6405 Acc: 57.0359% Time: 6917.7233s\n",
            "#907 Loss: 5.1412 Acc: 56.9361% Time: 6925.2042s\n",
            "#908 Loss: 4.8610 Acc: 57.2854% Time: 6932.7309s\n",
            "#909 Loss: 5.1122 Acc: 56.4870% Time: 6940.2300s\n",
            "#910 Loss: 5.0572 Acc: 57.4351% Time: 6947.7565s\n",
            "#911 Loss: 4.4597 Acc: 58.0838% Time: 6955.3154s\n",
            "#912 Loss: 4.9193 Acc: 57.8343% Time: 6962.9501s\n",
            "#913 Loss: 5.5376 Acc: 55.1896% Time: 6970.5378s\n",
            "#914 Loss: 4.6916 Acc: 55.5888% Time: 6978.0933s\n",
            "#915 Loss: 5.2960 Acc: 56.5868% Time: 6985.5445s\n",
            "#916 Loss: 4.7287 Acc: 57.4850% Time: 6993.0839s\n",
            "#917 Loss: 4.7338 Acc: 58.1836% Time: 7000.6877s\n",
            "#918 Loss: 4.5342 Acc: 55.9381% Time: 7008.1581s\n",
            "#919 Loss: 5.4240 Acc: 55.6387% Time: 7015.7564s\n",
            "#920 Loss: 4.9600 Acc: 54.9401% Time: 7023.3859s\n",
            "#921 Loss: 5.5177 Acc: 55.2894% Time: 7030.9238s\n",
            "#922 Loss: 4.2329 Acc: 60.5289% Time: 7038.5645s\n",
            "#923 Loss: 5.5593 Acc: 55.6886% Time: 7046.1072s\n",
            "#924 Loss: 4.6748 Acc: 57.3852% Time: 7053.6685s\n",
            "#925 Loss: 4.3662 Acc: 56.5369% Time: 7061.2716s\n",
            "#926 Loss: 5.5250 Acc: 57.1357% Time: 7068.7003s\n",
            "#927 Loss: 5.3234 Acc: 58.8822% Time: 7076.1184s\n",
            "#928 Loss: 4.1070 Acc: 58.7824% Time: 7083.7187s\n",
            "#929 Loss: 4.2614 Acc: 57.7345% Time: 7091.3139s\n",
            "#930 Loss: 4.7750 Acc: 56.3373% Time: 7098.8865s\n",
            "#931 Loss: 5.1653 Acc: 57.1357% Time: 7106.3977s\n",
            "#932 Loss: 4.7293 Acc: 56.3373% Time: 7113.9476s\n",
            "#933 Loss: 4.5767 Acc: 57.6846% Time: 7121.4261s\n",
            "#934 Loss: 5.7186 Acc: 56.6866% Time: 7129.0476s\n",
            "#935 Loss: 4.2221 Acc: 57.5848% Time: 7136.4853s\n",
            "#936 Loss: 4.7678 Acc: 56.8363% Time: 7144.1370s\n",
            "#937 Loss: 4.4034 Acc: 55.8882% Time: 7151.7204s\n",
            "#938 Loss: 4.4065 Acc: 57.2854% Time: 7159.2934s\n",
            "#939 Loss: 4.5349 Acc: 58.5828% Time: 7166.9115s\n",
            "#940 Loss: 5.1064 Acc: 56.3373% Time: 7174.4365s\n",
            "#941 Loss: 4.5277 Acc: 57.8842% Time: 7181.9013s\n",
            "#942 Loss: 4.5937 Acc: 57.1357% Time: 7189.4757s\n",
            "#943 Loss: 4.7905 Acc: 56.2874% Time: 7197.0585s\n",
            "#944 Loss: 5.2309 Acc: 55.4890% Time: 7204.4747s\n",
            "#945 Loss: 5.7297 Acc: 57.4351% Time: 7211.9944s\n",
            "#946 Loss: 5.2079 Acc: 56.7365% Time: 7219.4507s\n",
            "#947 Loss: 5.2255 Acc: 57.5848% Time: 7227.0472s\n",
            "#948 Loss: 5.3915 Acc: 56.3872% Time: 7234.6382s\n",
            "#949 Loss: 5.4530 Acc: 55.6886% Time: 7242.2256s\n",
            "#950 Loss: 5.2337 Acc: 55.4391% Time: 7249.7636s\n",
            "#951 Loss: 4.4789 Acc: 57.2355% Time: 7257.2504s\n",
            "#952 Loss: 5.3364 Acc: 56.1377% Time: 7264.7550s\n",
            "#953 Loss: 4.7182 Acc: 56.9860% Time: 7272.4246s\n",
            "#954 Loss: 5.0748 Acc: 55.7884% Time: 7280.0844s\n",
            "#955 Loss: 4.4264 Acc: 56.7864% Time: 7287.6153s\n",
            "#956 Loss: 4.0303 Acc: 56.8862% Time: 7295.2274s\n",
            "#957 Loss: 4.6666 Acc: 55.4890% Time: 7302.8033s\n",
            "#958 Loss: 3.7170 Acc: 57.6347% Time: 7310.2726s\n",
            "#959 Loss: 4.5866 Acc: 57.7345% Time: 7317.7872s\n",
            "#960 Loss: 4.4303 Acc: 56.4371% Time: 7325.4310s\n",
            "#961 Loss: 5.8378 Acc: 54.4910% Time: 7332.8392s\n",
            "#962 Loss: 4.6919 Acc: 57.2355% Time: 7340.4167s\n",
            "#963 Loss: 5.7535 Acc: 55.6886% Time: 7347.8126s\n",
            "#964 Loss: 4.9148 Acc: 57.4351% Time: 7355.3786s\n",
            "#965 Loss: 5.0746 Acc: 57.0858% Time: 7363.0147s\n",
            "#966 Loss: 4.2481 Acc: 58.9820% Time: 7370.5972s\n",
            "#967 Loss: 4.6513 Acc: 56.6866% Time: 7377.9900s\n",
            "#968 Loss: 4.2797 Acc: 57.6347% Time: 7385.5707s\n",
            "#969 Loss: 5.9452 Acc: 56.1377% Time: 7392.9965s\n",
            "#970 Loss: 4.9517 Acc: 56.8862% Time: 7400.3836s\n",
            "#971 Loss: 4.4356 Acc: 58.4830% Time: 7407.8133s\n",
            "#972 Loss: 4.1646 Acc: 56.7365% Time: 7415.5071s\n",
            "#973 Loss: 5.2204 Acc: 54.6407% Time: 7423.2927s\n",
            "#974 Loss: 4.5635 Acc: 56.2874% Time: 7431.0888s\n",
            "#975 Loss: 4.3706 Acc: 58.0339% Time: 7438.8620s\n",
            "#976 Loss: 4.4267 Acc: 56.6367% Time: 7446.6667s\n",
            "#977 Loss: 4.0403 Acc: 57.6347% Time: 7454.2053s\n",
            "#978 Loss: 5.2666 Acc: 55.6886% Time: 7461.7451s\n",
            "#979 Loss: 4.9416 Acc: 58.0339% Time: 7469.1639s\n",
            "#980 Loss: 5.0284 Acc: 58.8323% Time: 7476.7558s\n",
            "#981 Loss: 5.3545 Acc: 55.5389% Time: 7484.3586s\n",
            "#982 Loss: 4.0765 Acc: 57.1856% Time: 7491.7264s\n",
            "#983 Loss: 4.0790 Acc: 56.5868% Time: 7499.2029s\n",
            "#984 Loss: 4.3022 Acc: 55.5888% Time: 7506.8651s\n",
            "#985 Loss: 3.8509 Acc: 58.1836% Time: 7514.6179s\n",
            "#986 Loss: 4.1941 Acc: 55.9880% Time: 7522.4467s\n",
            "#987 Loss: 5.1287 Acc: 57.0858% Time: 7532.2113s\n",
            "#988 Loss: 4.7190 Acc: 56.7864% Time: 7541.7056s\n",
            "#989 Loss: 4.8283 Acc: 56.2874% Time: 7551.0786s\n",
            "#990 Loss: 4.5365 Acc: 56.9361% Time: 7560.7006s\n",
            "#991 Loss: 4.5856 Acc: 57.2854% Time: 7570.2811s\n",
            "#992 Loss: 4.1541 Acc: 59.4810% Time: 7579.6626s\n",
            "#993 Loss: 5.9981 Acc: 55.6387% Time: 7587.1310s\n",
            "#994 Loss: 4.7917 Acc: 55.5888% Time: 7594.6910s\n",
            "#995 Loss: 5.8413 Acc: 57.1856% Time: 7602.1398s\n",
            "#996 Loss: 4.6330 Acc: 57.8842% Time: 7609.7143s\n",
            "#997 Loss: 7.2556 Acc: 54.9900% Time: 7617.3690s\n",
            "#998 Loss: 4.5813 Acc: 57.7844% Time: 7626.5105s\n",
            "#999 Loss: 4.7870 Acc: 57.5848% Time: 7636.2361s\n"
          ]
        }
      ],
      "source": [
        "# num_epochs = 200\n",
        "# model.train()\n",
        "# start_time = time.time()\n",
        "# writer = SummaryWriter()\n",
        "\n",
        "# # 전체 반복(epoch) 수 만큼 반복하며\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.\n",
        "#     running_corrects = 0\n",
        "\n",
        "#     # 배치 단위로 학습 데이터 불러오기\n",
        "#     for inputs, labels in train_dataloader:\n",
        "#         inputs = inputs.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         # 모델에 입력(forward)하고 결과 계산\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#     epoch_loss = running_loss / len(train_datasets)\n",
        "#     epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "#     writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "#     writer.add_scalar(\"acc/train\", epoch_acc, epoch)\n",
        "#     writer.add_scalar(\"sum/train\", epoch_loss, epoch)\n",
        "#     # writer.add_scalar(\"sum/train\", epoch_acc/100, epoch)\n",
        "\n",
        "#     # 학습 과정 중에 결과 출력\n",
        "#     print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "# writer.flush()\n",
        "# writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Loss: 0.9142 Acc: 52.6447% Time: 7.7531s\n",
            "#1 Loss: 0.8453 Acc: 55.2395% Time: 15.4465s\n",
            "#2 Loss: 0.8730 Acc: 56.4371% Time: 23.0968s\n",
            "#3 Loss: 0.9051 Acc: 53.4431% Time: 30.8713s\n",
            "#4 Loss: 0.8011 Acc: 57.7345% Time: 38.4409s\n",
            "#5 Loss: 0.8252 Acc: 58.6327% Time: 46.2615s\n",
            "#6 Loss: 0.8683 Acc: 57.5349% Time: 53.8623s\n",
            "#7 Loss: 0.8586 Acc: 58.2834% Time: 61.4949s\n",
            "#8 Loss: 0.8649 Acc: 57.5349% Time: 69.2707s\n",
            "#9 Loss: 0.7985 Acc: 58.3333% Time: 76.8554s\n",
            "#10 Loss: 0.9021 Acc: 57.6347% Time: 84.5024s\n",
            "#11 Loss: 0.9106 Acc: 57.2854% Time: 92.0572s\n",
            "#12 Loss: 0.8071 Acc: 56.9860% Time: 99.5737s\n",
            "#13 Loss: 0.8517 Acc: 57.8343% Time: 107.1667s\n",
            "#14 Loss: 0.9366 Acc: 57.2854% Time: 114.8875s\n",
            "#15 Loss: 0.8437 Acc: 60.0299% Time: 122.5232s\n",
            "#16 Loss: 0.8869 Acc: 57.6846% Time: 130.1160s\n",
            "#17 Loss: 0.8557 Acc: 58.0339% Time: 137.5708s\n",
            "#18 Loss: 0.8646 Acc: 58.9321% Time: 144.9833s\n",
            "#19 Loss: 0.8591 Acc: 58.0838% Time: 152.2857s\n",
            "#20 Loss: 0.8292 Acc: 58.0339% Time: 159.7040s\n",
            "#21 Loss: 0.8561 Acc: 58.0838% Time: 167.1491s\n",
            "#22 Loss: 0.8175 Acc: 58.2335% Time: 174.4964s\n",
            "#23 Loss: 0.8961 Acc: 57.4850% Time: 181.8723s\n",
            "#24 Loss: 0.8206 Acc: 59.8802% Time: 189.2479s\n",
            "#25 Loss: 0.8655 Acc: 55.8882% Time: 196.6409s\n",
            "#26 Loss: 0.8555 Acc: 58.9321% Time: 204.0996s\n",
            "#27 Loss: 0.8012 Acc: 59.1317% Time: 211.5759s\n",
            "#28 Loss: 0.8765 Acc: 59.0818% Time: 219.0607s\n",
            "#29 Loss: 0.7996 Acc: 60.3293% Time: 226.4067s\n",
            "#30 Loss: 0.8379 Acc: 58.0838% Time: 233.8885s\n",
            "#31 Loss: 0.9175 Acc: 58.7824% Time: 241.2647s\n",
            "#32 Loss: 0.8253 Acc: 57.1856% Time: 248.8746s\n",
            "#33 Loss: 0.8095 Acc: 59.3812% Time: 256.2821s\n",
            "#34 Loss: 0.8427 Acc: 57.9341% Time: 263.6862s\n",
            "#35 Loss: 0.8337 Acc: 58.2335% Time: 271.0468s\n",
            "#36 Loss: 0.8847 Acc: 58.5828% Time: 278.4230s\n",
            "#37 Loss: 0.7983 Acc: 59.5309% Time: 285.8556s\n",
            "#38 Loss: 0.7955 Acc: 59.3812% Time: 293.2660s\n",
            "#39 Loss: 0.7899 Acc: 59.2814% Time: 300.6264s\n",
            "#40 Loss: 0.8497 Acc: 58.4331% Time: 308.0328s\n",
            "#41 Loss: 0.8632 Acc: 57.9341% Time: 315.3934s\n",
            "#42 Loss: 0.8180 Acc: 60.1297% Time: 322.8764s\n",
            "#43 Loss: 0.8062 Acc: 60.9281% Time: 330.4092s\n",
            "#44 Loss: 0.8318 Acc: 60.1796% Time: 337.8776s\n",
            "#45 Loss: 0.8210 Acc: 59.6307% Time: 345.3930s\n",
            "#46 Loss: 0.7686 Acc: 62.4251% Time: 352.8773s\n",
            "#47 Loss: 0.9599 Acc: 57.4850% Time: 360.3937s\n",
            "#48 Loss: 0.8918 Acc: 60.1297% Time: 367.8631s\n",
            "#49 Loss: 0.8478 Acc: 60.4291% Time: 375.3955s\n",
            "#50 Loss: 0.8420 Acc: 57.8343% Time: 382.8628s\n",
            "#51 Loss: 0.8340 Acc: 57.9840% Time: 390.4102s\n",
            "#52 Loss: 0.8519 Acc: 57.6846% Time: 397.9729s\n",
            "#53 Loss: 0.7920 Acc: 58.8822% Time: 405.4547s\n",
            "#54 Loss: 0.8717 Acc: 56.8862% Time: 412.8792s\n",
            "#55 Loss: 0.8190 Acc: 59.3313% Time: 420.2087s\n",
            "#56 Loss: 0.8036 Acc: 59.1816% Time: 427.6628s\n",
            "#57 Loss: 0.7352 Acc: 63.3733% Time: 435.1663s\n",
            "#58 Loss: 0.8912 Acc: 58.3333% Time: 442.5833s\n",
            "#59 Loss: 0.8077 Acc: 61.3772% Time: 450.0377s\n",
            "#60 Loss: 0.8299 Acc: 59.2814% Time: 457.3993s\n",
            "#61 Loss: 0.8759 Acc: 58.6327% Time: 464.8594s\n",
            "#62 Loss: 0.7759 Acc: 60.9780% Time: 472.2099s\n",
            "#63 Loss: 0.7902 Acc: 61.4271% Time: 479.7425s\n",
            "#64 Loss: 0.7921 Acc: 59.8802% Time: 487.1752s\n",
            "#65 Loss: 0.7745 Acc: 62.0758% Time: 494.5386s\n",
            "#66 Loss: 0.8576 Acc: 59.3313% Time: 501.9297s\n",
            "#67 Loss: 0.8493 Acc: 57.9840% Time: 509.3384s\n",
            "#68 Loss: 0.8680 Acc: 59.5309% Time: 516.8397s\n",
            "#69 Loss: 0.8055 Acc: 60.7784% Time: 524.1868s\n",
            "#70 Loss: 0.8062 Acc: 61.4770% Time: 531.5725s\n",
            "#71 Loss: 0.7837 Acc: 60.6287% Time: 538.9633s\n",
            "#72 Loss: 0.7645 Acc: 60.1297% Time: 546.3395s\n",
            "#73 Loss: 0.8590 Acc: 59.7305% Time: 553.7602s\n",
            "#74 Loss: 0.9051 Acc: 58.3333% Time: 561.1201s\n",
            "#75 Loss: 0.8196 Acc: 59.2315% Time: 568.5277s\n",
            "#76 Loss: 0.8939 Acc: 57.4850% Time: 575.9517s\n",
            "#77 Loss: 0.8650 Acc: 58.8822% Time: 583.3237s\n",
            "#78 Loss: 0.9145 Acc: 57.7844% Time: 590.7933s\n",
            "#79 Loss: 0.8180 Acc: 59.9301% Time: 598.1689s\n",
            "#80 Loss: 0.8666 Acc: 60.3792% Time: 605.4680s\n",
            "#81 Loss: 0.8296 Acc: 61.2275% Time: 612.9035s\n",
            "#82 Loss: 0.8757 Acc: 56.9860% Time: 620.2839s\n",
            "#83 Loss: 0.8346 Acc: 60.0798% Time: 627.7017s\n",
            "#84 Loss: 0.7918 Acc: 59.9800% Time: 635.1093s\n",
            "#85 Loss: 0.8273 Acc: 58.1836% Time: 642.5447s\n",
            "#86 Loss: 0.7515 Acc: 63.2236% Time: 649.9515s\n",
            "#87 Loss: 0.8110 Acc: 60.6786% Time: 657.3395s\n",
            "#88 Loss: 0.8618 Acc: 59.8303% Time: 664.7016s\n",
            "#89 Loss: 0.8619 Acc: 59.3812% Time: 672.1082s\n",
            "#90 Loss: 0.9152 Acc: 56.8363% Time: 679.3588s\n",
            "#91 Loss: 0.8198 Acc: 60.2794% Time: 686.6728s\n",
            "#92 Loss: 0.7887 Acc: 59.4810% Time: 694.1099s\n",
            "#93 Loss: 0.8360 Acc: 58.4830% Time: 701.4716s\n",
            "#94 Loss: 0.8422 Acc: 58.7325% Time: 708.6915s\n",
            "#95 Loss: 0.8339 Acc: 59.0319% Time: 716.1303s\n",
            "#96 Loss: 0.9214 Acc: 57.8842% Time: 723.5053s\n",
            "#97 Loss: 0.8783 Acc: 58.9321% Time: 730.8981s\n",
            "#98 Loss: 0.8441 Acc: 58.9321% Time: 738.2543s\n",
            "#99 Loss: 0.8576 Acc: 59.4311% Time: 745.4889s\n",
            "#100 Loss: 0.8044 Acc: 61.0778% Time: 752.9428s\n",
            "#101 Loss: 0.7897 Acc: 59.8802% Time: 760.2721s\n",
            "#102 Loss: 0.8581 Acc: 58.3832% Time: 767.6459s\n",
            "#103 Loss: 0.8489 Acc: 60.5788% Time: 775.0681s\n",
            "#104 Loss: 0.7541 Acc: 62.6248% Time: 782.4622s\n",
            "#105 Loss: 0.8004 Acc: 59.1317% Time: 789.8677s\n",
            "#106 Loss: 0.8776 Acc: 59.9800% Time: 797.2246s\n",
            "#107 Loss: 0.8255 Acc: 61.1776% Time: 804.5066s\n",
            "#108 Loss: 0.8250 Acc: 58.5828% Time: 811.9289s\n",
            "#109 Loss: 0.7798 Acc: 61.5768% Time: 819.2279s\n",
            "#110 Loss: 0.7891 Acc: 62.4251% Time: 826.6169s\n",
            "#111 Loss: 0.8249 Acc: 59.6307% Time: 834.0547s\n",
            "#112 Loss: 0.8400 Acc: 58.4830% Time: 841.4463s\n",
            "#113 Loss: 0.8993 Acc: 59.6806% Time: 848.8331s\n",
            "#114 Loss: 0.8479 Acc: 60.8782% Time: 856.2111s\n",
            "#115 Loss: 0.7962 Acc: 60.8283% Time: 863.6496s\n",
            "#116 Loss: 0.8079 Acc: 59.9301% Time: 871.0577s\n",
            "#117 Loss: 0.8044 Acc: 61.0279% Time: 878.4506s\n",
            "#118 Loss: 0.8568 Acc: 58.6826% Time: 885.8057s\n",
            "#119 Loss: 0.8454 Acc: 59.7305% Time: 893.2437s\n",
            "#120 Loss: 0.8262 Acc: 58.5329% Time: 900.6335s\n",
            "#121 Loss: 0.8148 Acc: 61.0279% Time: 908.0713s\n",
            "#122 Loss: 0.8702 Acc: 58.0838% Time: 915.4778s\n",
            "#123 Loss: 0.8246 Acc: 60.1297% Time: 922.7916s\n",
            "#124 Loss: 0.8270 Acc: 59.5808% Time: 930.2470s\n",
            "#125 Loss: 0.8721 Acc: 58.1337% Time: 937.6042s\n",
            "#126 Loss: 0.8618 Acc: 58.8822% Time: 945.0739s\n",
            "#127 Loss: 0.9308 Acc: 57.6846% Time: 952.4496s\n",
            "#128 Loss: 0.8511 Acc: 59.6806% Time: 959.8101s\n",
            "#129 Loss: 0.8302 Acc: 58.9820% Time: 967.1835s\n",
            "#130 Loss: 0.8281 Acc: 60.2794% Time: 974.4344s\n",
            "#131 Loss: 0.8517 Acc: 58.5828% Time: 981.8099s\n",
            "#132 Loss: 0.8151 Acc: 60.8283% Time: 989.1547s\n",
            "#133 Loss: 0.8780 Acc: 59.4810% Time: 996.5557s\n",
            "#134 Loss: 0.9018 Acc: 58.4331% Time: 1003.9352s\n",
            "#135 Loss: 0.8578 Acc: 58.8323% Time: 1011.3592s\n",
            "#136 Loss: 0.8901 Acc: 57.7345% Time: 1018.7655s\n",
            "#137 Loss: 0.8095 Acc: 59.7305% Time: 1026.1478s\n",
            "#138 Loss: 0.7841 Acc: 58.1836% Time: 1033.5459s\n",
            "#139 Loss: 0.8323 Acc: 58.8323% Time: 1040.7483s\n",
            "#140 Loss: 0.8487 Acc: 59.1317% Time: 1048.1394s\n",
            "#141 Loss: 0.8416 Acc: 57.8343% Time: 1055.5150s\n",
            "#142 Loss: 0.8296 Acc: 57.6347% Time: 1062.8905s\n",
            "#143 Loss: 0.7945 Acc: 61.4770% Time: 1070.2983s\n",
            "#144 Loss: 0.8431 Acc: 59.7804% Time: 1077.7030s\n",
            "#145 Loss: 0.8473 Acc: 61.3273% Time: 1084.9845s\n",
            "#146 Loss: 0.8463 Acc: 59.6307% Time: 1092.3605s\n",
            "#147 Loss: 0.7986 Acc: 59.4810% Time: 1099.6573s\n",
            "#148 Loss: 0.9086 Acc: 58.8822% Time: 1106.9852s\n",
            "#149 Loss: 0.8620 Acc: 58.4331% Time: 1114.3297s\n",
            "#150 Loss: 0.8396 Acc: 59.4311% Time: 1121.7533s\n",
            "#151 Loss: 0.8361 Acc: 58.4331% Time: 1129.1922s\n",
            "#152 Loss: 0.9503 Acc: 58.6327% Time: 1136.5343s\n",
            "#153 Loss: 0.8355 Acc: 59.8802% Time: 1144.0024s\n",
            "#154 Loss: 0.8160 Acc: 59.9301% Time: 1151.4100s\n",
            "#155 Loss: 0.8498 Acc: 59.2315% Time: 1158.8332s\n",
            "#156 Loss: 0.7837 Acc: 60.9780% Time: 1166.2682s\n",
            "#157 Loss: 0.8572 Acc: 56.7365% Time: 1173.7374s\n",
            "#158 Loss: 0.9253 Acc: 59.5309% Time: 1181.3167s\n",
            "#159 Loss: 0.8088 Acc: 60.4790% Time: 1188.6448s\n",
            "#160 Loss: 0.7565 Acc: 61.4271% Time: 1196.0818s\n",
            "#161 Loss: 0.7656 Acc: 61.4271% Time: 1203.5818s\n",
            "#162 Loss: 0.7728 Acc: 62.6248% Time: 1210.8340s\n",
            "#163 Loss: 0.8540 Acc: 59.7305% Time: 1218.1596s\n",
            "#164 Loss: 0.8540 Acc: 57.9341% Time: 1225.4262s\n",
            "#165 Loss: 0.8016 Acc: 60.4291% Time: 1232.6183s\n",
            "#166 Loss: 0.8441 Acc: 60.0299% Time: 1239.9487s\n",
            "#167 Loss: 0.7866 Acc: 60.9281% Time: 1247.1623s\n",
            "#168 Loss: 0.8120 Acc: 59.5808% Time: 1254.3972s\n",
            "#169 Loss: 0.8253 Acc: 59.8802% Time: 1261.7570s\n",
            "#170 Loss: 0.8571 Acc: 59.7804% Time: 1268.9939s\n",
            "#171 Loss: 0.8066 Acc: 59.5808% Time: 1276.1939s\n",
            "#172 Loss: 0.8105 Acc: 59.4311% Time: 1283.3825s\n",
            "#173 Loss: 0.8829 Acc: 60.9780% Time: 1290.5866s\n",
            "#174 Loss: 0.8715 Acc: 58.5329% Time: 1297.7731s\n",
            "#175 Loss: 0.8784 Acc: 59.3313% Time: 1304.9794s\n",
            "#176 Loss: 0.7993 Acc: 62.0259% Time: 1312.1668s\n",
            "#177 Loss: 0.8548 Acc: 59.0818% Time: 1319.3867s\n",
            "#178 Loss: 0.7943 Acc: 61.1277% Time: 1326.5727s\n",
            "#179 Loss: 0.7752 Acc: 60.8782% Time: 1333.8078s\n",
            "#180 Loss: 0.7843 Acc: 60.4291% Time: 1341.1577s\n",
            "#181 Loss: 0.8939 Acc: 58.2335% Time: 1348.3703s\n",
            "#182 Loss: 0.8044 Acc: 60.1297% Time: 1355.7156s\n",
            "#183 Loss: 0.8752 Acc: 60.5788% Time: 1363.1068s\n",
            "#184 Loss: 0.8069 Acc: 61.1776% Time: 1370.3424s\n",
            "#185 Loss: 0.7967 Acc: 59.5808% Time: 1377.5440s\n",
            "#186 Loss: 0.8277 Acc: 59.3313% Time: 1384.7633s\n",
            "#187 Loss: 0.8718 Acc: 59.2814% Time: 1392.1400s\n",
            "#188 Loss: 0.8661 Acc: 58.1337% Time: 1399.3287s\n",
            "#189 Loss: 0.8148 Acc: 60.4790% Time: 1406.5611s\n",
            "#190 Loss: 0.7962 Acc: 60.1796% Time: 1413.9064s\n",
            "#191 Loss: 0.8864 Acc: 58.1836% Time: 1421.1577s\n",
            "#192 Loss: 0.7906 Acc: 59.7305% Time: 1428.3596s\n",
            "#193 Loss: 0.8302 Acc: 58.9820% Time: 1435.5470s\n",
            "#194 Loss: 0.7866 Acc: 61.1277% Time: 1442.8916s\n",
            "#195 Loss: 0.7819 Acc: 61.6267% Time: 1450.1593s\n",
            "#196 Loss: 0.8837 Acc: 56.4371% Time: 1457.5005s\n",
            "#197 Loss: 0.8276 Acc: 59.9301% Time: 1464.7204s\n",
            "#198 Loss: 0.8575 Acc: 59.1317% Time: 1472.0796s\n",
            "#199 Loss: 0.8179 Acc: 60.3293% Time: 1479.3178s\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 200\n",
        "# model.train()\n",
        "start_time = time.time()\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# 전체 반복(epoch) 수 만큼 반복하며\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    # 배치 단위로 학습 데이터 불러오기\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # 모델에 입력(forward)하고 결과 계산\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/train\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/train\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "    writer.add_scalar(\"Loss/sum\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"Acc/sum\", epoch_acc, epoch)\n",
        "    writer.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_acc_train': epoch_acc/100}, epoch)\n",
        "\n",
        "    # 학습 과정 중에 결과 출력\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
        "\n",
        "    #save\n",
        "    torch.save(model.state_dict(), f'C:/team3/resnet/models/#4 resnet_models/resnet_dict{epoch}.pth')\n",
        "\n",
        "# writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Test Phase] Loss: 0.6071 Acc: 67.9065% Time: 1789.3857s\n",
            "[Test Phase] Loss: 0.8138 Acc: 52.8637% Time: 1798.6918s\n",
            "[Test Phase] Loss: 1.3549 Acc: 33.5747% Time: 1808.1645s\n",
            "[Test Phase] Loss: 0.3653 Acc: 83.8381% Time: 1817.6990s\n",
            "[Test Phase] Loss: 0.3081 Acc: 86.5043% Time: 1826.9350s\n",
            "[Test Phase] Loss: 0.2908 Acc: 88.0843% Time: 1836.1229s\n",
            "[Test Phase] Loss: 1.7251 Acc: 28.2093% Time: 1845.3244s\n",
            "[Test Phase] Loss: 0.5575 Acc: 73.5681% Time: 1854.4666s\n",
            "[Test Phase] Loss: 0.4753 Acc: 77.3864% Time: 1863.8304s\n",
            "[Test Phase] Loss: 1.6446 Acc: 27.3864% Time: 1873.0500s\n",
            "[Test Phase] Loss: 0.4563 Acc: 80.0856% Time: 1882.2722s\n",
            "[Test Phase] Loss: 0.3694 Acc: 84.5622% Time: 1891.6343s\n",
            "[Test Phase] Loss: 0.7714 Acc: 61.9487% Time: 1900.8334s\n",
            "[Test Phase] Loss: 0.9938 Acc: 50.3950% Time: 1910.0348s\n",
            "[Test Phase] Loss: 1.2493 Acc: 39.3680% Time: 1919.2334s\n",
            "[Test Phase] Loss: 1.0217 Acc: 49.4075% Time: 1928.4666s\n",
            "[Test Phase] Loss: 0.4227 Acc: 80.8097% Time: 1937.8321s\n",
            "[Test Phase] Loss: 0.3531 Acc: 84.9901% Time: 1947.0831s\n",
            "[Test Phase] Loss: 0.7137 Acc: 66.9519% Time: 1956.2696s\n",
            "[Test Phase] Loss: 0.4200 Acc: 80.6452% Time: 1965.6126s\n",
            "[Test Phase] Loss: 0.8042 Acc: 60.2370% Time: 1974.8472s\n",
            "[Test Phase] Loss: 0.2091 Acc: 92.7255% Time: 1984.0135s\n",
            "[Test Phase] Loss: 0.4694 Acc: 78.9335% Time: 1993.1695s\n",
            "[Test Phase] Loss: 1.0080 Acc: 51.4154% Time: 2002.3958s\n",
            "[Test Phase] Loss: 2.2396 Acc: 13.1666% Time: 2011.6144s\n",
            "[Test Phase] Loss: 1.0773 Acc: 50.0000% Time: 2020.8790s\n",
            "[Test Phase] Loss: 1.3977 Acc: 37.2284% Time: 2030.0478s\n",
            "[Test Phase] Loss: 0.2343 Acc: 90.9151% Time: 2039.1706s\n",
            "[Test Phase] Loss: 0.6324 Acc: 69.8157% Time: 2048.4008s\n",
            "[Test Phase] Loss: 1.7112 Acc: 26.9585% Time: 2057.5795s\n",
            "[Test Phase] Loss: 1.1666 Acc: 43.0217% Time: 2066.7645s\n",
            "[Test Phase] Loss: 0.6048 Acc: 70.9348% Time: 2077.0240s\n",
            "[Test Phase] Loss: 1.1059 Acc: 49.7038% Time: 2086.2575s\n",
            "[Test Phase] Loss: 0.1359 Acc: 96.3134% Time: 2095.6488s\n",
            "[Test Phase] Loss: 0.3948 Acc: 82.3897% Time: 2104.9597s\n",
            "[Test Phase] Loss: 0.3980 Acc: 82.3239% Time: 2114.2218s\n",
            "[Test Phase] Loss: 0.3955 Acc: 82.8835% Time: 2123.4242s\n",
            "[Test Phase] Loss: 2.0087 Acc: 18.4332% Time: 2132.7821s\n",
            "[Test Phase] Loss: 0.3538 Acc: 84.3976% Time: 2142.0018s\n",
            "[Test Phase] Loss: 0.2951 Acc: 87.8539% Time: 2151.1967s\n",
            "[Test Phase] Loss: 1.0981 Acc: 50.6912% Time: 2160.4098s\n",
            "[Test Phase] Loss: 0.9463 Acc: 53.4562% Time: 2169.5961s\n",
            "[Test Phase] Loss: 0.9428 Acc: 53.0283% Time: 2178.7685s\n",
            "[Test Phase] Loss: 0.1389 Acc: 96.1817% Time: 2187.9694s\n",
            "[Test Phase] Loss: 0.5397 Acc: 74.4240% Time: 2197.2019s\n",
            "[Test Phase] Loss: 1.2427 Acc: 41.8696% Time: 2206.3751s\n",
            "[Test Phase] Loss: 0.1677 Acc: 95.5563% Time: 2215.7428s\n",
            "[Test Phase] Loss: 0.5818 Acc: 71.6590% Time: 2224.9655s\n",
            "[Test Phase] Loss: 0.4478 Acc: 79.4273% Time: 2234.1683s\n",
            "[Test Phase] Loss: 0.5787 Acc: 72.3502% Time: 2243.3338s\n",
            "[Test Phase] Loss: 0.2841 Acc: 88.9401% Time: 2252.5729s\n",
            "[Test Phase] Loss: 1.5460 Acc: 32.4226% Time: 2261.8361s\n",
            "[Test Phase] Loss: 0.4179 Acc: 81.7314% Time: 2271.1227s\n",
            "[Test Phase] Loss: 0.8832 Acc: 56.6491% Time: 2280.3862s\n",
            "[Test Phase] Loss: 1.7500 Acc: 23.5352% Time: 2289.7185s\n",
            "[Test Phase] Loss: 1.1850 Acc: 43.8776% Time: 2298.9209s\n",
            "[Test Phase] Loss: 1.9270 Acc: 24.2594% Time: 2308.1330s\n",
            "[Test Phase] Loss: 0.1555 Acc: 95.3259% Time: 2317.3044s\n",
            "[Test Phase] Loss: 1.8545 Acc: 23.5023% Time: 2326.5060s\n",
            "[Test Phase] Loss: 1.7486 Acc: 26.3331% Time: 2335.7727s\n",
            "[Test Phase] Loss: 0.5857 Acc: 72.0211% Time: 2345.1284s\n",
            "[Test Phase] Loss: 0.6108 Acc: 71.1652% Time: 2354.2925s\n",
            "[Test Phase] Loss: 1.2676 Acc: 43.5484% Time: 2363.4936s\n",
            "[Test Phase] Loss: 0.8394 Acc: 60.6978% Time: 2372.6819s\n",
            "[Test Phase] Loss: 0.2523 Acc: 90.5859% Time: 2381.9293s\n",
            "[Test Phase] Loss: 1.9590 Acc: 23.3377% Time: 2391.3452s\n",
            "[Test Phase] Loss: 0.7849 Acc: 62.6728% Time: 2400.6781s\n",
            "[Test Phase] Loss: 1.4576 Acc: 37.6234% Time: 2409.9352s\n",
            "[Test Phase] Loss: 0.5769 Acc: 71.2640% Time: 2419.1302s\n",
            "[Test Phase] Loss: 0.4419 Acc: 79.9539% Time: 2428.4636s\n",
            "[Test Phase] Loss: 0.4853 Acc: 77.3535% Time: 2437.8552s\n",
            "[Test Phase] Loss: 0.3232 Acc: 87.0309% Time: 2447.1590s\n",
            "[Test Phase] Loss: 0.3421 Acc: 85.7472% Time: 2456.4926s\n",
            "[Test Phase] Loss: 0.5896 Acc: 71.2311% Time: 2465.7471s\n",
            "[Test Phase] Loss: 0.6607 Acc: 67.4786% Time: 2475.1123s\n",
            "[Test Phase] Loss: 1.4881 Acc: 32.9164% Time: 2484.5220s\n",
            "[Test Phase] Loss: 0.5344 Acc: 75.1152% Time: 2493.7116s\n",
            "[Test Phase] Loss: 1.2810 Acc: 42.2976% Time: 2503.0737s\n",
            "[Test Phase] Loss: 1.1584 Acc: 42.9559% Time: 2512.2787s\n",
            "[Test Phase] Loss: 1.3272 Acc: 38.5122% Time: 2521.5124s\n",
            "[Test Phase] Loss: 0.9230 Acc: 54.2462% Time: 2530.7003s\n",
            "[Test Phase] Loss: 1.2099 Acc: 43.3509% Time: 2540.1099s\n",
            "[Test Phase] Loss: 0.5534 Acc: 73.8315% Time: 2549.4759s\n",
            "[Test Phase] Loss: 0.7801 Acc: 63.2324% Time: 2558.7066s\n",
            "[Test Phase] Loss: 0.3528 Acc: 84.3318% Time: 2567.8810s\n",
            "[Test Phase] Loss: 0.7970 Acc: 61.0270% Time: 2577.2594s\n",
            "[Test Phase] Loss: 0.5408 Acc: 75.4115% Time: 2586.4800s\n",
            "[Test Phase] Loss: 0.9272 Acc: 56.4516% Time: 2595.8759s\n",
            "[Test Phase] Loss: 1.1434 Acc: 46.1488% Time: 2605.2841s\n",
            "[Test Phase] Loss: 0.1607 Acc: 95.8525% Time: 2614.4761s\n",
            "[Test Phase] Loss: 0.5719 Acc: 72.3502% Time: 2623.8308s\n",
            "[Test Phase] Loss: 0.3990 Acc: 82.2910% Time: 2633.0234s\n",
            "[Test Phase] Loss: 0.8322 Acc: 58.4924% Time: 2642.4387s\n",
            "[Test Phase] Loss: 0.9772 Acc: 51.6787% Time: 2651.6370s\n",
            "[Test Phase] Loss: 0.9517 Acc: 54.0487% Time: 2660.8912s\n",
            "[Test Phase] Loss: 0.4796 Acc: 76.9914% Time: 2670.0514s\n",
            "[Test Phase] Loss: 1.7088 Acc: 27.7485% Time: 2679.2799s\n",
            "[Test Phase] Loss: 0.7752 Acc: 62.6399% Time: 2688.6282s\n",
            "[Test Phase] Loss: 0.9362 Acc: 54.8058% Time: 2697.8589s\n",
            "[Test Phase] Loss: 0.7752 Acc: 61.7841% Time: 2707.1465s\n",
            "[Test Phase] Loss: 0.4294 Acc: 80.5793% Time: 2716.3971s\n",
            "[Test Phase] Loss: 2.3685 Acc: 13.8249% Time: 2725.6091s\n",
            "[Test Phase] Loss: 0.6540 Acc: 67.2482% Time: 2734.8317s\n",
            "[Test Phase] Loss: 0.5468 Acc: 73.8315% Time: 2744.0258s\n",
            "[Test Phase] Loss: 0.6128 Acc: 69.8815% Time: 2753.4366s\n",
            "[Test Phase] Loss: 1.4436 Acc: 37.3601% Time: 2762.6570s\n",
            "[Test Phase] Loss: 1.3605 Acc: 38.4793% Time: 2771.8504s\n",
            "[Test Phase] Loss: 2.2790 Acc: 16.4582% Time: 2781.1856s\n",
            "[Test Phase] Loss: 1.9674 Acc: 20.8361% Time: 2790.5828s\n",
            "[Test Phase] Loss: 2.5291 Acc: 15.7011% Time: 2799.8088s\n",
            "[Test Phase] Loss: 1.3907 Acc: 35.0889% Time: 2809.0101s\n",
            "[Test Phase] Loss: 0.5954 Acc: 72.7781% Time: 2818.2464s\n",
            "[Test Phase] Loss: 0.6196 Acc: 65.5695% Time: 2827.6067s\n",
            "[Test Phase] Loss: 0.9569 Acc: 52.1066% Time: 2836.9344s\n",
            "[Test Phase] Loss: 0.3793 Acc: 82.8835% Time: 2846.1579s\n",
            "[Test Phase] Loss: 1.1221 Acc: 43.6800% Time: 2855.3206s\n",
            "[Test Phase] Loss: 0.3465 Acc: 84.8255% Time: 2864.6191s\n",
            "[Test Phase] Loss: 0.4192 Acc: 80.5135% Time: 2874.0249s\n",
            "[Test Phase] Loss: 0.8373 Acc: 56.8795% Time: 2883.4265s\n",
            "[Test Phase] Loss: 0.4064 Acc: 81.0402% Time: 2892.6751s\n",
            "[Test Phase] Loss: 0.4035 Acc: 82.2581% Time: 2901.9348s\n",
            "[Test Phase] Loss: 1.0670 Acc: 46.3463% Time: 2911.1012s\n",
            "[Test Phase] Loss: 2.7042 Acc: 7.4062% Time: 2920.3095s\n",
            "[Test Phase] Loss: 0.8647 Acc: 53.4233% Time: 2929.5626s\n",
            "[Test Phase] Loss: 0.3793 Acc: 83.7722% Time: 2938.6986s\n",
            "[Test Phase] Loss: 0.7349 Acc: 64.7136% Time: 2947.9370s\n",
            "[Test Phase] Loss: 1.0242 Acc: 49.6708% Time: 2957.1244s\n",
            "[Test Phase] Loss: 1.3601 Acc: 39.7959% Time: 2966.3053s\n",
            "[Test Phase] Loss: 0.3059 Acc: 87.6563% Time: 2975.4901s\n",
            "[Test Phase] Loss: 0.7292 Acc: 63.2982% Time: 2984.6747s\n",
            "[Test Phase] Loss: 0.5104 Acc: 75.9052% Time: 2993.8923s\n",
            "[Test Phase] Loss: 0.6273 Acc: 69.3219% Time: 3003.0742s\n",
            "[Test Phase] Loss: 0.5966 Acc: 70.7044% Time: 3012.2905s\n",
            "[Test Phase] Loss: 1.8888 Acc: 23.0744% Time: 3021.4931s\n",
            "[Test Phase] Loss: 1.2798 Acc: 33.9039% Time: 3030.6632s\n",
            "[Test Phase] Loss: 0.9571 Acc: 52.8637% Time: 3039.8988s\n",
            "[Test Phase] Loss: 1.1237 Acc: 46.5438% Time: 3049.2106s\n",
            "[Test Phase] Loss: 0.6711 Acc: 67.5773% Time: 3058.4878s\n",
            "[Test Phase] Loss: 0.5759 Acc: 72.4490% Time: 3067.8634s\n",
            "[Test Phase] Loss: 1.6551 Acc: 24.4240% Time: 3077.0743s\n",
            "[Test Phase] Loss: 0.7615 Acc: 62.4424% Time: 3086.2624s\n",
            "[Test Phase] Loss: 0.2947 Acc: 88.2818% Time: 3095.4258s\n",
            "[Test Phase] Loss: 1.2521 Acc: 42.9888% Time: 3104.6180s\n",
            "[Test Phase] Loss: 0.6775 Acc: 67.8078% Time: 3113.8740s\n",
            "[Test Phase] Loss: 0.3629 Acc: 83.9368% Time: 3123.1206s\n",
            "[Test Phase] Loss: 0.5474 Acc: 72.6794% Time: 3132.2690s\n",
            "[Test Phase] Loss: 0.3946 Acc: 82.2581% Time: 3141.4378s\n",
            "[Test Phase] Loss: 1.0759 Acc: 47.1692% Time: 3150.6390s\n",
            "[Test Phase] Loss: 0.9089 Acc: 56.2870% Time: 3159.8428s\n",
            "[Test Phase] Loss: 1.1894 Acc: 42.1001% Time: 3169.0308s\n",
            "[Test Phase] Loss: 1.3519 Acc: 37.7880% Time: 3178.2424s\n",
            "[Test Phase] Loss: 0.9815 Acc: 52.2712% Time: 3187.4521s\n",
            "[Test Phase] Loss: 1.1151 Acc: 47.8275% Time: 3196.6518s\n",
            "[Test Phase] Loss: 1.0838 Acc: 47.1034% Time: 3205.8227s\n",
            "[Test Phase] Loss: 0.5625 Acc: 72.8769% Time: 3215.0975s\n",
            "[Test Phase] Loss: 2.2140 Acc: 18.3344% Time: 3224.3871s\n",
            "[Test Phase] Loss: 2.8350 Acc: 7.7683% Time: 3233.6448s\n",
            "[Test Phase] Loss: 0.6992 Acc: 65.7669% Time: 3242.8051s\n",
            "[Test Phase] Loss: 1.4863 Acc: 33.9697% Time: 3251.9835s\n",
            "[Test Phase] Loss: 2.0151 Acc: 19.9802% Time: 3261.1921s\n",
            "[Test Phase] Loss: 1.7763 Acc: 27.2877% Time: 3270.4171s\n",
            "[Test Phase] Loss: 0.5302 Acc: 74.1277% Time: 3279.5995s\n",
            "[Test Phase] Loss: 1.0271 Acc: 50.2633% Time: 3288.8091s\n",
            "[Test Phase] Loss: 0.5507 Acc: 73.8315% Time: 3298.1434s\n",
            "[Test Phase] Loss: 0.1438 Acc: 95.9842% Time: 3307.3999s\n",
            "[Test Phase] Loss: 0.3755 Acc: 83.7393% Time: 3316.6206s\n",
            "[Test Phase] Loss: 1.6394 Acc: 28.7031% Time: 3325.8582s\n",
            "[Test Phase] Loss: 0.8472 Acc: 55.0033% Time: 3334.9706s\n",
            "[Test Phase] Loss: 1.1809 Acc: 45.0625% Time: 3344.1845s\n",
            "[Test Phase] Loss: 0.2359 Acc: 91.3101% Time: 3353.3633s\n",
            "[Test Phase] Loss: 0.3374 Acc: 85.3193% Time: 3362.6013s\n",
            "[Test Phase] Loss: 0.2880 Acc: 88.6438% Time: 3371.7597s\n",
            "[Test Phase] Loss: 0.5205 Acc: 75.4444% Time: 3380.9945s\n",
            "[Test Phase] Loss: 0.6750 Acc: 67.8078% Time: 3390.1570s\n",
            "[Test Phase] Loss: 0.8120 Acc: 60.5003% Time: 3399.3820s\n",
            "[Test Phase] Loss: 0.1215 Acc: 97.0704% Time: 3408.5487s\n",
            "[Test Phase] Loss: 1.1592 Acc: 44.0092% Time: 3417.7583s\n",
            "[Test Phase] Loss: 0.2423 Acc: 90.9480% Time: 3426.9425s\n",
            "[Test Phase] Loss: 2.6218 Acc: 7.4391% Time: 3436.1512s\n",
            "[Test Phase] Loss: 1.1439 Acc: 46.2804% Time: 3445.4186s\n",
            "[Test Phase] Loss: 0.2702 Acc: 88.7426% Time: 3454.5625s\n",
            "[Test Phase] Loss: 0.1308 Acc: 96.5767% Time: 3463.7711s\n",
            "[Test Phase] Loss: 0.9776 Acc: 51.6129% Time: 3472.9361s\n",
            "[Test Phase] Loss: 0.2614 Acc: 90.0922% Time: 3482.1103s\n",
            "[Test Phase] Loss: 1.1041 Acc: 46.3792% Time: 3491.3153s\n",
            "[Test Phase] Loss: 0.6715 Acc: 67.0836% Time: 3500.5237s\n",
            "[Test Phase] Loss: 0.1964 Acc: 93.3509% Time: 3509.6991s\n",
            "[Test Phase] Loss: 0.4931 Acc: 75.8723% Time: 3518.9260s\n",
            "[Test Phase] Loss: 0.3241 Acc: 86.4384% Time: 3528.1131s\n",
            "[Test Phase] Loss: 0.3051 Acc: 87.9526% Time: 3537.2959s\n",
            "[Test Phase] Loss: 2.0419 Acc: 21.1982% Time: 3546.5488s\n",
            "[Test Phase] Loss: 0.5287 Acc: 74.6544% Time: 3555.7434s\n",
            "[Test Phase] Loss: 0.8817 Acc: 55.3325% Time: 3564.9137s\n",
            "[Test Phase] Loss: 1.4002 Acc: 36.2739% Time: 3574.1132s\n",
            "[Test Phase] Loss: 0.4945 Acc: 76.4319% Time: 3583.3601s\n",
            "[Test Phase] Loss: 0.6695 Acc: 66.7874% Time: 3592.4929s\n",
            "[Test Phase] Loss: 0.1545 Acc: 95.5563% Time: 3601.7380s\n",
            "[Test Phase] Loss: 0.4436 Acc: 79.7564% Time: 3610.8957s\n",
            "[Test Phase] Loss: 1.1348 Acc: 47.0375% Time: 3620.1077s\n",
            "[Test Phase] Loss: 4.4374 Acc: 2.5675% Time: 3629.3161s\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "path = \"C:/team3/resnet/models/#4 resnet_models\"\n",
        "file_list = os.listdir(path)\n",
        "writer1 = SummaryWriter()\n",
        "\n",
        "for epoch, file in enumerate(file_list):\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "    model.load_state_dict(torch.load(path + \"/\" + file))\n",
        "    model.eval()\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0.\n",
        "        test_corrects = 0\n",
        "\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "            # print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "            # imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "    epoch_loss1 = test_loss / len(test_datasets)\n",
        "    epoch_acc1 = test_corrects / len(test_datasets) * 100.\n",
        "    print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss1, epoch_acc1, time.time() - start_time))\n",
        "\n",
        "    writer1.add_scalar(\"Loss/test\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/test\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/test\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    writer1.add_scalar(\"Loss/sum\", epoch_loss1, epoch)\n",
        "    writer1.add_scalar(\"Acc/sum\", epoch_acc1, epoch)\n",
        "    writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_test': epoch_loss1, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "    # writer1.add_scalars(\"Loss/sum\", {'epoch_loss_train': epoch_loss, 'epoch_loss_test': epoch_loss1}, epoch)\n",
        "    # writer1.add_scalars(\"Acc/sum\", {'epoch_acc_train': epoch_acc/100, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "    # writer1.add_scalars(\"Loss_Acc/sum\", {'epoch_loss_train': epoch_loss, 'epoch_loss_test': epoch_loss1, 'epoch_acc_train': epoch_acc/100, 'epoch_acc_test': epoch_acc1/100}, epoch)\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#0 Loss: 0.8026 Acc: 50.9524% Time: 39.0565s\n",
        "#0 Loss: 0.8026 Acc: 50.9524% Time: 37.5618s\n",
        "#0 Loss: 0.8026 Acc: 50.9524% Time: 32.1134s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q78FprL_lPKP"
      },
      "outputs": [],
      "source": [
        "#모델 저장\n",
        "\n",
        "torch.save(model.state_dict(),'resnet_dict.pth')\n",
        "# torch.save(model,'model.pth')\n",
        "torch.save(model,'resnet.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = models.resnet34(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "model.load_state_dict(torch.load('#2 resnet_dict.pth'))\n",
        "model.eval()\n",
        "model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4ZRz72LDZpdP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 1)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 0] (실제 정답: 0)\n",
            "[예측 결과: 1] (실제 정답: 0)\n",
            "[Test Phase] Loss: 0.5954 Acc: 72.7781% Time: 9.2968s\n"
          ]
        }
      ],
      "source": [
        "# model.eval()\n",
        "start_time = time.time()\n",
        "writer = SummaryWriter()\n",
        "\n",
        "with torch.no_grad():\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "        print(f'[예측 결과: {class_names[preds[0]]}] (실제 정답: {class_names[labels.data[0]]})')\n",
        "        # imshow(inputs.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])\n",
        "\n",
        "    epoch_loss = running_loss / len(test_datasets)\n",
        "    epoch_acc = running_corrects / len(test_datasets) * 100.\n",
        "    print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc, time.time() - start_time))\n",
        "    \n",
        "    # writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "    # writer.add_scalar(\"acc/train\", epoch_acc, epoch)\n",
        "    # writer.add_scalar(\"sum/train\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"sum/train\", epoch_acc/100, epoch)\n",
        "\n",
        "writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "[Test Phase] Loss: 1.4326 Acc: 10.5003% Time: 209.7178s\n",
        "[Test Phase] Loss: 1.4326 Acc: 10.5003% Time: 215.8944s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHorVg4-Z68P"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "image = Image.open('test_img.jpg')\n",
        "image = transforms_test(image).unsqueeze(0).to(device)\n",
        "print(image.shape)\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    print(outputs)\n",
        "    print(class_names[preds[0]])\n",
        "    imshow(image.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 저장한 모델 가져와서 쓰기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9VvK7xiaax0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"resnet_dict.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKCkA0fXmEHm"
      },
      "outputs": [],
      "source": [
        "files.download(\"resnet.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0vAUYhKpmIsR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = models.resnet34(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "model.load_state_dict(torch.load('#2 resnet_dict.pth'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwzRuBb1nzxe"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "model = model.to(device)\n",
        "image = Image.open('test.jpg')\n",
        "image = transforms_test(image).unsqueeze(0).to(device)\n",
        "print(image.shape)\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    print(outputs)\n",
        "    print(class_names[preds[0]])\n",
        "    imshow(image.cpu().data[0], title='예측 결과: ' + class_names[preds[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzKPl5rLoNce"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "team3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "38eac6efdfb6e1d89e5adada41cd1cba1407b7df80f8c1b640481bdc8f4da74b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
